[{"label":"test_1","q_format":"single","q_text":"You are interacting with a Point Of Sale (PoS) terminal, which sends the transaction details only. Due to latest software update a bug was introduced in the terminal software that caused it to send individual PII and card details. As a security measure, you are required to implement a quick solution to prevent access to the PII. How would you design the solution?","answers":[{"ans":"Train Model using Tensorflow to identify PII and filter the information","val":false},{"ans":"Store the data in BigQuery and create a Authorized view for the users","val":false},{"ans":"Use Data Loss Prevention APIs to identify the PII information and filter the information","val":true},{"ans":"Use Cloud Natural Language API to identify PII and filter the information","val":false}],"q_expl":"Data Loss Prevention APIs can be used to quickly redact the sensitive information.\n\nDocumentation:\u00a0Cloud DLP\nCloud DLP\u00a0helps you better understand and manage sensitive data. It provides fast, scalable classification and redaction for sensitive data elements like credit card numbers, names, social security numbers, US and selected international identifier numbers, phone numbers and GCP credentials. Cloud DLP classifies this data using more than 90 predefined detectors to identify patterns, formats, and checksums, and even understands contextual clues. You can optionally redact data as well using techniques like masking, secure hashing, bucketing, and format-preserving encryption.\nA is wrong as building and training a model is not a quick and easy solution.\nB is wrong as the data would still be stored in the base tables and accessible.\nD is wrong as Cloud Natural APIs is for text analysis and does not handle sensitive information redaction."},{"label":"test_1","q_format":"single","q_text":"You are asked to design next generation of smart helmet for accident detection and reporting system. Each helmet will push 10kb of biometric data In JSON format every 1 second to a collection platform that will process and use trained machine learning model to predict and detect if an accident happens and send notification. Management has tasked you to architect the platform ensuring the following requirements are met:\u00b7\u00a0Provide the ability for real-time analytics of the inbound biometric data\u00b7\u00a0Ensure ingestion and processing of the biometric data is highly durable. Elastic and parallel\u00b7\u00a0The results of the analytic processing should be persisted for data mining to improve the accident detection ML model in the future.Which architecture outlined below will meet the initial requirements for the platform?","answers":[{"ans":"Utilize Cloud Storage to collect the inbound sensor data, analyze data with Dataproc and save the results to BigQuery.","val":false},{"ans":"Utilize Cloud Pub\/Sub to collect the inbound sensor data, analyze the data with Dataflow and save the results to BigQuery.","val":true},{"ans":"Utilize Cloud Pub\/Sub to collect the inbound sensor data, analyze the data with Dataflow and save the results to Cloud SQL.","val":false},{"ans":"Utilize Cloud Pub\/Sub to collect the inbound sensor data, analyze the data with Dataflow and save the results to Bigtable","val":false}],"q_expl":"Correct answer is B as Cloud Pub\/Sub provides elastic and scalable ingestion, Dataflow provides processing and BigQuery analytics. \nIoT:-\u00a0https:\/\/cloud.google.com\/solutions\/iot-overview \n1. Google Cloud Pub\/Sub\u00a0provides a globally durable message ingestion service. By creating topics for streams or channels, you can enable different components of your application to subscribe to specific streams of data without needing to construct subscriber-specific channels on each device. Cloud Pub\/Sub also natively connects to other Cloud Platform services, helping you to connect ingestion, data pipelines, and storage systems. \n2. Google Cloud Dataflow\u00a0provides the open Apache Beam programming model as a managed service for processing data in multiple ways, including batch operations, extract-transform-load (ETL) patterns, and continuous, streaming computation. Cloud Dataflow can be particularly useful for managing the high-volume data processing pipelines required for IoT scenarios. Cloud Dataflow is also designed to integrate seamlessly with the other Cloud Platform services you choose for your pipeline. \n3. Google BigQuery\u00a0provides a fully managed data warehouse with a familiar SQL interface, so you can store your IoT data alongside any of your other enterprise analytics and logs. The performance and cost of BigQuery means you might keep your valuable data longer, instead of deleting it just to save disk space. \nA is wrong as Cloud Storage is not an ideal ingestion service for real time high frequency data. Also Dataproc is a fast, easy-to-use, fully-managed cloud service for running Apache Spark and Apache Hadoop clusters in a simpler, more cost-efficient way. \nC is wrong as Cloud SQL is a relational database and not suited for analytics data storage. \nD is wrong as Bigtable is not ideal for long term analytics data storage."},{"label":"test_1","q_format":"single","q_text":"You are running your Spark jobs on Google Cloud using Dataproc. The jobs are running very slow and upon investigating you find out that only 1 core of the available 16 cores are being used for the jobs. How do you tune the cluster to use all the cores and improve the job performance?","answers":[{"ans":"Pass the spark.driver.cores to parameter tune the number of cores used","val":false},{"ans":"Increase the number of task nodes","val":false},{"ans":"Pass the spark.executor.cores parameter to tune the number of cores used","val":true},{"ans":"Update the job application to use all of the available cores","val":false}],"q_expl":"Correct answer is C as the number of cores used by the executor can be increased by passing the spark.executor.cores parameter.\nSpark configuration:\u00a0https:\/\/spark.apache.org\/docs\/latest\/configuration.html\nA is wrong as Spark driver cores is for the driver only. The executor performance would not improve.\nB is wrong as increasing the task nodes would improve performance. However, it does not improve efficiency of using the current machines.\nD is wrong as the job application cannot be modified to use all cores and it needs to be at the scheduler level."},{"label":"test_1","q_format":"single","q_text":"You created a job which runs daily to import highly sensitive data from an on-premises location to Cloud Storage. You also set up a streaming data insert into Cloud Storage via a Kafka node that is running on a Compute Engine instance. You need to encrypt the data at rest and supply your own encryption key. Your key should not be stored in the Google Cloud. What should you do?","answers":[{"ans":"Create a dedicated service account, and use encryption at rest to reference your data stored in Cloud Storage and Compute Engine data as part of your API service calls.","val":false},{"ans":"Upload your own encryption key to Cloud Key Management Service, and use it to encrypt your data in Cloud Storage. Use your uploaded encryption key and reference it as part of your API service calls to encrypt your data in the Kafka node hosted on Compute Engine.","val":false},{"ans":"Upload your own encryption key to Cloud Key Management Service, and use it to encrypt your data in your Kafka node hosted on Compute Engine.","val":false},{"ans":"Supply your own encryption key, and reference it as part of your API service calls to encrypt your data in Cloud Storage and your Kafka node hosted on Compute Engine.","val":true}],"q_expl":"Correct answer is D as the scenario requires you to use your own key and also to not store your key on Compute Engine.\nEncryption at Rest:-\u00a0https:\/\/cloud.google.com\/security\/encryption-at-rest\/\nA is wrong as the scenario states that you must supply your own encryption key instead of using one generated by Google Cloud Platform.\nB is wrong as the scenario states that you should use, but not store, your own key with Google Cloud Platform services.\nC is wrong as it does not meet the scenario requirement of not storing the keys on Google Platform services."},{"label":"test_1","q_format":"single","q_text":"Your company uses Google Analytics for tracking. You need to export the session and hit data from a Google Analytics 360 reporting view on scheduled basis into BigQuery for analysis. How can the data be exported?","answers":[{"ans":"Configure a scheduler in Google Analytics to convert the Google Analytics data to JSON format, then import directly into BigQuery using bq command line.","val":false},{"ans":"Use gsutil to export the Google Analytics data to Cloud Storage, then import into BigQuery and schedule it using Cron.","val":false},{"ans":"Import data to BigQuery directly from Google Analytics using Cron","val":false},{"ans":"Use BigQuery Data Transfer Service to import the data from Google Analytics","val":true}],"q_expl":"The correct option to export session and hit data from a Google Analytics 360 reporting view on a scheduled basis into BigQuery for analysis is to use the BigQuery Data Transfer Service to import the data from Google Analytics.\nThe BigQuery Data Transfer Service is a fully managed service that automates the transfer of data from SaaS applications like Google Analytics, into BigQuery. This service simplifies the process of importing data from Google Analytics and provides features like scheduling, monitoring, and error handling.\nTo use the BigQuery Data Transfer Service to import data from Google Analytics, follow these steps:\n1. Open the BigQuery console and create a dataset where the imported data will be stored.\n2. Go to the BigQuery Data Transfer Service page and create a new transfer configuration for Google Analytics.\n3. Configure the transfer settings, including the reporting view, the dataset, and the schedule for data imports.\n4. Start the transfer process and monitor the progress of the import job.\nOnce the data is imported into BigQuery, it can be used for analysis and reporting using SQL queries, visualization tools, or other analysis tools.\nThe other options mentioned in the question are not correct. Configuring a scheduler in Google Analytics to convert the data to JSON format and importing directly into BigQuery using bq command line, or importing data to BigQuery directly from Google Analytics using Cron requires manual configuration and monitoring, and does not provide the automation and error handling features provided by the BigQuery Data Transfer Service. Using gsutil to export data to Cloud Storage and importing it into BigQuery using Cron is also a manual process that requires additional steps and monitoring.\nBigQuery Data Transfer Service:-\u00a0https:\/\/cloud.google.com\/bigquery\/docs\/loading-data#data_transfer_service"},{"label":"test_1","q_format":"single","q_text":"You have been asked to select the storage system for the click-data of your company\u2019s large portfolio of websites. This data is streamed in from a custom website analytics package at a typical rate of 6,000 clicks per minute. With bursts of up to 8,500 clicks per second. It must have been stored for future analysis by your data science and user experience teams. Which storage infrastructure should you choose?","answers":[{"ans":"Google Cloud SQL","val":false},{"ans":"Google Cloud Bigtable","val":true},{"ans":"Google Cloud Storage","val":false},{"ans":"Google Cloud Datastore","val":false}],"q_expl":"Bigtable\u00a0provides a scalable, fully-managed NoSQL wide-column database that is suitable for both real-time access and analytics workloads.\nStorage Options:-\u00a0https:\/\/cloud.google.com\/storage-options\/\nA is wrong as Google Cloud SQL is mainly for OLTP (Transactional, CRUD) not for taking and storing streaming data. It does not have the scalability and elasticity to absorb this amount of data in real time.\nD is wrong as Google Cloud Datastore does not provide analytics capabilities. Google Cloud Datastore is a NoSQL document database built for automatic scaling, high performance, and ease of application development and integrating well with App Engine.\nC is wrong as Google Cloud Storage is not suitable to handle real time streaming data. It also needs to be used with BigQuery for analytics."},{"label":"test_1","q_format":"single","q_text":"A company has its data distributed across multiple projects. They want to enable users to be able to execute BigQuery queries across dataset owned by the different projects. However, to optimize cost they want the billing to a single separate account. How should the access be controlled?","answers":[{"ans":"Add users to groups. Groups to have BigQuery User role for billing project and data viewer role to projects with dataset","val":false},{"ans":"Add users to groups. Groups to have BigQuery jobUser role for billing project and data viewer role to projects with dataset","val":true},{"ans":"Add users to groups. Groups to have BigQuery User role for projects with dataset billing project and data viewer role to billing project","val":false},{"ans":"Add users to groups. Groups to have BigQuery jobUser role for projects with dataset billing project and data viewer role to billing project","val":false}],"q_expl":"To enable users to execute BigQuery queries across datasets owned by different projects while optimizing costs to a single separate account, the access should be controlled by adding users to groups with the BigQuery jobUser role for the billing project and the data viewer role for the projects with the dataset.\nThe jobUser role allows users to submit queries and cancel queries submitted by themselves, while the data viewer role allows users to view the data in the datasets without being able to make any changes. By granting these roles to the appropriate groups, users will be able to query across datasets in different projects while keeping costs in check.\nOption 2 is the correct answer as it grants the necessary permissions to users while also optimizing costs.\nOption 1 and option 3 both assign the BigQuery User role to groups, which grants more permissions than necessary for querying across datasets.\nOption 4 assigns only the BigQuery jobUser role to the groups, which is not sufficient to view data in the datasets.\nThis option ensures that users are added to groups, which simplifies access management. The groups should have the BigQuery User role for the billing project, allowing them to execute BigQuery queries, and the data viewer role for projects with datasets, allowing them to access and view the data in those projects. By separating the billing project from the projects with datasets, the company can optimize cost by consolidating the billing to a single separate account.\nReference:\nControlling access to BigQuery resources.\u00a0https:\/\/cloud.google.com\/bigquery\/docs\/access-control"},{"label":"test_1","q_format":"single","q_text":"You are using a Compute Engine instance to manage your Cloud Dataflow processing workloads. What IAM role do you need to grant to the instance so that it has the necessary access?","answers":[{"ans":"Dataflow Viewer","val":false},{"ans":"Dataflow Developer","val":false},{"ans":"Dataflow Worker","val":true},{"ans":"Dataflow Computer","val":false}],"q_expl":"Dataflow Worker is the correct IAM role for service accounts assigned to Compute engines.\nDataflow Roles:-\u00a0https:\/\/cloud.google.com\/dataflow\/docs\/concepts\/access-control#roles\nTo manage Cloud Dataflow processing workloads using a Compute Engine instance, you need to grant the Dataflow Worker IAM role to the instance. This role has permissions to create and manage Dataflow jobs, read and write to Cloud Storage and Pub\/Sub, and create and manage Compute Engine instances as necessary for the Dataflow job.\nA & B are wrong as Viewer and Developer role does not provide access process data for the Cloud Dataflow service.\nD is wrong as Dataflow computer role does not exist.\nReferences:\nAccess control for Dataflow\nIAM roles for Dataflow"},{"label":"test_1","q_format":"single","q_text":"You are part of your company\u2019s migration team to transfer 1PB of data to Google Cloud. The network speed between the on-premises data center and Google Cloud is 100Mbps. The migration activity has a timeframe of 6 months. What is the efficient way to transfer the data?","answers":[{"ans":"Use BigQuery Data Transfer Service to transfer the data to Cloud Storage","val":false},{"ans":"Expose the data as a public URL and Storage Transfer Service to transfer it","val":false},{"ans":"Use Transfer appliance to transfer the data to Cloud Storage","val":true},{"ans":"Use gsutil command to transfer the data to Cloud Storage","val":false}],"q_expl":"Correct answer is C as even with 100Mbps of transfer speed it would take minimum 3 years (assuming consistent speed and no interruption) to transfer the complete data. So the best option is to use Google Transfer Appliance.\nData Transfer:-\u00a0https:\/\/cloud.google.com\/products\/data-transfer\/\nGoogle Transfer Appliance\u00a0\u2013 Securely capture, ship, and upload your data to Google Cloud Storage using the Transfer Appliance 100 TB or 480 TB models.\nA, B & D are wrong as they would still route the request through Internet."},{"label":"test_1","q_format":"single","q_text":"As a mandate for least privilege within the company, the external users have been provided the Cloud Dataproc Viewer role. Which action these external users perform?","answers":[{"ans":"Submit a job.","val":false},{"ans":"Create a cluster.","val":false},{"ans":"Delete a cluster.","val":false},{"ans":"List the jobs.","val":true}],"q_expl":"Correct answer is D as with Data Viewer role, the users would only be able to perform get and list operations.\nDataproc IAM Access Control:-\u00a0https:\/\/cloud.google.com\/dataproc\/docs\/concepts\/iam\/iam\nIdentity and Access Management (IAM) Cloud Dataproc roles are a bundle of one or more permissions. You grant roles to users or groups to allow them to perform actions on the Cloud Dataproc resources in your project. For example, the Dataproc Viewerrole contains the\u00a0dataproc.*.get\u00a0and\u00a0dataproc.*.list\u00a0permissions, which allow a user to get and list Cloud Dataproc clusters, jobs, and operations in a project."},{"label":"test_1","q_format":"single","q_text":"You receive customer transactions through Cloud Pub\/Sub and you are planning to use Google\u2019s Dataflow SDK to analyze customer data such as displayed below:1. Tom, 555 X street2. Tim, 553 Y street3. Sam, 111 Z streetYour project requirement is to extract only the customer name from the data source and then write to an output PCollection. Which operation is best suited for the above data processing requirement?","answers":[{"ans":"Sink API","val":false},{"ans":"ParDo","val":true},{"ans":"Transform","val":false},{"ans":"Extract","val":false}],"q_expl":"Correct answer is B as ParDo helps in extracting parts from elements. \nDataflow ParDo & Programming Model:-\u00a0https:\/\/cloud.google.com\/dataflow\/model\/par-do\u00a0&\u00a0https:\/\/cloud.google.com\/dataflow\/model\/programming-model \nParDo is useful for a variety of data processing operations, including: \n1. Filtering a data set. You can use ParDo to consider each element in a PCollection and either output that element to a new collection or discard it. \n2. Formatting or converting the type of each element in a data set. You can use ParDo to format the elements in your PCollection, such as formatting key\/value pairs into printable strings. \n3. Extracting parts of each element in a data set. You can use ParDo to extract just a part of each element in your PCollection. This can be particularly useful for extracting individual fields from BigQuery table rows. \n4. Performing computations on each element in a data set. You can use ParDo to perform simple or complex computations on every element, or certain elements, of a PCollection. \nA is wrong as Sink API helps to write output data from your pipeline \nC is wrong as Transform represents a data processing operation, or a step, in your pipeline. \nD is wrong as Extract does not exist."},{"label":"test_1","q_format":"single","q_text":"Your production Bigtable instance is currently using four nodes. Due to the increased size of your table, you need to add additional nodes to offer better performance. How should you accomplish this without the risk of data loss?","answers":[{"ans":"Power off your Bigtable instance, then increase the node count, then power back on. Be sure to schedule downtime in advance.","val":false},{"ans":"Export your Bigtable data as sequence files into Cloud Storage, then import the data into a new Bigtable instance with additional nodes added.","val":false},{"ans":"Use the node migration service to add additional nodes.","val":false},{"ans":"Edit instance details and increase the number of nodes. Save your changes. Data will re-distribute with no downtime.","val":true}],"q_expl":"To add additional nodes to a Bigtable instance without the risk of data loss, the recommended approach is to edit the instance details and increase the number of nodes. This can be done through the Google Cloud Console or the Cloud Bigtable API, and it does not require any downtime or data migration.\nWhen the number of nodes is increased, Bigtable automatically redistributes data across the new nodes to balance the workload, and the data remains fully available during the process. This is a safe and efficient method to scale up the performance of a Bigtable instance.\nOption 1 is not recommended as it requires downtime and has the potential for data loss during the power-off and power-on process.\nOption 2 is also not recommended as it requires exporting and importing the data, which is a time-consuming and resource-intensive process. It may also result in data inconsistencies or errors.\nOption 3 refers to the node migration service, which is currently in beta and may not be suitable for production environments.\nTherefore, the best option is to use the built-in capability of Bigtable to increase the number of nodes and allow for automatic data redistribution.\nReference:\nGoogle Cloud. (n.d.). Scaling Cloud Bigtable instances. Retrieved from\u00a0https:\/\/cloud.google.com\/bigtable\/docs\/scaling-instances"},{"label":"test_1","q_format":"single","q_text":"Your company\u2019s on-premises Apache Hadoop servers are approaching end-of-life, and IT has decided to migrate the cluster to Google Cloud Dataproc. A like-for-like migration of the cluster would require 50 TB of Google Persistent Disk per node. The CIO is concerned about the cost of using that much block storage. You want to minimize the storage cost of the migration. What should you do?","answers":[{"ans":"Put the data into Google Cloud Storage.","val":true},{"ans":"Use preemptible virtual machines (VMs) for the Cloud Dataproc cluster.","val":false},{"ans":"Tune the Cloud Dataproc cluster so that there is just enough disk for all data.","val":false},{"ans":"Migrate some of the cold data into Google Cloud Storage, and keep only the hot data in Persistent Disk.","val":false}],"q_expl":"Correct answer is A as moving the data to Cloud Storage would reduce the storage cost a lot. Also, this would enable the customer to use transient cluster. \nDataproc Data Accessibility:-\u00a0https:\/\/cloud.google.com\/dataproc\/docs\/resources\/faq#data_access_availability \nHow can I get data in and out of a cluster? \nCloud Dataproc utilizes the Hadoop Distributed File System (HDFS) for storage. Additionally, Cloud Dataproc automatically installs the HDFS-compatible Google Cloud Storage connector, which enables the use of Cloud Storage in parallel with HDFS. Data can be moved in and out of a cluster through upload\/download to HDFS or Cloud Storage. \nCan I use Cloud Storage with Dataproc? \nYes,\u00a0Cloud Dataproc\u00a0clusters automatically install the Cloud Storage connector. There are a number of benefits to choosing Cloud Storage over traditional HDFS including data persistence, reliability, and performance. \nB is wrong as preemptible VMs would only help improve execution performance and cost, but would not reduce the storage cost. \nC is wrong as it would not reduce costs. \nD is wrong as it would not reduce the cost by much as it would still need the data to be stored across nodes."},{"label":"test_1","q_format":"single","q_text":"As part of a complex rollout, you have hired a third party developer consultant to assist with creating your Dataflow processing pipeline. The data that this pipeline will process is very confidential, and the consultant cannot be allowed to view the data itself. What actions should you take so that they have the ability to help build the pipeline but cannot see the data it will process?","answers":[{"ans":"Assign the consultant the Dataflow Developer IAM role.","val":false},{"ans":"Apply custom encryption to the data before it goes through the pipeline.","val":false},{"ans":"Use a separate development project to construct the pipeline with example data, therefore not exposing the live data to the developer\u2018s work environment.","val":true},{"ans":"Anonymize the data before it gets to the Dataflow pipeline.","val":false}],"q_expl":"The correct action to take in this scenario is: \nUse a separate development project to construct the pipeline with example data, therefore not exposing the live data to the developer\u2018s work environment. \nBy creating a separate development project and using example data, you can ensure that the confidential live data is not exposed to the developer\u2018s work environment. This approach allows the consultant to build the pipeline and test its functionality without having access to the actual confidential data. \nAssigning the consultant the Dataflow Developer IAM role (Option A) may give them access to the data, which is not desirable in this case as the data is confidential. \nApplying custom encryption to the data before it goes through the pipeline (Option B) may provide additional security for the data, but it doesn\u2018t address the requirement of preventing the consultant from viewing the data itself. \nAnonymizing the data before it gets to the Dataflow pipeline (Option D) is another possible approach, but it may introduce complexity and potential risks if the anonymization process is not properly implemented. \nTherefore, using a separate development project with example data is the best option to allow the consultant to assist with building the pipeline while ensuring the confidentiality of the actual data. \nReference: \nManaging Access for Projects and Resources:\u00a0https:\/\/cloud.google.com\/iam\/docs\/overview \nB,C & D are wrong as that would need additional steps and the access can be easily controlled using IAM roles."},{"label":"test_1","q_format":"single","q_text":"You need to design a real time streaming data processing pipeline. The pipeline needs to read data from Cloud Pub\/Sub, enrich it using Static reference data in BigQuery, transform it and store the results back in BigQuery for further analytics. How would you design the pipeline?","answers":[{"ans":"Dataflow, BigQueryIO and PubSubIO, SideOutputs","val":false},{"ans":"Dataflow, BigQueryIO and PubSubIO, SideInputs","val":true},{"ans":"DataProc, BigQueryIO and PubSubIO, SideInputs","val":false},{"ans":"DataProc, BigQueryIO and PubSubIO, SideOutputs","val":false}],"q_expl":"Dataflow\u00a0is needed for real time streaming pipeline with the ability to enrich and transform using SideInputs. BigQueryIO and PubSubIO to interact with BigQuery and Pub\/Sub.\nDataflow Use Case Patterns:-\u00a0https:\/\/cloud.google.com\/blog\/products\/gcp\/guide-to-common-cloud-dataflow-use-case-patterns-part-1\n\nIn streaming mode, lookup tables need to be accessible by your pipeline. If the lookup table never changes, then the standard Cloud Dataflow SideInput pattern reading from a bounded source such as BigQuery is a perfect fit. However, if the lookup data changes over time, in streaming mode there are additional considerations and options. The pattern described here focuses on slowly-changing data \u2014 for example, a table that\u2019s updated daily rather than every few hours.\nC & D are wrong as Dataproc is not ideal for handling real time streaming data.\nA & D are wrong as the lookup tables can be referred using SideInputs."},{"label":"test_1","q_format":"single","q_text":"Your company hosts a 2PB on-premises Hadoop cluster with sensitive data. They want to plan the migration of the cluster to Google Cloud as part of phase 1 activity before the jobs are moved. Current network speed between the colocation and cloud is 10Gbps. What is the efficient way to transfer the data?","answers":[{"ans":"Use Transfer appliance to transfer the data to Cloud Storage","val":true},{"ans":"Expose the data as a public URL and Storage Transfer Service to transfer it","val":false},{"ans":"Use gsutil command to transfer the data to Cloud Storage","val":false},{"ans":"Use hadoop distcp command to copy the data between cluster","val":false}],"q_expl":"With 10Gbps of transfer speed it would take minimum 24 days (assuming consistent speed and no interruption) to transfer the complete data. So the best option is to use Google Transfer Appliance.\nData Transfer:-\u00a0https:\/\/cloud.google.com\/products\/data-transfer\/\nGoogle Transfer Appliance\u00a0\u2013 Securely capture, ship, and upload your data to Google Cloud Storage using the Transfer Appliance 100 TB or 480 TB models.\nB, C & D are wrong as they would still route the request through Internet."},{"label":"test_1","q_format":"single","q_text":"You have a real time data processing pipeline running in Dataflow. As a part of changed requirement you need to update the windowing and triggering strategy for the pipeline. You want to update the pipeline without any loss of in-flight messages. What is the best way to deploy the changes?","answers":[{"ans":"Stop with pipeline using the drain option and use new Dataflow pipeline","val":false},{"ans":"Stop with pipeline using the cancel option and use new Dataflow pipeline","val":false},{"ans":"Pass the --update option with --jobname parameter to the same name as the job you want to update","val":true},{"ans":"Pass the --update option with --jobname parameter to the new job name you want to update","val":false}],"q_expl":"Dataflow allows updates to the existing pipeline in case of compatible changes while saving the intermediate state data.\nDataflow Updating a Pipeline:-\u00a0https:\/\/cloud.google.com\/dataflow\/docs\/guides\/updating-a-pipeline\n\nA is wrong as with Drain option the windows and triggers would closed immediately.\nB is wrong as Cancel immediately halts processing, you may lose any \u201cin-flight\u201d data.\nD is wrong as the job name should be the same."},{"label":"test_1","q_format":"single","q_text":"Your company is working on real time click stream analysis. They want to implement a feature to capture user click during a session and aggregate the count for that session. Session timeout is 30 mins. How would you design the data processing?","answers":[{"ans":"Use Dataflow and fixed windowing of 30 minutes.","val":false},{"ans":"Use Dataflow and Session windowing with gap duration of 30 minutes.","val":true},{"ans":"Use Dataflow and Global window with gap duration of 30 minutes.","val":false},{"ans":"Use Dataproc and store the data in BigQuery and aggregate the same.","val":false}],"q_expl":"Dataflow\u00a0would help in performing real time analytics and data count aggregation over a window. Session windows to track the session for the aggregate click count by the user.\nBeam Windowing Basics:-\u00a0https:\/\/beam.apache.org\/documentation\/programming-guide\/#windowing-basics\n\nA session window function defines windows that contain elements that are within a certain gap duration of another element. Session windowing applies on a per-key basis and is useful for data that is irregularly distributed with respect to time. For example, a data stream representing user mouse activity may have long periods of idle time interspersed with high concentrations of clicks. If data arrives after the minimum specified gap duration time, this initiates the start of a new window.\nA & C are wrong as Fixed and Global windowing would not work.\nD is wrong as Dataproc and BigQuery would not provide real time analytics."},{"label":"test_1","q_format":"single","q_text":"You are setting up Cloud Dataproc to perform some data transformations using Apache Spark jobs. The data will be used for a new set of non-critical experiments in your marketing group. You want to set up a cluster that can transform a large amount of data in the most cost-effective way. What should you do?","answers":[{"ans":"Set up a cluster in High Availability mode with high-memory machine types. Add 10 additional local SSDs.","val":false},{"ans":"Set up a cluster in High Availability mode with default machine types. Add 10 additional Preemptible worker nodes.","val":false},{"ans":"Set up a cluster in Standard mode with high-memory machine types. Add 10 additional Preemptible worker nodes.","val":true},{"ans":"Set up a cluster in Standard mode with the default machine types. Add 10 additional local SSDs.","val":false}],"q_expl":"Dataproc\u00a0is a managed service which handles Spark and Hadoop jobs and Spark and high-memory machines only need the Standard mode. Also, using Preemptible nodes provides\u00a0cost-efficiency\u00a0as this is not mission-critical.\n\nDataproc pricing:-\u00a0https:\/\/cloud.google.com\/dataproc\/pricing\nNote: Preemptible instances can be used to lower your Compute Engine costs for Cloud Dataproc clusters, but do not change the way you are billed for the Cloud Dataproc premium.\nA & B are wrong as this scenario does not call for High Availability mode because it handles non-critical experiments.\nD is wrong as local SSDs would cost more; instead, use Preemptible nodes to meet your objective of delivering a cost-effective solution."},{"label":"test_1","q_format":"multiple","q_text":"You have migrated your Hadoop jobs with external dependencies on a Dataproc cluster. As a security requirement, the cluster has been setup using internal IP addresses only and does not have a direct Internet connectivity. How can the cluster be configured to allow the installation of the dependencies? (select 2)","answers":[{"ans":"Setup a SSH tunnel to Internet and route outbound requests through it.","val":false},{"ans":"Use Cloud Dataproc custom images instead of initialization actions to set up job dependencies.","val":true},{"ans":"Setup a SOCKS proxy and route outbound requests through it.","val":false},{"ans":"Setup a NAT Gateway to allow Dataproc cluster to download external dependencies","val":true}],"q_expl":"Correct answers are B & D as the Dataproc cluster has been setup using internal IP addresses only, the external dependencies can either be bundled into a custom image or the internet outbound request can be securely routed using NAT gateway.\nDataproc Init Scripts:-\u00a0https:\/\/cloud.google.com\/dataproc\/docs\/concepts\/configuring-clusters\/init-actions#important_things_to_know\nIf you create a\u00a0Cloud Dataproc\u00a0cluster with internal IP addresses only, attempts to access the Internet in an initialization action will fail unless you have configured routes to direct the traffic through a NAT or a VPN gateway. Without access to the Internet, you can enable Private Google Access, and place job dependencies in Cloud Storage; cluster nodes can download the dependencies from Cloud Storage from internal IPs.\nYou can use Cloud Dataproc custom images instead of initialization actions to set up job dependencies.\nA & C are wrong as they would not provide internet access or ability to install dependencies."},{"label":"test_1","q_format":"single","q_text":"You are developing a software application using Google\u2019s Dataflow SDK, and want to use conditional, for loops and other complex programming structures to create a branching pipeline. Which component will be used for the data processing operation?","answers":[{"ans":"PCollection","val":false},{"ans":"Transform","val":true},{"ans":"Pipeline","val":false},{"ans":"Sink API","val":false}],"q_expl":"PCollection: Represents a collection of data elements that flow through the pipeline.\nTransform: Defines the data processing operations applied to a PCollection. This is where you would use conditional statements, loops, and other complex logic to create branching pipelines.\nPipeline: Represents the entire dataflow job. It orchestrates the execution of the PCollections and Transforms.\nSink API: Defines the output of the pipeline, such as writing data to a storage system or sending data to a downstream system.\n\nThe Transform component provides the flexibility to create complex data processing workflows using various programming constructs, making it the ideal choice for branching pipelines in Dataflow."},{"label":"test_1","q_format":"single","q_text":"You are upgrading your existing (development) Cloud Bigtable instance for use in your production environment. The instance contains a large amount of data that you want to make available for production immediately. You need to design for fastest performance. What should you do?","answers":[{"ans":"Change your Cloud Bigtable instance type from Development to Production, and set the number of nodes to at least 3. Verify that the storage type is HDD.","val":false},{"ans":"Change your Cloud Bigtable instance type from Development to Production, and set the number of nodes to at least 3. Verify that the storage type is SSD.","val":false},{"ans":"Export the data from your current Cloud Bigtable instance to Cloud Storage. Create a new Cloud Bigtable Production instance type with at least 3 nodes. Select the HDD storage type. Import the data into the new instance from Cloud Storage.","val":false},{"ans":"Export the data from your current Cloud Bigtable instance to Cloud Storage. Create a new Cloud Bigtable Production instance type with at least 3 nodes. Select the SSD storage type. Import the data into the new instance from Cloud Storage.","val":true}],"q_expl":"The correct answer is:\nExport the data from your current Cloud Bigtable instance to Cloud Storage. Create a new Cloud Bigtable Production instance type with at least 3 nodes. Select the SSD storage type. Import the data into the new instance from Cloud Storage.\nHere\u2019s why:\nChange instance type to Production: This ensures that the instance is configured for production use with higher availability and performance guarantees.\nSet the number of nodes to at least 3: This provides redundancy and improves performance.\nSelect SSD storage: SSD storage provides much faster read and write speeds compared to HDD storage, which is crucial for performance-critical applications.\nExport and import data: Exporting the data to Cloud Storage allows you to create a new Cloud Bigtable instance with the desired configuration and then import the data into it. This ensures that you have a clean slate for the production instance and can optimize its configuration without affecting the existing data.\nBy following these steps, you can create a Cloud Bigtable instance that is optimized for production use and can handle your large dataset efficiently."},{"label":"test_1","q_format":"single","q_text":"Your company wants to transcribe the conversations between the manufacturing employees at real time. The conversations are recorded using old radio systems in the 8000Hz frequency. They are in English with a short duration of 35-40 secs. You need to design the system inline with Google recommended best practice. How would you design the application?","answers":[{"ans":"Use Cloud Speech-to-Text API in synchronous mode","val":false},{"ans":"Use Cloud Speech-to-Text API in asynchronous mode","val":false},{"ans":"Re-sample the audio using 16000Hz frequency and Use Cloud Speech-to-Text API in synchronous mode","val":false},{"ans":"Re-sample the audio using 16000Hz frequency and Use Cloud Speech-to-Text API in asynchronous mode","val":true}],"q_expl":"The best approach to transcribe the conversations between the manufacturing employees at real time would be to:\nRe-sample the audio using 16000Hz frequency and Use Cloud Speech-to-Text API in asynchronous mode.\nHere\u2019s a breakdown of why this is the recommended approach:\nRe-sampling:\n\nThe 8000Hz frequency of the recorded conversations is relatively low and may not be optimal for accurate speech recognition. Re-sampling the audio to 16000Hz will improve the quality and potentially increase the accuracy of the transcription.\n\nAsynchronous Mode:\n\nReal-time transcription: Using the asynchronous mode of the Cloud Speech-to-Text API allows for near real-time transcription. While it may not be instantaneous, the delay should be minimal for conversations of 35-40 seconds.\nEfficient resource utilization: Asynchronous mode enables the API to handle multiple transcription requests concurrently, making it more efficient for real-time use cases.\nScalability: The asynchronous mode can scale to handle a large number of concurrent transcription requests, ensuring that the system can handle the transcription needs of the manufacturing employees.\n\nSynchronous Mode Considerations:\n\nWhile synchronous mode can be used for real-time transcription, it may introduce latency, especially for longer conversations. This could impact the real-time nature of the transcription process.\nSynchronous mode might not be as efficient in handling a large number of concurrent transcription requests, potentially leading to performance bottlenecks.\n\nAdditional Considerations:\n\nAudio preprocessing: Consider applying additional audio preprocessing techniques, such as noise reduction and echo cancellation, to improve the quality of the recorded audio and enhance transcription accuracy.\nLanguage model customization: If the manufacturing employees use specific jargon or terminology, consider customizing the language model used by the Cloud Speech-to-Text API to improve its accuracy for domain-specific vocabulary.\nError handling: Implement robust error handling mechanisms to address potential issues, such as network failures or API errors, and ensure the system\u2019s reliability.\n\nBy following these recommendations, you can design a transcription system that effectively meets the real-time transcription needs of your manufacturing employees while maximizing accuracy and efficiency."},{"label":"test_1","q_format":"multiple","q_text":"Your company is planning to migrate their historical dataset into BigQuery. This data would be exposed to the data scientists for perform analysis using BigQuery ML. The data scientists would like to know which ML models does the BigQuery ML support. What would be your answer? (select 2)","answers":[{"ans":"Random Forest","val":false},{"ans":"Linear Regression","val":true},{"ans":"K Means","val":false},{"ans":"Principal Component Analysis","val":false},{"ans":"Multiclass logistic regression for Classification","val":true}],"q_expl":"Linear Regression\nMulticlass logistic regression for Classification\n\n\nBigQuery ML currently supports the following machine learning models:\n\nLinear Regression: For predicting numerical values.\nLogistic Regression: For binary classification tasks.\nMulti-class Logistic Regression: For classification tasks with more than two classes.\nK-Means Clustering: For grouping similar data points into clusters.\n\nIt\u2019s important to note that BigQuery ML is a relatively new service, and the list of supported models may evolve over time."},{"label":"test_1","q_format":"single","q_text":"Your infrastructure runs on AWS and includes a set of multi-TB enterprise databases that are backed up nightly on the S3. You need to create a redundant backup to Google Cloud. You are responsible for performing scheduled monthly disaster recovery drills. You want to create a cost-effective solution. What should you do?","answers":[{"ans":"Use Transfer Appliance to transfer the backup files to a Cloud Storage Nearline storage bucket as a final destination.","val":false},{"ans":"Use Transfer Appliance to transfer the backup files to a Cloud Storage Coldline bucket as a final destination.","val":false},{"ans":"Use Storage Transfer Service to transfer the backup files to a Cloud Storage Nearline storage bucket as a final destination.","val":true},{"ans":"Use Storage Transfer Service to transfer the backup files to a Cloud Storage Coldline storage bucket as a final destination.","val":false}],"q_expl":"The data needs to be backed up nightly and accessed monthly, Storage Transfer service can be used to transfer it from S3 and Nearline storage for cost-effective storage solution.\nUse Storage Transfer Service to transfer the backup files to a Cloud Storage Nearline storage bucket as a final destination.\nHere\u2019s why:\n\nCost-Effective: Storage Transfer Service is a managed service that optimizes data transfer costs. It automatically schedules transfers during off-peak hours when network costs are lower.\nNearline Storage: This storage class is ideal for long-term storage of data that is accessed infrequently, like backups. It offers a balance between cost and performance.\nAutomation: Storage Transfer Service can be scheduled to run automatically on a monthly basis, simplifying the disaster recovery drill process.\nSecurity: You can configure the transfer job to use encryption to ensure data security during transit.\n\nWhile Transfer Appliance can be used for large data transfers, it\u2019s generally more suitable for one-time, large-scale data migrations. For recurring, scheduled transfers, Storage Transfer Service is a more efficient and cost-effective solution."},{"label":"test_1","q_format":"single","q_text":"Your company receives a lot of financial data in CSV files. The files need to be processed, cleaned and transformed before they are made available for analytics. The schema of the data also changes every third month. The Data analysts should be able to perform the tasks\n    1. No prior knowledge of any language with no coding\n    2. Provided a GUI tool to build and modify the schema\n    What solution best fits the need?","answers":[{"ans":"Use Dataflow code and provide Data Analysts the access to the code.","val":false},{"ans":"Store the schema externally to be easily modified.","val":false},{"ans":"Use Dataprep with transformation recipes.","val":true},{"ans":"Use Dataproc spark and provide Data Analysts the access to the code.","val":false},{"ans":"Store the schema externally to be easily modified.","val":false},{"ans":"Use DataLab with transformation recipes.","val":false}],"q_expl":"Use Dataprep with transformation recipes.\nDataprep is a self-service data preparation tool that allows users to clean, shape, and blend data without writing code. This aligns perfectly with the requirement of providing data analysts with a GUI tool to build and modify the schema.\nHere\u2019s how Dataprep can be used to address the given scenario:\n\nIngest CSV Files: Dataprep can directly ingest CSV files from various sources, including cloud storage and local files.\nSchema-Less Approach: Dataprep can handle schema changes automatically, making it ideal for scenarios where the schema changes frequently.\nInteractive Data Cleaning and Transformation: Data analysts can use a visual interface to clean, transform, and enrich data. This includes tasks like:\nHandling missing values\nRemoving duplicates\nFormatting data\nCreating new columns\nJoining and merging datasets\n\nRecipe-Based Approach: Data analysts can create and save transformation recipes. These recipes can be reused and shared, making the process efficient and repeatable.\nCollaboration and Version Control: Dataprep supports collaboration and version control, allowing multiple analysts to work on the same data and track changes.\n\nBy using Dataprep, data analysts can efficiently process, clean, and transform financial data without requiring coding knowledge. This empowers them to focus on data analysis and insights, rather than on data preparation tasks."},{"label":"test_1","q_format":"single","q_text":"Your company wants to develop a system to measure the feedback of their products from the reviews posted by people on various Social media platforms. The reviews are mainly text based. You need to do a quick Proof of Concept (PoC) to implement and demo the same. How would you design your application?","answers":[{"ans":"Create and Train a sentiment analysis model using Tensorflow","val":false},{"ans":"Use Cloud Speech-to-Text API for sentiment analysis","val":false},{"ans":"Use Cloud Natural Language API for sentiment analysis","val":true},{"ans":"Use Cloud Vision API for sentiment analysis","val":false}],"q_expl":"Natural Language processing provides pre-model to perform sentiment analysis.\nCloud Natural Language:-\u00a0https:\/\/cloud.google.com\/natural-language\/\nYou can use Cloud Natural Language to extract information about people, places, events, and much more mentioned in text documents, news articles, or blog posts. You can use it to understand sentiment about your product on social media or parse intent from customer conversations happening in a call center or a messaging app. You can analyze text uploaded in your request or integrate with your document storage on Google Cloud Storage.\nA is wrong as building and training a senetiment analysis model using Tensorflow would take time and effort.\nB is wrong as Speech-to-Text API is for audio to text conversion.\nD is wrong as Cloud Vision is for image analysis."},{"label":"test_1","q_format":"single","q_text":"You have multiple Data Analysts who work with the dataset hosted in BigQuery within the same project. As a BigQuery Administrator, you are required to grant the data analyst only the privilege to create jobs\/queries and an ability to cancel self-submitted jobs. Which role should assign to the user?","answers":[{"ans":"User","val":false},{"ans":"Jobuser","val":true},{"ans":"Owner","val":false},{"ans":"Viewer","val":false}],"q_expl":"The role that should be assigned to the user is \u201cJob User\u201c. The Job User role allows the user to submit queries and cancel their own queries but does not provide any access to view or modify datasets or tables within BigQuery.\nAccording to the official documentation of Google Cloud Platform, \u201cJob User\u201c is a predefined role in BigQuery that grants the user the following permissions:\nbigquery.jobs.create: Allows the user to create new jobs, including queries.\nbigquery.jobs.get: Allows the user to view the details of jobs they have created.\nbigquery.jobs.list: Allows the user to list jobs they have created.\nbigquery.jobs.cancel: Allows the user to cancel jobs they have created.\nReferences:\nGoogle Cloud Platform. (n.d.). Predefined roles and permissions. Retrieved from\u00a0https:\/\/cloud.google.com\/bigquery\/docs\/access-control#predefined_roles_and_permissions"},{"label":"test_1","q_format":"single","q_text":"You are selecting a streaming service for log messages that must include final result message ordering as part of building a data pipeline on Google Cloud. You want to stream input for 5 days and be able to query the most recent message value. You will be storing the data in a searchable repository. How should you set up the input messages?","answers":[{"ans":"Use Cloud Pub\/Sub for input. Attach a timestamp to every message in the publisher.","val":true},{"ans":"Use Cloud Pub\/Sub for input. Attach a unique identifier to every message in the publisher.","val":false},{"ans":"Use Apache Kafka on Compute Engine for input. Attach a timestamp to every message in the publisher.","val":false},{"ans":"Use Apache Kafka on Compute Engine for input. Attach a unique identifier to every message in the publisher.","val":false}],"q_expl":"Cloud Pub\/Sub\u00a0does not maintain the order of the messages, and it is recommended to have it timestamped or watermarked from the publisher and ordered using Dataflow.\nPub\/Sub Ordering Messages:-\u00a0https:\/\/cloud.google.com\/pubsub\/docs\/ordering\nHow do you assign an order to messages published from different publishers? Either the publishers themselves have to coordinate, or the message delivery service itself has to attach a notion of order to every incoming message. Each message would need to include the ordering information. The order information could be a timestamp (though it has to be a timestamp that all servers get from the same source in order to avoid issues of clock drift), or a sequence number (acquired from a single source with ACID guarantees). Other messaging systems that guarantee ordering of messages require settings that effectively limit the system to multiple publishers sending messages through a single server to a single subscriber.\nB is wrong as you should not attach a GUID to each message to support the scenario.\nC & D are wrong as you should not use Apache Kafka for this scenario (it is overly complex compared to using Cloud Pub\/Sub, which can support all of the requirements)."},{"label":"test_1","q_format":"multiple","q_text":"Which of these numbers are adjusted by a neural network as it learns from a training dataset? (select 2)","answers":[{"ans":"Continuous features","val":false},{"ans":"Input values","val":false},{"ans":"Weights","val":true},{"ans":"Biases","val":true}],"q_expl":"Correct answers are C & D as weights and bias are the parameters learned by the computer from the training datasets.\nUnderstanding Neural Network:-\u00a0https:\/\/cloud.google.com\/blog\/products\/gcp\/understanding-neural-networks-with-tensorflow-playground\nAs you can see a neural network is a simple mechanism that\u2019s implemented with basic math. The only difference between the traditional programming and neural network is, again, that you let the computer determine the parameters (weights and bias) by learning from training datasets. In other words, the trained weight pattern in our example wasn\u2019t programmed by humans."},{"label":"test_1","q_format":"single","q_text":"An organization wishes to enable real time analytics on user interactions on their web application. They estimate that there will be 1000 interactions per second and wishes to use services, which are ops free. Which combination of services can be used in this case?","answers":[{"ans":"App Engine, Dataproc, DataStudio","val":false},{"ans":"Compute Engine, BigQuery Streaming Inserts, DataStudio","val":false},{"ans":"App Engine, BigQuery Streaming Inserts, DataStudio","val":true},{"ans":"App Engine, Dataflow, DataStudio","val":false}],"q_expl":"To enable real-time analytics on user interactions with a web application, the combination of services that can be used, while being \u201cops free\u201c (requiring minimal operational overhead), is App Engine, BigQuery Streaming Inserts, and DataStudio.\nOption C: App Engine, BigQuery Streaming Inserts, DataStudio.\nHere\u2018s a breakdown of the services and why they fit the requirements:\n1. App Engine: App Engine is a fully managed serverless platform provided by Google Cloud Platform (GCP). It can automatically scale based on incoming traffic and handle the anticipated 1000 interactions per second. App Engine allows you to deploy and run web applications without worrying about infrastructure management, making it \u201cops free.\u201c\n2. BigQuery Streaming Inserts:\u00a0BigQuery is a powerful and scalable data warehouse provided by GCP. With BigQuery Streaming Inserts, you can ingest and process data in real-time as it arrives. By streaming user interaction data directly into BigQuery, you can achieve near real-time analytics and avoid the need for batch processing. BigQuery handles the storage and querying of the data.\n3. DataStudio: DataStudio is a data visualization and reporting tool provided by GCP. It allows you to create interactive and real-time dashboards to visualize the analyzed data from BigQuery. DataStudio integrates seamlessly with BigQuery, enabling you to build real-time analytics dashboards without the need for manual data extraction or transformation.\nBy combining App Engine, BigQuery Streaming Inserts, and DataStudio, you can achieve real-time analytics on user interactions with the web application while minimizing operational overhead.\nReferences:\n1. App Engine:\u00a0https:\/\/cloud.google.com\/appengine\n2. BigQuery Streaming Inserts:\u00a0https:\/\/cloud.google.com\/bigquery\/streaming-data-into-bigquery\n3. DataStudio:\u00a0https:\/\/datastudio.google.com\/"},{"label":"test_1","q_format":"single","q_text":"Your company wants to develop an REST based application for text analysis to identify entities and label by types such as person, organization, location, events, products, and media from within a text. You need to do a quick Proof of Concept (PoC) to implement and demo the same. How would you design your application?","answers":[{"ans":"Create and Train a model using Tensorflow and Develop an REST based wrapper over it","val":false},{"ans":"Create and Train a model using BigQuery ML and Develop an REST based wrapper over it","val":false},{"ans":"Use Cloud Natural Language API and Develop an REST based wrapper over it","val":true},{"ans":"Use Cloud Vision API and Develop an REST based wrapper over it","val":false}],"q_expl":"Correct answer is C as the solution needs to developed quickly, the Cloud Natural Language API can be used to perform text analysis.\nAI Products:-\u00a0https:\/\/cloud.google.com\/products\/ai\/\nCloud Natural Language API reveals the structure and meaning of text by offering powerful machine learning models in an easy-to-use REST API. And with AutoML Natural Language Beta you can build and train ML models easily, without extensive ML expertise. You can use Natural Language to extract information about people, places, events, and much more mentioned in text documents, news articles, or blog posts. You can also use it to understand sentiment about your product on social media or parse intent from customer conversations happening in a call center or a messaging app.\nA & B are wrong as they do not provide quick results.\nD is wrong as Cloud Vision is for image analysis and not text analysis."},{"label":"test_1","q_format":"single","q_text":"You have lot of Spark jobs. Some jobs need to run independently while others can run parallelly. There is also inter-dependency between the jobs and the dependent jobs should not be triggered unless the previous ones are completed. How do you orchestrate the pipelines?","answers":[{"ans":"Cloud Dataproc","val":false},{"ans":"Cloud Scheduler","val":false},{"ans":"Schedule jobs on a single Compute Engine using Cron.","val":false},{"ans":"Cloud Composer","val":true}],"q_expl":"Correct answer is D as Cloud Composer can help create workflows that connect data, processing, and services across clouds, giving you a unified data environment.\nCloud Composer:-\u00a0https:\/\/cloud.google.com\/composer\/\nCloud Composer is a fully managed workflow orchestration service that empowers you to author, schedule, and monitor pipelines that span across clouds and on-premises data centers. Built on the popular Apache Airflow open source project and operated using the Python programming language, Cloud Composer is free from lock-in and easy to use.\nCloud Composer pipelines are configured as directed acyclic graphs (DAGs) using Python, making it easy for users of any experience level to author and schedule a workflow. One-click deployment yields instant access to a rich library of connectors and multiple graphical representations of your workflow in action, increasing pipeline reliability by making troubleshooting easy. Automatic synchronization of your directed acyclic graphs ensures your jobs stay on schedule.\nA is wrong as Google Cloud Dataproc is a fast, easy to use, managed Spark and Hadoop service for distributed data processing. It does not help easy orchestration.\nB is wrong as Cloud Scheduler is a fully managed enterprise-grade cron job scheduler. It is not an orchestration tool.\nC is wrong as it does not help orchestrate the dependency between jobs, but merely schedule them."},{"label":"test_1","q_format":"single","q_text":"Your company is in a highly regulated industry. You have 2 groups of analysts, who perform the initial analysis and sanitization of the data. You now need to provide analyst three secure access to these BigQuery query results, but not the underlying tables or datasets. How would you share the data?","answers":[{"ans":"Export the query results to a public Cloud Storage bucket.","val":false},{"ans":"Create a BigQuery Authorized View and assign a project-level user role to analyst three.","val":true},{"ans":"Assign the bigquery.resultsonly.viewer role to analyst three.","val":false},{"ans":"Create a BigQuery Authorized View and assign an organizational level role to analyst three.","val":false}],"q_expl":"You need to copy or store the query results in a separate dataset and provide authorization to view and\/or use that dataset. The other solutions are not secure. \nBigQuery Authorized Views:-\u00a0https:\/\/cloud.google.com\/bigquery\/docs\/authorized-views \nGiving a view access to a dataset is also known as creating an authorized view in BigQuery. An authorized view allows you to share query results with particular users and groups without giving them access to the underlying tables. You can also use the view\u2019s SQL query to restrict the columns (fields) the users are able to query. \nWhen you create the view, it must be created in a dataset separate from the source data queried by the view. Because you can assign access controls only at the dataset level, if the view is created in the same dataset as the source data, your users would have access to both the view and the data. \nA is wrong as a public Cloud Storage bucket is accessible to all. \nC is wrong as there is no resultsonly viewer role. \nD is wrong as an Organizational role would provide access to the underlying data as well."},{"label":"test_1","q_format":"single","q_text":"Your company is making the move to Google Cloud and has chosen to use a managed database service to reduce overhead. Your existing database is used for a product catalog that provides real-time inventory tracking for a retailer. Your database is 500 GB in size. The data is semi-structured and does not need full atomicity. You are looking for a truly no-ops\/serverless solution. What storage option should you choose?","answers":[{"ans":"Cloud Datastore","val":true},{"ans":"Cloud Bigtable","val":false},{"ans":"Cloud SQL","val":false},{"ans":"BigQuery","val":false}],"q_expl":"Cloud Datastore offers NoOps NoSQL solution which is suited for Semistructured data and ideal for product catalogs.\nStorage Options:-\u00a0https:\/\/cloud.google.com\/storage-options\/\nB & C are wrong as they are not complete NoOps solution. Also Cloud SQL is not suited for Semi Structured data.\nD is wrong as BigQuery is ideal for analytics solution."},{"label":"test_1","q_format":"single","q_text":"A user wishes to generate reports on petabyte scale data using a Business Intelligence (BI) tools. Which storage option provides integration with BI tools and supports OLAP workloads up to petabyte-scale?","answers":[{"ans":"Bigtable","val":false},{"ans":"Cloud Datastore","val":false},{"ans":"Cloud Storage","val":false},{"ans":"BigQuery","val":true}],"q_expl":"BigQuery is fully managed data warehouse and is fast and easy to use on data of any size. With BigQuery, you\u2019ll get great performance on your data, while knowing you can scale seamlessly to store and analyze petabytes more without having to buy more capacity.\nStorage Options:-\u00a0https:\/\/cloud.google.com\/storage-options\/\nA & B are wrong as Bigtable & Datastore are NoSQL solution and not suitable for OLAP data warehouse work loads.\nC is wrong as Cloud Storage provides object storage only."},{"label":"test_1","q_format":"single","q_text":"Your company has assigned fixed number for slots to each project for BigQuery. Each project wants to monitor the number of available slots. How can the monitoring be configured?","answers":[{"ans":"Monitor the BigQuery Slots Used metric.","val":false},{"ans":"Monitor the BigQuery Slots Pending metric.","val":false},{"ans":"Monitor the BigQuery Slots Allocated metric.","val":false},{"ans":"Monitor the BigQuery Slots Available metric.","val":true}],"q_expl":"BigQuery provides 2 metrics for Slots. Slots Allocated to the project and Slots Available for the project.\nBigQuery Metrics:-\u00a0https:\/\/cloud.google.com\/bigquery\/docs\/monitoring#metrics\nTo monitor the number of available slots in BigQuery, you should monitor the \u201cBigQuery Slots Available\u00a0metric\u201c.\nThe \u201cBigQuery Slots Available\u201c metric provides information about the number of slots that are currently available for query execution in BigQuery. This metric is useful for monitoring and managing the slot usage across different projects in your organization.\nBy monitoring this metric, you can track the availability of slots and ensure that projects do not exceed their assigned slot limits. It allows you to proactively manage and optimize the slot allocation for each project, preventing resource contention and performance issues.\nReferences:\n1. BigQuery Monitoring with Cloud Monitoring:\u00a0https:\/\/cloud.google.com\/monitoring\/bigquery-metrics\n2. BigQuery Slots Available metric documentation:\u00a0https:\/\/cloud.google.com\/monitoring\/api\/metrics_gcp#gcp-bigquery\n3. Managing Concurrency in BigQuery:\u00a0https:\/\/cloud.google.com\/blog\/products\/data-analytics\/how-to-manage-concurrency-in-bigquery-to-avoid-quota-exceeded-errors"},{"label":"test_1","q_format":"single","q_text":"Your company is planning to migrate its data first to Google Cloud Storage. You need to keep the contents of this bucket in sync with a new Google Cloud Storage bucket to support a backup storage destination. What is the best method to achieve this?","answers":[{"ans":"Once per week, use a gsutil cp command to copy over newly modified files.","val":false},{"ans":"Use\u00a0gsutil rsync\u00a0commands to keep both locations in sync.","val":true},{"ans":"Use Storage Transfer Service to keep both the source and destination in sync.","val":false},{"ans":"Use\u00a0gsutil -m cp\u00a0to keep both locations in sync.","val":false}],"q_expl":"The data transfer is between on-premises and Google Cloud, the gsutil rsync can be used to keep the source and destination in sync.\ngsutil rsync\u00a0command makes the contents under dst_url the same as the contents under src_url, by copying any missing files\/objects (or those whose data has changed), and (if the -d option is specified) deleting any extra files\/objects. src_url must specify a directory, bucket, or bucket subdirectory.\nA & D are wrong as copy can be used to copy, however there needs to be more handling to keep it in sync.\nC is wrong as the data is not available in an online location."},{"label":"test_1","q_format":"single","q_text":"Your company is planning to host its analytics data in BigQuery. You are required to control access to the dataset with least privilege meeting the following guidelinesEach team has multiple Team Leaders, who should have the ability to create, delete tables, but not delete dataset.Each team has Data Analysts, who should be able to query data, but not modify itHow would you design the access control?","answers":[{"ans":"Grant Team leader group - OWNER and Data Analyst - WRITER","val":false},{"ans":"Grant Team leader group - OWNER and Data Analyst - READER","val":false},{"ans":"Grant Team leader group - WRITER and Data Analyst - READER","val":true},{"ans":"Grant Team leader group - READER and Data Analyst - WRITER","val":false}],"q_expl":"Team leader group should be provider the WRITER access and the Data Analysts should be provided only the reader access. \nBigQuery Dataset Primitive Roles:-\u00a0https:\/\/cloud.google.com\/bigquery\/docs\/access-control#dataset-primitive-roles \nA & D are wrong as Data Analyst should not have the WRITER permissions \nA & B are wrong as Team leader should not have the OWNER permission"},{"label":"test_1","q_format":"single","q_text":"Your infrastructure includes two 100-TB enterprise file servers. You need to perform a one-way, one-time migration of this data to the Google Cloud securely. Only users in Germany will access this data. You want to create the most cost-effective solution. What should you do?","answers":[{"ans":"Use Transfer Appliance to transfer the offsite backup files to a Cloud Storage Regional storage bucket as a final destination.","val":true},{"ans":"Use Transfer Appliance to transfer the offsite backup files to a Cloud Storage Multi-Regional bucket as a final destination.","val":false},{"ans":"Use Storage Transfer Service to transfer the offsite backup files to a Cloud Storage Regional storage bucket as a final destination.","val":false},{"ans":"Use Storage Transfer Service to transfer the offsite backup files to a Cloud Storage Multi-Regional storage bucket as a final destination.","val":false}],"q_expl":"The data is huge it can be transferred using\u00a0Transfer Appliance in a time and cost effective way. Also, as the data is going to be accessed in a single region it can be hosted in a regional bucket.\nStorage Classes:-\u00a0https:\/\/cloud.google.com\/storage\/docs\/storage-classes\nB is wrong as the data is accessed in a single region, it would be more cost effective storing it in a regional bucket.\nC & D are wrong as the data is huge it is more time and cost effective to transfer the data Transfer Appliance."},{"label":"test_1","q_format":"single","q_text":"You are building a data pipeline on Google Cloud. You need to prepare source data for a machine-learning model. This involves quickly deduplicating rows from three input tables and also removing outliers from data columns where you do not know the data distribution. What should you do?","answers":[{"ans":"Write an Apache Spark job with a series of steps for Cloud Dataflow.The first step will examine the source data, and the second and third steps step will perform data transformations.","val":false},{"ans":"Write an Apache Spark job with a series of steps for Cloud Dataproc.The first step will examine the source data, and the second and third steps step will perform data transformations.","val":false},{"ans":"Use Cloud Dataprep to preview the data distributions in sample source data table columns.Write a recipe to transform the data and add it to the Cloud Dataprep job.","val":false},{"ans":"Use Cloud Dataprep to preview the data distributions in sample source data table columns.Click on each column name, click on each appropriate suggested transformation, and then click \u2018Add\u2018 to add each transformation to the Cloud Dataprep job.","val":true}],"q_expl":"The requirements is to prepare\/clean source data, use\u00a0Cloud Dataprep\u00a0suggested transformations to quickly build a transformation job.\nDataprep:-\u00a0https:\/\/cloud.google.com\/dataprep\/\nCloud Dataprep by Trifacta is an intelligent data service for visually exploring, cleaning, and preparing structured and unstructured data for analysis. Cloud Dataprep is serverless and works at any scale. There is no infrastructure to deploy or manage. Easy data preparation with clicks and no code.\nCloud Dataprep automatically identifies data anomalies and helps you to take corrective action fast. Get data transformation suggestions based on your usage pattern. Standardize, structure, and join datasets easily with a guided approach.\nC is wrong as you can simply use the suggested transformations instead of writing custom recipe in Cloud Dataprep\nA & B are wrong as you should not use Apache Spark and Cloud Dataflow or Cloud Dataproc for this scenario."},{"label":"test_1","q_format":"single","q_text":"Your company plans to migrate a multi-petabyte data set to the cloud. The data set must be available 24hrs a day. Your business analysts have experience only with using a SQL interface. How should you store the data to optimize it for ease of analysis?","answers":[{"ans":"Load data into Google BigQuery.","val":true},{"ans":"Insert data into Google Cloud SQL.","val":false},{"ans":"Put flat files into Google Cloud Storage.","val":false},{"ans":"Stream data into Google Cloud Datastore.","val":false}],"q_expl":"To optimize the data for ease of analysis and meet the requirement of 24\/7 availability, the best option would be to\u00a0load the data into Google BigQuery\u00a0(Option A). \nGoogle BigQuery is a fully managed and highly scalable data warehouse service provided by Google Cloud Platform (GCP). It is specifically designed for storing and analyzing large datasets. BigQuery supports a SQL interface, which aligns with the business analysts\u2018 familiarity with SQL. \nBy loading the data into BigQuery, you can take advantage of its distributed architecture and powerful querying capabilities. BigQuery can handle multi-petabyte-scale datasets and provides fast query performance, making it well-suited for large-scale data analysis tasks. It also offers features like automatic scaling, built-in data visualization, and integration with popular business intelligence tools. \nOn the other hand, the other options mentioned: \nInserting data into Google Cloud SQL: While Google Cloud SQL is a managed relational database service, it may not be the optimal choice for storing and analyzing multi-petabyte-scale datasets. It is more suitable for traditional transactional workloads and may have limitations in handling large volumes of data. \nPutting flat files into Google Cloud Storage: While Google Cloud Storage is a scalable and durable object storage service, it doesn\u2018t provide the same level of query performance and analysis capabilities as BigQuery. Analyzing data directly from flat files would require additional processing steps and may not be as efficient as using a dedicated data warehouse service like BigQuery. \nStreaming data into Google Cloud Datastore: Google Cloud Datastore is a NoSQL document database, primarily designed for transactional and operational workloads. It may not be the most suitable choice for large-scale data analysis tasks, especially if the analysts are more comfortable with SQL-based interfaces. \nTherefore, in this scenario, loading the data into Google BigQuery would provide the optimal solution, enabling efficient analysis of the multi-petabyte dataset using a familiar SQL interface. \nReferences: \n1. Google BigQuery:\u00a0https:\/\/cloud.google.com\/bigquery \n2. Google Cloud SQL:\u00a0https:\/\/cloud.google.com\/sql \n3. Google Cloud Storage:\u00a0https:\/\/cloud.google.com\/storage \n4. Google Cloud Datastore:\u00a0https:\/\/cloud.google.com\/datastore"},{"label":"test_1","q_format":"single","q_text":"A company has lot of data sources from multiple systems used for reporting. Over a period of time, a lot data is missing and you are asked to perform anomaly detection. How would you design the system?","answers":[{"ans":"Use Dataprep with Data Studio","val":false},{"ans":"Load in Cloud Storage and use Dataflow with Data Studio","val":false},{"ans":"Load in Cloud Storage and use Dataprep with Data Studio","val":true},{"ans":"Use Dataflow with Data Studio","val":false}],"q_expl":"Dataprep\u00a0provides data cleaning and automatically identifies anomalies in the data. It can integrated with Cloud Storage and BigQuery\nDataprep:-\u00a0https:\/\/cloud.google.com\/dataprep\/\nA is wrong as Dataprep would not be able interact directly with local system.\nB & D are wrong as Cloud Dataflow is a fully-managed service for transforming and enriching data in stream (real time) and batch (historical) modes with equal reliability and expressiveness \u2014 no more complex workarounds or compromises needed. It does not provide anomaly detection."},{"label":"test_1","q_format":"single","q_text":"You are designing storage for event data as part of building a data pipeline on Google Cloud. Your input data is in CSV format. You want to minimize the cost of querying individual values over time windows. Which storage service and schema design should you use?","answers":[{"ans":"Use Cloud Bigtable for storage. Design tall and narrow tables, and use a new row for each single event version.","val":true},{"ans":"Use Cloud Bigtable for storage. Design short and wide tables, and use a new column for each single event version.","val":false},{"ans":"Use Cloud Storage for storage. Join the raw file data with a BigQuery log table.","val":false},{"ans":"Use Cloud Storage for storage. Write a Cloud Dataprep job to split the data into partitioned tables.","val":false}],"q_expl":"Correct answer is A as its an event data (time series) and need to be restricted to individual values over time windows, it is best to use\u00a0Bigtable\u00a0with tall and narrow tables.\nBigtable Time series schema:-\u00a0https:\/\/cloud.google.com\/bigtable\/docs\/schema-design-time-series#use_tall_and_narrow_tables\nFor time series, you should generally use tall and narrow tables. This is for two reasons: Storing one event per row makes it easier to run queries against your data. Storing many events per row makes it more likely that the total row size will exceed the recommended maximum.\nAs an optimization, you can use short and wide tables, but avoid unbounded numbers of events. For example, if you usually need to retrieve an entire month of events at once, the temperature table above is a reasonable optimization\u2014the row is bounded in size to the number of days in a month.\nB is wrong as short and wide tables and are ideal for storing time series data.\nC & D are wrong as you do not need to use GCS\/BQ for this scenario."},{"label":"test_1","q_format":"single","q_text":"You want to display aggregate view counts for your YouTube channel data in Data Studio. You want to see the video tiles and view counts summarized over the last 30 days. You also want to segment the data by the Country Code using the fewest possible steps. What should you do?","answers":[{"ans":"Set up a YouTube data source for your channel data for Data Studio. Set Views as the metric and set Video Title as a report dimension. Set Country Code as a filter.","val":false},{"ans":"Set up a YouTube data source for your channel data for Data Studio. Set Views as the metric and set Video Title and Country Code as report dimensions.","val":true},{"ans":"Export your YouTube views to Cloud Storage. Set up a Cloud Storage data source for Data Studio. Set Views as the metric and set Video Title as a report dimension. Set Country Code as a filter.","val":false},{"ans":"Export your YouTube views to Cloud Storage. Set up a Cloud Storage data source for Data Studio. Set Views as the metric and set Video Title and Country Code as report dimensions.","val":false}],"q_expl":"Correct answer is B as there is no need to export; you can use the existing YouTube data source. Country Code is a dimension because it\u2019s a string and should be displayed as such, that is, showing all countries, instead of filtering.\nData Studio Youtube connector:-\u00a0https:\/\/support.google.com\/datastudio\/answer\/7020432?hl=en\nA is wrong as you cannot produce a summarized report that meets your business requirements using the options listed.\nC & D are wrong as you do not need to export data from YouTube to Cloud Storage; you can simply use the existing YouTube data source."},{"label":"test_1","q_format":"single","q_text":"A company wants to transfer petabyte scale of data to Google Cloud for their analytics, however are constrained on their internet connectivity? Which GCP service can help them transfer the data quickly?","answers":[{"ans":"Transfer appliance and Dataprep to decrypt the data","val":false},{"ans":"Google Transfer service using multiple VPN connections","val":false},{"ans":"gustil with multiple VPN connections","val":false},{"ans":"Transfer appliance and rehydrator to decrypt the data","val":true}],"q_expl":"Correct answer is D as the data is huge it should be transferred using Transfer Appliance and use a Rehydrator to decrypt the data.\nData Rehydration:-\u00a0https:\/\/cloud.google.com\/transfer-appliance\/docs\/2.0\/data-rehydration\nA is wrong as Dataprep does not help is decrypting the data.\nB is wrong as Google Transfer Service does not support importing data from on-premises data center. It only supports online imports.\nC is wrong as the data is huge transferring it with gsutil would take a long time."},{"label":"test_1","q_format":"single","q_text":"Your BigQuery table needs to be accessed by team members who are not proficient in technology. You want to simplify the columns they need to query to avoid confusion. How can you do this while preserving all of the data in your table?","answers":[{"ans":"Train your team members on how to query larger tables.Create a query that uses the reduced number of columns they will access.","val":false},{"ans":"Save this query as a view in a different dataset.Give your team members access to the new dataset and instruct them to query against the saved view instead of the main table.","val":true},{"ans":"Apply column filtering to your table, and restrict the unfiltered view to yourself and those who need access to the full table.Create a copy of your table in a different dataset, and remove the unneeded columns from the copy.","val":false},{"ans":"Have your team members run queries against this copy.","val":false}],"q_expl":"Correct answer is B as the best way to limit and expose number of columns and access is to create a View. With BigQuery, the access can only be controlled on Datasets and Views, but not on tables.\nBigQuery Views:-\u00a0https:\/\/cloud.google.com\/bigquery\/docs\/views-intro\nA is wrong as it is not a feasible solution.\nC is wrong as column filtering cannot be applied to Table and it can be done through Views.\nD is wrong as it is not an ideal solution, as it results in duplication of data. Also, deletion of Columns is not supported."},{"label":"test_1","q_format":"single","q_text":"Your company hosts its data into multiple Cloud SQL databases. You need to export your Cloud SQL tables into BigQuery for analysis. How can the data be exported?","answers":[{"ans":"Convert your Cloud SQL data to JSON format, then import directly into BigQuery","val":false},{"ans":"Export your Cloud SQL data to Cloud Storage, then import into BigQuery","val":true},{"ans":"Import data to BigQuery directly from Cloud SQL.","val":false},{"ans":"Use the BigQuery export function in Cloud SQL to manage exporting data into BigQuery.","val":false}],"q_expl":"Correct answer is B as BigQuery does not provide direct load from Cloud SQL. The data needs to be loaded through Cloud Storage.\nThe advantages when loading data from Cloud SQL to Cloud Storage to BigQuery are:\nCloud storage provides services like resumable uploads, whereas combining the job and data means you\u2018d need to be more careful about managing any issues with jobs, and concerning yourself with transient issues.\nAccording to this\u00a0documentation, using Cloud Storage you can take advantage of long term storage:\nBigQuery loading data:-\u00a0https:\/\/cloud.google.com\/bigquery\/docs\/loading-data"},{"label":"test_1","q_format":"multiple","q_text":"You are using Dataflow for running a real time streaming data processing pipeline. The pipeline currently uses 3 workers and is running on n1-standard-2 compute engine machine types. The pipeline is currently running slow and you want to increase its performance. How can you update the pipeline to improve the performance? (select 2)","answers":[{"ans":"Change\u00a0workerMachineType\u00a0machine type from n1-standard-2 to n1-standard-4","val":true},{"ans":"Move the Dataflow pipeline is a dedicated network","val":false},{"ans":"Modify the\u00a0maxNumWorkers\u00a0parameter to increase the worker nodes","val":true},{"ans":"Change\u00a0workerMachineType\u00a0machine type from n1-standard-2 to n1-standard-1","val":false}],"q_expl":"Correct answers are A & C as the performance of the Dataflow pipeline can be improve by increasing the maximum number of nodes that can be used and upgrading the machine type from n1-standard-2 to n1-standard-4.\nDataflow Execution parameters:-\u00a0https:\/\/cloud.google.com\/dataflow\/docs\/guides\/specifying-exec-params\nB is wrong as network does not have any impact on the Dataflow pipeline performance.\nD is wrong as it reduces the machine configuration and hence performance would be impacted."},{"label":"test_1","q_format":"single","q_text":"Your company is building a package tracking application to track the complete lifecycle of the package. The data is stored in a BigQuery time partitioned table. Over the period of time the data in the table and grown manifold and Data Scientists are complaining of slowness in their package tracking queries. How can the table be modified to improve the performance and maintaining cost effectiveness?","answers":[{"ans":"Import the table data to Bigtable","val":false},{"ans":"Change the partitioned table column from time to date","val":false},{"ans":"Update to table to perform clustering on package id","val":true},{"ans":"Ask the Data Scientists to use LIMIT parameter on the queries","val":false}],"q_expl":"Clustering the data on the package Id can greatly improve the performance. \nBigQuery Clustered Table:-\u00a0https:\/\/cloud.google.com\/bigquery\/docs\/clustered-tables \nClustering can improve the performance of certain types of queries such as queries that use filter clauses and queries that aggregate data. When data is written to a clustered table by a query job or a load job, BigQuery sorts the data using the values in the clustering columns. These values are used to organize the data into multiple blocks in BigQuery storage. When you submit a query containing a clause that filters data based on the clustering columns, BigQuery uses the sorted blocks to eliminate scans of unnecessary data \nIn a table partitioned by a date or timestamp column, each partition contains a single day of data. When the data is stored, BigQuery ensures that all the data in a block belongs to a single partition. A partitioned table maintains these properties across all operations that modify it: query jobs, Data Manipulation Language (DML) statements, Data Definition Language (DDL) statements, load jobs, and copy jobs. This requires BigQuery to maintain more metadata than a non-partitioned table. As the number of partitions increases, the amount of metadata overhead increases. \nAlthough more metadata must be maintained, by ensuring that data is partitioned globally, BigQuery can more accurately estimate the bytes processed by a query before you run it. This cost calculation provides an upper bound on the final cost of the query. \nIn a clustered table, BigQuery automatically sorts the data based on the values in the clustering columns and organizes them in optimally sized storage blocks. You can achieve more finely grained sorting by creating a table that is clustered and partitioned. A clustered table maintains the sort properties in the context of each operation that modifies it. As a result, BigQuery may not be able to accurately estimate the bytes processed by the query or the query costs. When blocks of data are eliminated during query execution, BigQuery provides a best effort reduction of the query costs. \nA is wrong as Bigtable would not be a cost effective option. \nB is wrong as changing the partitioning from time to date would be impact queries on packages. \nD is wrong as LIMIT parameter does limit the amount of data queried."},{"label":"test_1","q_format":"single","q_text":"You have a dataset in BigQuery storing transaction data with details of product and date purchased. Query fired on the data for a product using \u2013dry-run shows that is performs a complete scan. How can the performance of the query be improved?","answers":[{"ans":"Dry run always shows complete scan and the result would be different when the actual query is fired","val":false},{"ans":"Use the limit parameter to limit the data queried","val":false},{"ans":"Set maximum bytes on the query to limit the amount of data queried","val":false},{"ans":"Use Partitioning and clustering on the table","val":true}],"q_expl":"Partitioning and clustering can be effective strategies to improve query performance in BigQuery. Partitioning involves dividing a large table into smaller and more manageable pieces based on a specified column, such as date or region. This allows queries to only scan the relevant partitions, rather than scanning the entire table, which can significantly reduce query time and cost. \nClustering, on the other hand, involves grouping related rows in a table together based on one or more columns, such as product or category. This can improve query performance by reducing the amount of data that needs to be scanned within a partition, since the related data will be physically located closer together. \nTherefore, by utilizing partitioning and clustering, the query can be optimized to scan only the relevant partitions and clusters, which can result in significant performance improvements. \nReference: \nhttps:\/\/cloud.google.com\/bigquery\/docs\/partitioned-tables \nhttps:\/\/cloud.google.com\/bigquery\/docs\/clustered-tables \nBigQuery Cost Best Practices and Clustering:\u00a0\u00a0https:\/\/cloud.google.com\/bigquery\/docs\/best-practices-costs\u00a0\u00a0\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\u00a0 \u00a0 \u00a0\u00a0\u00a0https:\/\/cloud.google.com\/bigquery\/docs\/clustered-tables \nA is wrong as dry-run shows the actual data that would be queried and is one of the best practices to check before the actual query is fired. \nB is wrong as limit does not limit the amount of data queried, but only the results. \nC is wrong as maximum bytes limits the amount of data queried. However, it fails if the bytes is exceeded without any results."},{"label":"test_2","q_format":"multiple","q_text":"Select all true statements about Cloud Composer. (select 2)","answers":[{"ans":"Cloud Composer is a free GCP service.","val":false},{"ans":"Cloud Composer is built on Apache Beam.","val":false},{"ans":"Cloud Composer is built on Apache Airflow.","val":true},{"ans":"Cloud Composer supports Python 3.x","val":true}],"q_expl":"The correct options are:\nCloud Composer is built on Apache Airflow.\nCloud Composer supports Python 3.x.\nExplanation:\nCloud Composer is a managed workflow orchestration service for authoring, scheduling, and monitoring pipelines that are based on the Apache Airflow project. It is not a free service, and users are billed based on usage. Reference: [1]\nCloud Composer is built on Apache Airflow, not Apache Beam. Apache Beam is a unified programming model for batch and streaming data processing, whereas Apache Airflow is a platform to programmatically author, schedule, and monitor workflows. Reference: [2]\nCloud Composer supports Python 3.x, and users can write their workflow code using Python. Reference: [3]\n\nReferences:\n[1] Google Cloud Composer documentation:\u00a0https:\/\/cloud.google.com\/composer\/docs\n[2] Apache Airflow website:\u00a0https:\/\/airflow.apache.org\/\n[3] Google Cloud Composer FAQ:\u00a0https:\/\/cloud.google.com\/composer\/faq"},{"label":"test_2","q_format":"single","q_text":"You have raw data related to retail chain products and purchase records, stored as CSV files in Google Storage. The analytics team wants to use the data for extracting useful statistics for management. They want to run a simple ETL pipeline which analytics will use SQL to do the required transformation and table joins. What is the best approach to achieve this?","answers":[{"ans":"Create external tables on data using BigQuery.Run transformation queries on data then load the output to an internal BigQuery table for reporting and visualization.","val":true},{"ans":"Create external tables on data using BigQuery.Run transformation queries on data then load the output to BigTable for reporting and visualization.","val":false},{"ans":"Import the data from Google Storage to BigQuery.Run transformation queries on data and insert the transformed records to a different BigQuery table.","val":false},{"ans":"Import the data from Google Storage to BigQuery.Run transformation queries on data and export the data to Google Storage. Launch Dataproc cluster and use Hive to query the transformed data.","val":false}],"q_expl":"An external data source (also known as a federated data source) is a data source that allows you to query directly even though the data is not stored in BigQuery. Instead of loading or streaming the data, you create a table that references the external data source.\nQuerying an external data source using a temporary table is useful for one-time, ad-hoc queries over external data, or for extract, transform, and load (ETL) processes.\nIn summary, using external tables in BigQuery is useful for such cases:\n1. Perform ETL operations on data.\n2. Frequently changed data.\n3. Data is being ingested periodically.\nOption B is incorrect because BigTable isn\u2019t a practical (and cheap) approach to report and visualize data.\nOptions C and D are incorrect: Based on Google\u2019s best practices, using external tables for ETL is better than loading data to BigQuery.\nReferences:\nhttps:\/\/cloud.google.com\/bigquery\/external-data-sources\nhttps:\/\/cloud.google.com\/bigquery\/external-table-definition"},{"label":"test_2","q_format":"single","q_text":"You imported a CSV file to BigQuery using API. You checked the data and found that data is skewed and not properly aligned by the table\u2019s columns. What could be the possible issue?","answers":[{"ans":"You need to explicitly specify the delimiter of the file before loading the data.","val":false},{"ans":"The file\u2019s encoding is not UTF-8. You need to convert the file\u2019s encoding and load it again.","val":false},{"ans":"The file\u2019s encoding is not UTF-8. You need to explicitly specify the encoding while loading data.","val":true},{"ans":"File size may exceed 1GB. You need to break the file into smaller files.","val":false}],"q_expl":"BigQuery supports UTF-8 encoding for both nested (repeated) and flat data. BigQuery supports ISO- 8859-1 encoding for flat data only for CSV files.\nBy default, the BigQuery service expects all source data to be UTF-8 encoded. Optionally, if you have CSV files with data encoded in ISO-8859-1 format, you should explicitly specify the encoding when you import your data so that BigQuery can properly convert your data to UTF-8 during the import process.\nOption A is incorrect: While you may explicitly define the delimiter, BigQuery can detect the delimiter.\nOption B is incorrect: There is no need to manually convert the file to UTF-8. BigQuery can convert it for you.\nOption D is incorrect: There is no limit in data size while loading data to BigQuery.\nReference:\u00a0https:\/\/cloud.google.com\/bigquery\/docs\/loading-data-cloud-storage-csv"},{"label":"test_2","q_format":"single","q_text":"You are building an ETL pipeline to process over 25GB of data every day. You decide to use Dataproc for custom Apache Spark jobs for data cleansing and transformation. Jobs running on Dataproc require dependencies that are not installed in Dataproc by default while launching a cluster. Security measures in your company don\u2019t allow resources to connect to the internet and Dataproc cannot install these dependencies online. What would you do in this situation?","answers":[{"ans":"Launch a Compute Engine instance and use it as NAT instance to install the dependencies from.","val":false},{"ans":"Store required dependencies in Google Storage.Install the dependencies to Dataproc nodes using initialization actions.","val":true},{"ans":"Launch a Compute Engine instance.Download the dependencies on a persistent disk using VM instance.Stop the VM instance and use persistent disk for Dataproc cluster to install the dependencies from.","val":false},{"ans":"While launching Dataproc cluster, you may provide the URIs with which you can download the dependencies from.Dependencies will be installed before the cluster is up.","val":false}],"q_expl":"You can create a Cloud Dataproc cluster with internal IP addresses only. However, attempts to access the Internet in an initialization action will fail unless you have configured routes to direct the traffic through a NAT or a VPN gateway. Without having access to the Internet, you can enable Private Google Access, and place job dependencies in Cloud Storage; cluster nodes can download the dependencies from Cloud Storage from internal IPs.\nOptions A & C are incorrect: There is no need to use a Compute Engine instance as answer B provides a better and cheaper solution.\nOption D is incorrect: This is not possible because the cluster will not be able to access the internet if it was set with internal IP addresses only. This also does not comply with security measures that do not allow installing dependencies from the internet.\nReference:\u00a0https:\/\/cloud.google.com\/dataproc\/docs\/concepts\/configuring-clusters\/init-actions"},{"label":"test_2","q_format":"multiple","q_text":"A team of data engineers wants to import a large set of data that contains clickstream logs to Bigtable. They want to test Bigtable before considering it as their final decision. Which of the following conditions should be taken into consideration for successful testing? (select 3)","answers":[{"ans":"You need to wait at least 20 minutes if you scaled up the instance before running the test.","val":false},{"ans":"Scale up the instance just before the test starts.","val":true},{"ans":"Run a heavy pre-test for several minutes before the test starts.","val":false},{"ans":"Don\u2019t use less than 300GB of test data.","val":true},{"ans":"The test should take no longer than 10 minutes.","val":false},{"ans":"Use the development instance for testing.","val":true}],"q_expl":"The correct options for successful testing of Bigtable with a large set of clickstream logs are:\n1. Scale up the instance just before the test starts.\n2. Don\u2018t use less than 300GB of test data.\n3. Use the development instance for testing.\nExplanation:\nScaling up the instance just before the test starts will ensure that the instance has enough resources to handle the workload during the test. Waiting for 20 minutes is not necessary according to the official documentation of Google Cloud Bigtable (1).\nUsing less than 300GB of test data may not give an accurate representation of how Bigtable performs with large data sets. This is because Bigtable is designed to handle large data sets efficiently (2).\nThe development instance is a low-cost, low-performance option for testing and development purposes, making it a suitable choice for testing before committing to the production instance (3).\nReferences:\n1. Google Cloud Bigtable Documentation:\u00a0https:\/\/cloud.google.com\/bigtable\/docs\/instances#resizing_instances\n2. Google Cloud Bigtable Documentation:\u00a0https:\/\/cloud.google.com\/bigtable\/docs\/introduction\n3. Google Cloud Bigtable Pricing:\u00a0https:\/\/cloud.google.com\/bigtable\/pricing"},{"label":"test_2","q_format":"multiple","q_text":"A company uses Bigtable to store session logs generated from its web service. The security team queries these logs to detect possible DDoS (Distributed Denial of Service) attacks coming from specific regions. Lately, the security team informed that they face a slow performance as opposed to wait was expected when Bigtable was considered as data storage for session logs. Which of the following are the possible reasons? (select 2)","answers":[{"ans":"The rows in the tables have a large data size.","val":true},{"ans":"Data is over 300GB in size.","val":false},{"ans":"The instance doesn\u2019t have enough nodes.","val":true},{"ans":"The instance uses HDD storage type.","val":false}],"q_expl":"There are several factors that can cause Cloud Bigtable to perform more slowly than expected:\nThe table\u2019s schema is not designed correctly. To get good performance from Cloud BigTable, it\u2019s essential to design a schema that makes it possible to distribute reads and writes evenly across each table.\nThe workload isn\u2019t appropriate for Cloud BigTable. If you test with a small amount (< 300 GB) of data, or if you test for a very short period of time (seconds rather than minutes or hours), Cloud BigTable won\u2018t be able to balance your data in a way that gives you good performance. The rows in your Cloud Bigtable contain large amounts of data. You can read and write a larger amount of data per row, but increasing the amount of data per row will also reduce the number of rows per second. The rows in your Cloud Bigtable contain a very large number of cells. It takes time for Cloud Bigtable to process each cell in a row.\nAlso, each cell adds some overhead to the amount of data that\u2018s stored in your table and sent over the network. The Cloud Bigtable cluster doesn\u2018t have enough nodes.\nIf your Cloud Bigtable cluster is overloaded, adding more nodes can improve the performance. The Cloud Bigtable cluster was scaled up or scaled down recently. After you change the number of nodes in a cluster, it can take up to 20 minutes under load before you see an improvement in the cluster\u2018s performance. The Cloud Bigtable cluster uses HDD disks. In most cases, your cluster should use SSD disks, which have significantly better performance than HDD disks. The Cloud Bigtable instance is a development instance. The performance of development instance is equivalent to an instance with one single-node cluster, it will not perform as well as a production instance. There are issues with the network connection. Network issues can reduce throughput and cause reads and writes to take longer than usual.\nReference:\u00a0https:\/\/cloud.google.com\/bigtable\/docs\/performance"},{"label":"test_2","q_format":"single","q_text":"You have over 2,000 video clips with dialog scenes and you need to transcribe the dialog to text. Since transcribing this amount of clips can be time-consuming, you want to find a product in Google Cloud which can achieve this instead. Which of the following is that best for this scenario?","answers":[{"ans":"AutoML Vision API.","val":false},{"ans":"Cloud Natural Language.","val":false},{"ans":"Machine Learning Engine.","val":false},{"ans":"Cloud Speech-to-Text.","val":true}],"q_expl":"Cloud Speech-to-Text is a service to generate captions from videos by detecting the speaker\u2019s language and speech. Google Cloud Speech-to-Text enables developers to convert audio to text by applying powerful neural network models in an easy-to-use API. The API recognizes 120 languages and variants to support your global user base. You can enable voice command-and-control, transcribe audio from call centers, and more. It can process real-time streaming or prerecorded audio, using Google\u2019s machine learning technology.\nOption A is incorrect: AutoML Vision API is a service to recognize and derive insights from images by either using pre-trained models or training a custom model based on a set of photographic.\nOption B is incorrect: Cloud natural language service is used to derive insights from unstructured text, revealing the meaning of the documents and categorize articles. It won\u2019t help in extracting captions from videos.\nOption C is incorrect: Machine Learning Engine is a managed service that allows developers and scientists to build their own models and run them in production. This means you have to build your own model to generate text from videos that needs much effort and experience to build such a model. So, it\u2019s not a practical solution for this scenario.\nReferences:\nhttps:\/\/cloud.google.com\/speech-to-text\/\nhttps:\/\/cloud.google.com\/natural-language\/\nhttps:\/\/cloud.google.com\/ml-engine\/\nhttps:\/\/cloud.google.com\/vision"},{"label":"test_2","q_format":"multiple","q_text":"You are building a model using TensorFlow. Upon training the model, the results show that the model could return 73% true positives. When you tested the model with a set derived from real data. You noticed a decrease in true positive returns to 65%. You need to tune the model for better prediction. What would you do? (select 2)","answers":[{"ans":"Increase feature parameters.","val":false},{"ans":"Increase regularization.","val":true},{"ans":"Decrease feature parameters.","val":true},{"ans":"Decrease regularization.","val":false}],"q_expl":"Overfitting happens when a model performs well on a training set, generating only a small error while giving the wrong output for the test set. This happens because the model is only picking up specific features input found in the training set instead of picking out the general features of the given training set. \nTo solve overfitting, the following would help in improving the model\u2019s quality: \n1. Increase the number of examples, the more data a model is trained with, the more use cases the model can be training on and better improves its predictions. \n2. Tune hyperparameters are related to the number and size of hidden layers (for neural networks), and regularization, which means using techniques to make your model simpler such as using a dropout method to remove neuron networks or adding \u201cpenalty\u201d parameters to the cost function. \n3. Remove features by removing irrelevant features. Feature engineering is a wide subject and feature selection is a critical part of building and training a model. Some algorithms have built-in feature selection, but in some cases, data scientists need to cherry-pick or manually select or remove features for debugging and finding the best model output. \nFrom the brief explanation, to solve the overfitting problem in the scenario, you need to: \nIncrease the training set. \nDecrease features parameters. \nIncrease regularization. \nReference:\u00a0https:\/\/cloud.google.com\/solutions\/building-a- serverless-ml-model"},{"label":"test_2","q_format":"single","q_text":"A medical facility is building a machine learning model to predict Addison disease, an uncommon disorder in which the body fails to produce enough amount of certain hormones. Since the disease is rare and only found in every 1 out of 100,000 people, the model should be measured to ensure that the model returns true positives while scanning a patient\u2019s profile with Addison disease signs. Which of the following measures should be used to evaluate the accuracy of the model?","answers":[{"ans":"Dropout Regularization","val":false},{"ans":"Precision","val":false},{"ans":"Gradient Descent","val":false},{"ans":"Recall","val":true}],"q_expl":"Dropout Regularization: It is a regularization method to remove a random selection of the fixed number of units in a neural network layer. More units dropped out, the stronger the regularization.\nPrecision is the formula to check how accurate the model is when most of the output are positives. In other words, if most of the output is yes.\nGradient Descent: It is an optimization algorithm to find the minimal value of a function. Gradient descent is used to find the minimal RMSE or cost function.\nRecall: It is the formula to check how accurate the model is when most of the output are negatives. In other words, if most of the output is no.\nFrom the explanation, options A & C are unrelated so they are incorrect.\nSince very few cases are positively diagnosed with Addison disease, recall formula should be used to calculate the accuracy of the model. So, option D is the correct answer.\nReferences:\nhttps:\/\/developers.google.com\/machine-learning\/crash-course\/classification\/ precision-and-recall\nhttps:\/\/en.wikipedia.org\/wiki\/Gradient_descent\nhttps:\/\/developers.google.com\/machine-learning\/glossary\/"},{"label":"test_2","q_format":"multiple","q_text":"You are building a machine learning model using TensorFlow. The model aims to predict the next earthquake\u2019s locations, approximate time and Richter scale based on data records since 1913. The model needs tuning each number of epochs on training data for higher accuracy. Which of the variables are used for hyperparameter tuning? (select 2)","answers":[{"ans":"Number of features","val":false},{"ans":"Number of hidden layers","val":true},{"ans":"Number of nodes in hidden layers","val":true},{"ans":"Weight values","val":false}],"q_expl":"Hyperparameters are the variables that govern the training process itself. For example, part of setting up a deep neural network is deciding how many hidden layers of nodes to use between the input layer and the output layer, and how many nodes each layer should use. These variables are not directly related to the training data but these are configuration variables. Note that parameters change during a training job, while hyperparameters are usually constant during a job.\nOption A is incorrect: Numbers of features are set by feature engineering, not hyperparameter tuning.\nOption D is incorrect: Weight values are set while training the model.\nReference:\u00a0https:\/\/cloud.google.com\/ml-engine\/docs\/tensorflow\/hyperparameter-tuning-overview"},{"label":"test_2","q_format":"single","q_text":"Data analysts in your company are looking for a tool that can be used to check the data tables uploaded by showing a review of the data rows and some of the useful stats on columns such as missing cells, expected data type, and possible pattern. The tool should provide a user-friendly and easy-to-use UI for data analysts. Which of the following products would you recommend for this scenario?","answers":[{"ans":"Dataflow","val":false},{"ans":"BigQuery","val":false},{"ans":"Dataproc","val":false},{"ans":"Cloud Dataprep","val":true}],"q_expl":"Cloud Dataprep is an intelligent data service for visually exploring, cleaning, and preparing structured and unstructured data for analysis, reporting, and machine learning.\nBecause Cloud Dataprep is serverless and works at any scale, there is no infrastructure to deploy or manage. Your next ideal data transformation is suggested and predicted with each UI input, so you don\u2019t have to write code.\nWith the automatic schema, datatype, possible joins, and anomaly detection, you can skip time-consuming data profiling and focus on data analysis.\nOption A is incorrect: Using Dataflow means building a workflow pipeline which can be complicated and time-consuming for this scenario.\nOption B is incorrect: BigQuery does not provide stats required in the scenario by default. You need to run and schedule queries to return such stats.\nOption C is incorrect: Dataproc is a complicated service for data profiling comparing to Dataprep.\nReference:\u00a0\u00a0https:\/\/cloud.google.com\/dataprep\/"},{"label":"test_2","q_format":"single","q_text":"You have a compute engine virtual instance hosting your WordPress blog on the cloud. You\u2019ve scheduled daily snapshots for your VM\u2019s persistent disk. One day, you faced an issue and the blog crashed due to incompatible PHP configuration while trying to upgrade to a newer version, so you need to recover the persistent disk from a snapshot for the previous day. How would you achieve this?","answers":[{"ans":"Create a replacement instance directly by selecting the snapshot from the list of daily snapshots available.","val":true},{"ans":"Create a new compute instance with the same exact machine type as the one in production which the snapshots were created from before.","val":false},{"ans":"Create a persistent disk using the snapshot to be restored from.","val":false},{"ans":"Attach the persistent disk to the compute engine instance.","val":false},{"ans":"You need to create a persistent disk from the snapshot to be restored from. Then, create a new compute engine instance and attach it to the restored persistent disk.","val":false},{"ans":"Export one of the snapshots to be used for recovery to Google Storage.","val":false}],"q_expl":"Google Cloud supports easy snapshot restoration to a persistent disk as well as restoring a book disk snapshot to create a new VM instance. You can simply create a replacement instance directly by selecting the snapshot from the list of snapshots available.\nReference:\u00a0https:\/\/cloud.google.com\/compute\/docs\/disks\/restore-and-delete-snapshots"},{"label":"test_2","q_format":"single","q_text":"A company wants to migrate its on-premise MySQL relational database to the cloud. The company is looking for a cost-effective solution that grants the same capabilities as its on-premise database. The company mentions that performance is not an issue and latency is acceptable. However, their main demand is high availability with minimum costs. What would you do in this situation?","answers":[{"ans":"Use BigTable with 10-node cluster.","val":false},{"ans":"Use Cloud Spanner multi-regional instance with multiple nodes.","val":false},{"ans":"Use Cloud SQL with read replicas.","val":false},{"ans":"Use Cloud SQL with fail-over replicas.","val":true}],"q_expl":"The failover replica in Cloud SQL is configured with the same database flags, users (including root) and passwords, authorized applications and networks, and databases as the primary instance. If a High-availability-configured instance becomes unresponsive, Cloud SQL automatically switches to serving data from the failover replica. This is called a failover.\nOption A is incorrect: BigTable is a NoSQL table.\nOption B is incorrect: Cloud Spanner can be an expensive approach while the scenario is seeking a cost-efficient alternative for their on-premise database.\nOption C is incorrect: A read replica is a copy of the master that reflects changes to the master instance in almost real-time. The main purpose of reading replicas is for additional read capacity for analytics. Read replicas are NOT for failure recovery in case the primary database is out of service.\nReferences:\nhttps:\/\/cloud.google.com\/sql\/docs\/mysql\/high-availability\nhttps:\/\/cloud.google.com\/sql\/docs\/mysql\/replication"},{"label":"test_2","q_format":"single","q_text":"You are building a machine learning classification model using TensorFlow. You trained the model by using 70% of the total set available for training, validation and testing. After testing the model, AUC returned from the test results was 0.68. The main issue here is due to overfitting. You want to increase the AUC for better accuracy of results. What should you do?","answers":[{"ans":"Increase regularization.","val":true},{"ans":"Reduce samples used for training.","val":false},{"ans":"Reduce regularization.","val":false},{"ans":"Increase feature parameters.","val":false}],"q_expl":"AUC stands for \u201cArea under the ROC Curve.\u201c That is, AUC measures the entire two-dimensional area underneath the entire ROC curve (think integral calculus) from (0,0) to (1,1):\n\nAUC provides an aggregate measure of performance across all possible classification thresholds. One way of interpreting AUC is as the probability that the model ranks a random positive example more highly than a random negative example. AUC ranges in value from 0 to 1. A model whose predictions are 100% wrong has an AUC of 0.0; one whose predictions are 100% correct has an AUC of 1.0.\nThe problem in this scenario is due to overfitting. To solve the overfitting problem, you need to:\n1. Increase the training set.\n2. Decrease features parameters.\n3. Increase regularization.\nReference:\u00a0https:\/\/developers.google.com\/machine-learning\/crash-course\/classification\/roc-and-auc"},{"label":"test_2","q_format":"single","q_text":"You have an on-premise MySQL database that you have been asked to move to Google Cloud. Users should run SQL queries to fetch data from the database. Your solution should be cost-effective and allow increasing read capacities in the future. Which of the following Google Cloud product is the best for this scenario?","answers":[{"ans":"Cloud Storage","val":false},{"ans":"Cloud Spanner","val":false},{"ans":"Cloud SQL","val":true},{"ans":"Cloud Datastore","val":false},{"ans":"BigTable","val":false}],"q_expl":"Cloud SQL is a fully-managed database service that makes it easy to set up, maintain, manage, and administer your relational PostgreSQL, MySQL, and SQL Server databases in the cloud.\nOption A is incorrect: Google Storage is blob storage. It does not work as an RDMS.\nOption B is incorrect: Cloud Spanner is a very sophisticated and expensive approach for this scenario. Cloud SQL is enough to cover the requirements.\nOption D is incorrect: Datastore is a schemaless NoSQL database. Migration is from a structured SQL database so Datastore is not a viable choice.\nReference:\u00a0https:\/\/cloud.google.com\/sql\/"},{"label":"test_2","q_format":"single","q_text":"A stock market company receives real-time updates from different stock prices in the USA. The company seeks a solution that can use stock price data for real-time analysis. The solution should allow high throughput to allow queries to run and return the required results with minimum latency. The solution should also be scaled out for more performance. Which of the following products is the best solution in this scenario?","answers":[{"ans":"BigTable","val":true},{"ans":"BigQuery","val":false},{"ans":"Datastore","val":false},{"ans":"Cloud Spanner","val":false}],"q_expl":"Cloud BigTable is a petabyte-scale, fully managed NoSQL database service for large analytical and operational workloads. Under a typical workload, Cloud BigTable delivers highly predictable performance. When everything is running smoothly, a typical workload can achieve the following performance for each node in the Cloud Bigtable cluster, depending on which type of storage the cluster uses:\n\nIn general, a cluster\u2019s performance increases linearly as you add nodes to the cluster. For example, if you create an SSD cluster with 10 nodes, the cluster can support up to 100,000 rows per second for a typical read-only or write-only workload, with 6 ms latency for each read or write operation.\nOption B is incorrect: BigQuery doesn\u2019t provide the high throughput and low latency competent to Bigtable. Moreover, you are unable to increase BigQuery\u2019s performance, as opposed to Bigtable which you can add more nodes for linear performance return.\nOption C is incorrect: Datastore is not built for storing and reading huge data volumes as required in this scenario. Datastore is designed for web applications of a small scale.\nOption D is incorrect: Cloud Spanner does not guarantee the same performance and low latency as BigTable.\nReference:\u00a0https:\/\/cloud.google.com\/bigtable\/docs\/performance"},{"label":"test_2","q_format":"single","q_text":"You have an on-premise relational database that you want to migrate to Google Cloud. You choose Cloud Spanner for importing the database. You want users to be fully benefited from Cloud Spanner by ensuring the best performance. Which of the following should be taken into consideration while migrating your tables to Cloud Spanner?","answers":[{"ans":"Use version 4 UUID as primary keys for your tables. Keep the original primary keys for legacy queries.","val":true},{"ans":"Make sure your tables primary keys are monotonically increased.","val":false},{"ans":"Use UNIX timestamp as a primary key for your tables. Keep the original primary keys for legacy queries.","val":false},{"ans":"Use the combination of a timestamp and a primary key (pk) as #timestamp-pk.","val":false}],"q_expl":"Cloud Spanner uses compute nodes to read and write data. The data of tables is stored lexicographically by the primary key. Data is distributed among multiple storage \u201csplits.\u201d\n\nThis is the reason why choosing the right primary key for Cloud Spanner is important for performance. If the primary key is monotonic, it leads to storing table data to one storage split, which in return leads to compute nodes to hit the same storage split for reading & writing. A good primary key is a key that helps in distributing data evenly among different storage splits.\nOption A suggests using version 4 UUID as a primary key. A version 4 UUID or a universally unique identifier is a 128-bit number used to identify information in computer systems. An example of a UUID is \u201c81c96908-6a8f-46b2-bc16-3ee4c5376182\u201d which consists of 32 hex characters.\nUUIDs generate widely unique and diverse keys that allow potential primary keys, so it can be a good choice to consider generating primary keys for records in Cloud Spanner.\nOption B is incorrect: As mentioned, monotonic primary keys will lead to performance issues while reading & writing data to only one storage split.\nOption C is incorrect: UNIX timestamps are monotonic since only right-most digits are changing while left-most digits will be the same in almost all cases, which leads to a performance issue.\nOption D is incorrect: Combining timestamp with primary key as #timestamp-pk will lead to having PK combination with the left-most characters being the same for all primary keys.\nReference:\u00a0https:\/\/en.wikipedia.org\/wiki\/Universally_unique_identifier"},{"label":"test_2","q_format":"single","q_text":"The data science team in your company is developing a forecast model using TensorFlow. They are looking for the resources to develop and test the model on. It was suggested to use Google Cloud for this project. As a data engineer, you have been asked to prepare the required resources for the data science team, taking into consideration the costs of the resources used. Which of the following machine types is the best for this scenario?","answers":[{"ans":"TPU","val":true},{"ans":"Standard CPU","val":false},{"ans":"High CPU","val":false},{"ans":"GPU","val":false}],"q_expl":"Google has built the Tensor Processing Unit (TPU) in order to make it possible for data scientists to achieve business and research breakthroughs ranging from network security to medical diagnoses. Cloud TPU is the custom-designed machine learning ASIC that powers Google products like Translate, Photos, Search, Assistant, and Gmail.\nAs for cost, below is a benchmark for a cost comparison between TPU & GPU types:\n\nSo, for this scenario, using the TPU machine type is the recommended type to build Tensorflow models on.\nOptions B and C are incorrect: CPU will not provide the best performance for TensorFlow as for other machine types.\nOption D is incorrect: GPU is more expensive as compared to TPU and will not perform as fast.\nReferences:\nhttps:\/\/cloud.google.com\/tpu\/\nhttps:\/\/cloud.google.com\/gpu\/\nhttps:\/\/cloud.google.com\/blog\/products\/ai-machine-learning\/what-makes-tpus-fine-tuned-for-deep-learning"},{"label":"test_2","q_format":"single","q_text":"Your company has hired an external consultant to help import its relational data into BigQuery for analysis. The consultant mentions the data needs to be denormalized in BigQuery. What are the two benefits of using denormalized data structures in BigQuery?","answers":[{"ans":"1. Reduces the amount of data processed. 2.  Reduces the amount of storage required.","val":false},{"ans":"1. Increases query speed. 2. Makes queries simpler.","val":true},{"ans":"1. Reduces the amount of storage required. 2. Increases query speed.","val":false},{"ans":"1. Reduces the amount of data processed.  2.Increases query speed.","val":false}],"q_expl":"Correct answer is B as Denormalization help increase query speed without the need to join tables with simpler queries.\n1. Increases query speed:\n\nDenormalization reduces the need for joins,\u00a0which can be expensive operations in BigQuery.\u00a0Joins involve bringing together data from multiple tables based on matching criteria,\u00a0and they can be computationally intensive,\u00a0especially for large datasets.\nBy storing frequently accessed data together in denormalized tables,\u00a0you eliminate the need for these joins,\u00a0allowing for faster retrieval of information.\u00a0This is because BigQuery can scan a single denormalized table much more efficiently than performing multiple joins across separate normalized tables.\n\n2. Makes queries simpler:\n\nDenormalized tables can eliminate the need to write complex joins,\u00a0making queries easier to write and understand.\u00a0This is especially beneficial for analysts who are not familiar with SQL or BigQuery,\u00a0as they can focus on the core logic of their analysis without getting bogged down in complex join conditions.\nDenormalized tables can also make it easier to filter and aggregate data,\u00a0as the relevant columns are already present in the same table.\u00a0This can streamline the query writing process and improve the clarity of the analysis.\n\nWhile denormalization can offer these advantages, it\u2019s important to consider the potential drawbacks as well:\n\nIncreased storage requirements:\u00a0By duplicating data across multiple tables,\u00a0denormalization can lead to larger data sizes and potentially higher storage costs.\nIncreased complexity in maintaining data consistency:\u00a0When data changes in one place,\u00a0it may need to be updated in multiple denormalized tables,\u00a0which can increase the risk of inconsistencies and require additional development effort.\n\nBigQuery Managing Data:- https:\/\/cloud.google.com\/solutions\/bigquery-data-warehouse#managing_data\nA, C & D are wrong as Denormalization requires more storage, and also increased the amount of data processed."},{"label":"test_2","q_format":"multiple","q_text":"As a Data Engineer, you are responsible for machine learning workflows. Select all true statements. (select 2)","answers":[{"ans":"If you want to predict one of two classes, you should choose a binary classification model, such as Logistic Regression or Decision Trees.","val":true},{"ans":"It\u2018s best practice to train the model with 20% of all available data, and test the model on the rest of the data.","val":false},{"ans":"If you want to predict a continuous value, you should choose a regression model.","val":true},{"ans":"In the data preparation step, you should select feature columns like randomly assigned identifier with a unique value for each row.","val":false}],"q_expl":"Binary classification models are used when the target variable has only two possible outcomes, such as yes\/no or true\/false. Logistic Regression and Decision Trees are popular binary classification models. Reference: [1]\nRegression models are used when the target variable is continuous and can take any value within a range, such as a price or a temperature. Linear Regression and Random Forest Regression are popular regression models. Reference: [2]\nSplitting the data into a training set and a test set is a common practice in machine learning, but the split ratio depends on the size and quality of the data. There is no fixed rule for the split ratio, and it can vary from 60\/40 to 90\/10 or even higher. Reference: [3]\nFeature selection is a critical step in machine learning, but selecting a randomly assigned identifier with a unique value for each row is not a useful feature for the model. It is usually ignored during feature selection or removed as it does not contribute to the prediction. Reference: [4]\nHere\u2019s why the other statements are not entirely accurate:\n\nB. It\u2019s best practice to train the model with 20% of all available data, and test the model on the rest of the data.\nWhile a common split is 80% training and 20% testing, the ideal ratio can vary depending on the dataset size and complexity. It\u2019s important to experiment and evaluate different splits using techniques like cross-validation.\n\nD. In the data preparation step, you should select feature columns like randomly assigned identifier with a unique value for each row.\nThese identifier columns (often called IDs) are typically not used as features for model training. They might be helpful for tracking data or merging datasets, but they usually don\u2019t contribute meaningful information to the model\u2019s learning process. In fact, including them might even harm model performance.\n\nReferences:\n[1] Binary Classification:\u00a0https:\/\/en.wikipedia.org\/wiki\/Binary_classification\n[2] Regression Analysis:\u00a0https:\/\/en.wikipedia.org\/wiki\/Regression_analysis\n[3] Train\/Test Split:\u00a0https:\/\/towardsdatascience.com\/train-test-split-and-cross-validation-in-python-80b61beca4b6\n[4] Feature Selection:\u00a0https:\/\/en.wikipedia.org\/wiki\/Feature_selection"},{"label":"test_2","q_format":"single","q_text":"A system managed by your company is using Datastore as a NoSQL database for data storage. In the last meeting, your team decided that a compulsory daily backup should be done for Datastore for future recovery in case of data loss or uncontrolled manipulation. You are tasked to implement a way to do daily backups of the data in Datastore. What would you do?","answers":[{"ans":"From Google Cloud console, you can schedule daily backups specifying which Storage bucket and path location to be exported to.","val":false},{"ans":"Import data to BigQuery, run a query to select data for the past 24 hours and export the results to Google Storage.","val":false},{"ans":"Create a BigTable cluster to be used as a backup database. Import whole data from Datastore to BigTable every day.","val":false},{"ans":"Using gcloud command, run a cron job to export Datastore data to Google Storage. You may import the data later using gcloud when needed.","val":true}],"q_expl":"The best approach to implement daily backups for your Datastore data is:\nD. Using gcloud command, run a cron job to export Datastore data to Google Storage. You may import the data later using gcloud when needed.\nHere\u2019s why this option is the most suitable:\n\nNative Datastore Backup:\u00a0Google Cloud offers built-in functionality for exporting Datastore data to Google Cloud Storage (GCS). This eliminates the need for complex workarounds or additional services like BigQuery or BigTable.\nScheduling with Cron Jobs:\u00a0Cron jobs allow you to automate tasks at specific intervals. In this case, you can set up a cron job that triggers the gcloud command daily to export the Datastore data to a designated GCS bucket.\nEfficient Restoration:\u00a0GCS provides a cost-effective and scalable storage solution for your backups. You can restore the data from GCS back to Datastore using the gcloud command when needed.\n\nLet\u2019s analyze why the other options are not as effective for daily Datastore backups:\n\nA. Google Cloud Console:\u00a0While the console allows manual Datastore exports, it\u2019s not suitable for automating daily backups. You\u2019d need to perform the export manually every day, which wouldn\u2019t be a reliable or efficient solution.\nB. BigQuery and Data Export:\u00a0Importing data to BigQuery, filtering it for the past 24 hours, and then exporting it to GCS is a more complex approach. It involves additional services and potentially unnecessary data processing steps compared to a direct Datastore export.\nC. BigTable Backup:\u00a0BigTable is a NoSQL database service like Datastore, but it\u2019s not designed specifically for backups. Daily imports of the entire Datastore dataset into BigTable would be resource-intensive and less efficient than a direct export to GCS.\n\nTherefore, using gcloud commands and cron jobs to automate daily Datastore backups to Google Cloud Storage is the recommended path for reliable and efficient data protection.\nReference:\u00a0https:\/\/cloud.google.com\/datastore\/docs\/export-import-entities"},{"label":"test_2","q_format":"single","q_text":"The security team in your company asked to apply the following rules: Data on-premise and on the cloud should be encrypted at all times. Encryption is done using 256-bit AES keys provided by the security team. Keys should be rotated every 72 days. You use Google Storage to store raw and transformed data. As of the rules above, data should be encrypted when written to Google Storage. As a data engineer, what would you do to satisfy the security team\u2019s requirement?","answers":[{"ans":"Supply the encryption key provided by the security team and reference it as part of the API service calls to encrypt data in Cloud Storage.","val":false},{"ans":"Upload encryption key provided by the security team to Cloud Key Management Service (KMS) and use the key to encrypt data while writing to Google Storage.","val":true},{"ans":"Create symmetric keys using Cloud Key Management Service (KMS) and use them to encrypt data while writing to Google Storage. Create new keys every 72 days.","val":false},{"ans":"Create asymmetric keys using Cloud Key Management Service (KMS) and use them to encrypt data while writing to Google Storage. Create new keys every 72 days.","val":false}],"q_expl":"The most secure and recommended approach to meet the security team\u2019s requirements for encrypting data in Google Storage is:\nB. Upload encryption key provided by the security team to Cloud Key Management Service (KMS) and use the key to encrypt data while writing to Google Storage.\nHere\u2019s why this option aligns with the security team\u2019s requirements:\n\nSecurity Team\u2019s Keys:\u00a0It leverages the 256-bit AES keys provided by the security team, ensuring adherence to their established security policies.\nCloud KMS for Key Management:\u00a0Uploading the keys to Cloud KMS centralizes key management and access control. Cloud KMS offers robust features for key rotation and access control, ensuring secure storage and usage of the encryption keys.\nData Encryption at Rest:\u00a0Using the keys from Cloud KMS to encrypt data while writing to Google Storage fulfills the requirement of data encryption at rest.\n\nLet\u2019s explore why the other options are not ideal:\n\nA. Supplying Key Directly:\u00a0Supplying the encryption key directly in API calls is not recommended. It exposes the key within the code, making it vulnerable to potential breaches. Cloud KMS provides a more secure way to manage and reference keys.\nC. Creating Keys in KMS with Manual Rotation:\u00a0While Cloud KMS can create keys, it\u2019s unnecessary in this scenario as the security team already provides the keys. Manually creating new keys every 72 days for rotation adds complexity and potential for human error. Cloud KMS can automate key rotation based on policies.\nD. Asymmetric Keys:\u00a0Asymmetric keys are typically used for encryption\/decryption where separate keys are used for each operation. However, for this scenario, symmetric keys (where the same key is used for both) are more suitable for data encryption at rest in Google Storage.\n\nBy following option B, you meet the security team\u2019s requirements for using their provided keys, ensure secure key management with Cloud KMS, and encrypt data at rest in Google Storage with automatic key rotation if configured."},{"label":"test_2","q_format":"single","q_text":"Your company currently hosts an AWS S3 bucket. You need to keep the contents of this bucket in sync with a new Google Cloud Storage bucket to support a backup storage destination. What is the best method to achieve this?","answers":[{"ans":"Once per week, use a\u00a0gsutil cp\u00a0command to copy over newly modified files.","val":false},{"ans":"Use\u00a0gsutil rsync\u00a0commands to keep both locations in sync.","val":false},{"ans":"Use Storage Transfer Service to keep both the source and destination in sync.","val":true},{"ans":"Use\u00a0gsutil -m cp\u00a0to keep both locations in sync.","val":false}],"q_expl":"The best method to keep the contents of an AWS S3 bucket in sync with a Google Cloud Storage bucket is to use the \u201cStorage Transfer Service\u201c (Option C).\nThe Storage Transfer Service is a managed service provided by Google Cloud Platform (GCP) that allows you to transfer data between different cloud storage providers, including AWS S3 and Google Cloud Storage. It provides a simple and automated way to keep the source and destination buckets in sync.\nUsing the Storage Transfer Service, you can set up a transfer job to periodically sync the contents of the AWS S3 bucket with the Google Cloud Storage bucket. The service will handle the transfer and synchronization of the files, including newly modified or added files, efficiently and securely.\nThis approach eliminates the need for manual intervention and ensures that the backup storage destination remains synchronized with the source bucket, maintaining data integrity and consistency.\nA & D are wrong as copy can be used to copy, however there needs to be more handling to keep it in sync.\nB is wrong as gsutil rsync option is a valid option, however preferred when the transfer cannot be done through Storage transfer service.\nCorrect Option:\n\nC. Use Storage Transfer Service (STS) to keep both the source and destination in sync.\n\nExplanation:\n\nThe links from Google Cloud (https:\/\/cloud.google.com\/storage-transfer-service) and\u00a0https:\/\/cloud.google.com\/architecture\/transferring-data-from-amazon-s3-to-cloud-storage-using-vpc-service-controls-and-storage-transfer-service)\u00a0clearly demonstrate Storage Transfer Service as the recommended approach for automated and scalable data transfer between cloud storage services, including AWS S3 and Google Cloud Storage. It allows for scheduled or event-driven synchronization, making it ideal for backups.\n\nIncorrect Options:\n\nA. Once per week, use a gsutil cp command to copy over newly modified files.\nThis approach is manual and error-prone. It doesn\u2019t offer automated synchronization, requiring recurring executions, which STS can automate.\n\nB. Use gsutil rsync commands to keep both locations in sync.\nWhile the link from Google Cloud (https:\/\/cloud.google.com\/storage\/docs\/gsutil\/commands\/rsync) details the\u00a0gsutil rsync\u00a0command, it\u2019s a one-time operation. You\u2019d need to schedule it repeatedly, similar to the\u00a0gsutil cp\u00a0approach. STS provides a more robust solution.\n\nD. Use gsutil -m cp to keep both locations in sync.\nThe\u00a0-m\u00a0flag with\u00a0cp\u00a0enables parallel transfers, but it doesn\u2019t establish ongoing synchronization. You would still need to run the command repeatedly. STS offers a more automated and efficient solution.\n\n\nAdditional Considerations:\n\nThe links from AWS (https:\/\/docs.aws.amazon.com\/datasync\/) and\u00a0https:\/\/docs.aws.amazon.com\/datasync\/latest\/userguide\/tutorial_transfer-google-cloud-storage.html) discuss AWS DataSync, which can also be used for data transfer. However, Storage Transfer Service is the native and more integrated solution within Google Cloud Platform.\nThe GitHub repository link (https:\/\/github.com\/patrickwyler\/gcs-bucket-sync-action) points to a third-party tool for S3 and GCS synchronization. While these tools exist, it\u2019s generally recommended to leverage the managed services offered by the respective cloud providers for optimal integration, security, and support.\n\nConclusion:\nFor automated, efficient, and scalable synchronization between your AWS S3 bucket and a new Google Cloud Storage bucket, Storage Transfer Service remains the most suitable option.\nReferences:\n1. Storage Transfer Service Overview:\u00a0https:\/\/cloud.google.com\/storage-transfer-service\n2. Storage Transfer Service Documentation:\u00a0https:\/\/cloud.google.com\/storage-transfer-service\/docs\n3. How to Set Up a Transfer Job with Storage Transfer Service:\u00a0https:\/\/cloud.google.com\/storage-transfer-service\/docs\/create-transfer-job"},{"label":"test_2","q_format":"single","q_text":"Your BigQuery dataset contains 1500 tables. When conducting a query, you are limited to a maximum of 1000 tables that you can query at once. You need to query data across all 1500 tables. What should you do?","answers":[{"ans":"Place tables into separate datasets.","val":false},{"ans":"If possible, merge the 1500 tables to bring the total number below 1000. You may still partition single tables to divide data for queries.","val":false},{"ans":"Export the data to Bigtable, and conduct your query inside of Bigtable.","val":false},{"ans":"Create multiple views of chunks of the 1500 tables, then query the multiple views.","val":true}],"q_expl":"The best option to query data across all 1500 tables in BigQuery with a 1000-table limit is:\nD. Create multiple views of chunks of the 1500 tables, then query the multiple views.\nHere\u2019s why the other options are not ideal:\n\nA. Place tables into separate datasets:\u00a0This wouldn\u2019t solve the problem. The limit applies to tables referenced in a single query, regardless of dataset.\nB. Merge tables:\u00a0Merging 1500 tables into a manageable number might be complex and might not be feasible depending on the schema and data size. Partitioning a single table wouldn\u2019t address the limit on referenced tables.\nC. Export to Bigtable:\u00a0Bigtable is a NoSQL database fundamentally different from BigQuery. Exporting and querying data in Bigtable would require significant schema modifications and wouldn\u2019t leverage BigQuery\u2019s functionalities for large-scale analytics.\n\nExplanation of Option D:\n\nChunking:\u00a0Divide the 1500 tables into smaller groups (chunks) of less than 1000 tables each.\nCreate Views:\u00a0For each chunk, create a view in BigQuery that references the tables within that chunk.\nQuery the Views:\u00a0In your final query, reference the created views instead of the individual tables.\n\nThis approach allows you to work within the 1000-table limit while still accessing data across all your original tables.\nAdditional Considerations:\n\nThe number of chunks and composition of each chunk depend on your specific needs and the distribution of tables across different categories.\nEnsure proper naming conventions for your views to clearly indicate their purpose and the tables they reference."},{"label":"test_2","q_format":"single","q_text":"A client wants to store files from one location and retrieve them from another location. Security requirements are that no one should be able to access the contents of the file while it is hosted in the cloud. What is the best option?","answers":[{"ans":"Default encryption should be sufficient","val":false},{"ans":"Customer-Supplied Encryption Keys (CSEK)","val":false},{"ans":"Customer Managed Encryption Keys (CMEK)","val":false},{"ans":"Client-side encryption","val":true}],"q_expl":"Correct answer is D as the requirement is that the file cannot be decrypted in the cloud, so encrypt it before it is uploaded and after it is downloaded adds a layer of encryption.\nData Encryption Options:-\u00a0https:\/\/cloud.google.com\/storage\/docs\/encryption\/\nServer-side encryption: encryption that occurs after Cloud Storage receives your data, but before the data is written to disk and stored.\nCustomer-supplied encryption keys: You can create and manage your own encryption keys for server-side encryption, which act as an additional encryption layer on top of the standard Cloud Storage encryption.\nCustomer-managed encryption keys: You can generate and manage your encryption keys using Cloud Key Management Service, which act as an additional encryption layer on top of the standard Cloud Storage encryption.\nClient-side encryption: encryption that occurs before data is sent to Cloud Storage. Such data arrives at Cloud Storage already encrypted but also undergoes server-side encryption.\n\nLet\u2019s re-analyze the options considering both cloud provider encryption and client-side encryption:\nClient Requirement:\n\u201cSecurity requirements are that no one should be able to access the contents of the file while it is hosted in the cloud.\u201d\nA. Default Encryption (Incorrect):\nWhile Google Cloud Storage encrypts data at rest by default, the client might not have control over the encryption keys. This conflicts with the requirement of absolute control.\nDefault encryption offers a good security posture, but for this specific client requirement of absolute control, it falls short.\nIf the client trusts Google and doesn\u2019t need granular control over key access, default encryption might be sufficient. However, based on the prompt, they seem to require stricter control.\nB. Customer-Supplied Encryption Keys (CSEK) (Less Common):\nThis approach might not be readily available on all cloud platforms. Managing and distributing these keys can be complex for the client.\nC. Customer Managed Encryption Keys (CMEK) (Secure, but Might Not Be Necessary):\nCMEK allows the client to manage their own keys stored in Google Cloud KMS. This offers strong security but might be overkill if the client trusts Google\u2019s default encryption and just wants to ensure their own control over the data.\nD. Client-side Encryption (Correct for This Scenario):\nClient-side encryption offers the strongest guarantee that no one, not even the cloud provider, can access the file content in the cloud. Here\u2019s why:\nEncryption Before Upload:\u00a0The data is encrypted on the client-side before being uploaded to the cloud storage.\nCloud Storage Unawareness:\u00a0The cloud storage service receives and stores the encrypted data without ever decrypting it. Even with access to the data, they cannot access the content.\nDecryption Key Control:\u00a0The decryption key remains solely under the client\u2019s control.\nPotential Drawbacks of Client-Side Encryption:\nClient-Side Security:\u00a0The security of the data relies heavily on the client-side environment. If the decryption key is compromised on the client\u2019s system, the data could be vulnerable.\nKey Management:\u00a0The client needs to manage the decryption keys securely. Losing the key could lead to permanent data loss.\nConclusion:\nIn this specific scenario, where the client prioritizes absolute control over data confidentiality within the cloud, client-side encryption (option D) is the most suitable solution. It ensures that even the cloud provider cannot access the file content.\nHowever, it\u2019s important to weigh the potential drawbacks mentioned above. If the client trusts Google\u2019s default encryption and is comfortable with some level of cloud provider involvement in key management, then CMEK might be a viable alternative.\nThe best choice depends on the client\u2019s specific risk tolerance and security priorities.\nSources\n\n\n\n\n\nbrax.gg\/11-next-steps\/"},{"label":"test_2","q_format":"single","q_text":"You use BigQuery to store your startup\u2019s data related to your clients\u2019 orders and payment transaction logs. You hired a team of data analysts to create reports and several visualization dashboards for better presentation to shareholders and sharing in meetings and investor conferences. Some fields are sensitive and should not be viewed by anyone other than you. You want data analysts to query the data without having access to the sensitive columns. What would you do?","answers":[{"ans":"Grant data analysts viewer role on tables with specifying what columns data analysts are authorized to query.Create authorized views on tables in the same dataset in which the tables reside.","val":false},{"ans":"Grant viewer role to data analysts on the views.Create a new dataset in BigQuery. Create authorized views on tables.","val":false},{"ans":"Create authorized views on tables in the same dataset in which the tables reside. Grant viewer role to data analysts on the views.","val":true},{"ans":"Create a new dataset in BigQuery. Grant viewer role to data analysts on the new dataset.Copy the tables from the current dataset to the new one with columns data analysts are authorized to query.","val":false}],"q_expl":"The best approach to grant data analysts query access to sensitive data without exposing the sensitive columns would be:\nCreate authorized views on tables in the same dataset in which the tables reside. Grant viewer role to data analysts on the views.\nHere\u2019s why:\n\nAuthorized views: By creating authorized views, you can define the exact columns that data analysts can query. This ensures that they only have access to the information they need for their analysis.\nSame dataset: Keeping the views in the same dataset as the original tables simplifies management and reduces the complexity of your data organization.\nViewer role: Granting the viewer role to data analysts on the views provides them with the necessary permissions to query the data without granting them access to the underlying tables or their sensitive columns.\n\nThis approach effectively balances the need for data access with the requirement to protect sensitive information, ensuring that data analysts can perform their tasks while maintaining data security.\nReference: https:\/\/cloud.google.com\/bigquery\/docs\/authorized-views"},{"label":"test_2","q_format":"multiple","q_text":"Business owners at your company have given you a database of bank transactions. Each row contains the user ID, transaction type, transaction location, and transaction amount. They ask you to investigate what type of machine learning can be applied to the data. Which three machine learning applications can you use?","answers":[{"ans":"A. Supervised learning to determine which transactions are most likely to be fraudulent.","val":true},{"ans":"B. Unsupervised learning to determine which transactions are most likely to be fraudulent.","val":false},{"ans":"C. Clustering to divide the transactions into N categories based on feature similarity.","val":true},{"ans":"D. Supervised learning to predict the location of a transaction.","val":true},{"ans":"E. Reinforcement learning to predict the location of a transaction.","val":false},{"ans":"F. Unsupervised learning to predict the location of a transaction.","val":false}],"q_expl":"A. Supervised learning to determine which transactions are most likely to be fraudulent.\nC. Clustering to divide the transactions into N categories based on feature similarity.\nD. Supervised learning to predict the location of a transaction. \u00a0 \n\n\n\n\n\n\nHere\u2019s a breakdown of why these three applications are suitable:\nA. Supervised learning to determine which transactions are most likely to be fraudulent.\n\nLabeling Data: You can label historical transactions as \u201cfraudulent\u201d or \u201clegitimate.\u201d\nTraining a Model: Train a model on this labeled data to learn patterns associated with fraudulent transactions.\nPrediction: Use the trained model to predict the likelihood of new transactions being fraudulent.\n\nC. Clustering to divide the transactions into N categories based on feature similarity.\n\nIdentifying Patterns: Cluster similar transactions together, revealing patterns or anomalies.\nAnomaly Detection: Identify unusual clusters that might indicate fraudulent activity.\nCustomer Segmentation: Group customers based on their transaction behavior.\n\nD. Supervised learning to predict the location of a transaction.\n\nFeature Engineering: Extract relevant features from the data, such as transaction amount, time, and user behavior.\nModel Training: Train a model to predict the location based on these features.\nLocation Prediction: Use the trained model to predict the location of new transactions.\n\nWhile unsupervised learning can be used for tasks like anomaly detection, it\u2019s not suitable for predicting specific outcomes like transaction fraud or location. Reinforcement learning, on the other hand, is used for sequential decision-making problems, which is not relevant to this scenario."},{"label":"test_2","q_format":"single","q_text":"You are responsible for security and access control to a BigQuery dataset hosted within a project. Multiple users from multiple teams need to have access to the different tables within the dataset. How can the access be control?","answers":[{"ans":"Create Authorized views for tables in a separate project and grant access to the teams","val":true},{"ans":"Create Authorized views for tables in a same project and grant access to the teams","val":false},{"ans":"Create Materialized views for tables in a separate project and grant access to the teams","val":false},{"ans":"Create Materialized views for tables in a same project and grant access to the teams","val":false}],"q_expl":"Correct answer is A as the controlled access can be provided using Authorized views created in a separate project.\nBigQuery is a petabyte-scale analytics data warehouse that you can use to run SQL queries over vast amounts of data in near realtime.\nGiving a view access to a dataset is also known as creating an authorized view in BigQuery. An authorized view allows you to share query results with particular users and groups without giving them access to the underlying tables. You can also use the view\u2019s SQL query to restrict the columns (fields) the users are able to query.\nWhen you create the view, it must be created in a dataset separate from the source data queried by the view. Because you can assign access controls only at the dataset level, if the view is created in the same dataset as the source data, your data analysts would have access to both the view and the data.\nB is wrong as Authorized views should be created in a separate project. If they are created in the same project, the users would have access to the underlying tables as well.\nC & D are wrong as only logical views can be created and not materialized."},{"label":"test_2","q_format":"single","q_text":"We should follow the Principle of Least Privilege (POLP) rule when granting permissions in Google Cloud. If a role has many permissions that are not used by your application, it is recommended to\u2026","answers":[{"ans":"...create several service accounts with the appropriate permissions. One service account for one permission.","val":false},{"ans":"...use this role anyway.","val":false},{"ans":"...create a custom role with only the necessary permissions.","val":true},{"ans":"...use basic roles.","val":false}],"q_expl":"The Principle of Least Privilege (POLP) is a security principle that recommends granting only the minimum level of access that is required to perform a specific task. This principle also applies to the Google Cloud Platform (GCP), and it is important to follow it when granting permissions to users, groups, or service accounts.\nIf a role has many permissions that are not used by your application, it is recommended to create a custom role with only the necessary permissions. This allows you to limit the scope of access to your resources and reduce the risk of accidental or intentional misuse of permissions.\nCreating several service accounts with the appropriate permissions is not recommended because it can lead to a proliferation of service accounts, which can be difficult to manage and track. Similarly, using a role with many unused permissions or basic roles is not recommended as it violates the POLP principle and can increase the risk of unauthorized access or data breaches.\nReferences:\nGoogle Cloud IAM Best Practices:\u00a0https:\/\/cloud.google.com\/iam\/docs\/best-practices\nPrinciple of Least Privilege (POLP):\u00a0https:\/\/en.wikipedia.org\/wiki\/Principle_of_least_privilege"},{"label":"test_2","q_format":"multiple","q_text":"The BigQuery Data Transfer Service facilitates transfers from many data sources, including: (select 3)","answers":[{"ans":"Amazon S3 buckets","val":true},{"ans":"LinkedIn account","val":false},{"ans":"Google Play","val":true},{"ans":"Twitter account","val":false},{"ans":"YouTube channel","val":true}],"q_expl":"The BigQuery Data Transfer Service is a fully managed service that allows you to transfer data from various sources into BigQuery with minimal setup and configuration. Some of the supported data sources are:\nAmazon S3 buckets: Allows you to transfer data from Amazon Simple Storage Service (S3) into BigQuery. Reference: [1]\nGoogle Play: Allows you to transfer data from Google Play into BigQuery. Reference: [2]\nYouTube channel: Allows you to transfer data from YouTube Analytics into BigQuery. Reference: [3]\nLinkedIn account and Twitter account are not currently supported data sources for the BigQuery Data Transfer Service.\nReferences:\n[1] BigQuery Data Transfer Service \u2013 Amazon S3 transfers:\u00a0https:\/\/cloud.google.com\/bigquery-transfer\/docs\/amazon-s3-transfer\n[2] BigQuery Data Transfer Service \u2013 Google Play transfers:\u00a0https:\/\/cloud.google.com\/bigquery-transfer\/docs\/google-play-transfer\n[3] BigQuery Data Transfer Service \u2013 YouTube Analytics transfers:\u00a0https:\/\/cloud.google.com\/bigquery-transfer\/docs\/youtube-analytics-transfer"},{"label":"test_2","q_format":"single","q_text":"You are designing the database schema for a machine learning-based food ordering service that will predict what users want to eat. Here is some of the information you need to store:The user profile: What the user likes and doesn\u2019t like to eatThe user account information: Name, address, preferred meal timesThe order information: When orders are made, from where, to whomThe database will be used to store all the transactional data of the product. You want to optimize the data schema. Which Google Cloud Platform product should you use?","answers":[{"ans":"BigQuery","val":true},{"ans":"Cloud SQL","val":false},{"ans":"Cloud Bigtable","val":false},{"ans":"Cloud Datastore","val":false}],"q_expl":"Based on the provided requirements, the most suitable Google Cloud Platform product for optimizing the data schema of the machine learning-based food ordering service would be:\nA. BigQuery\nBigQuery is a fully-managed, serverless data warehouse that is highly scalable and designed for handling large volumes of data. It provides fast SQL queries and supports real-time analytics. BigQuery is well-suited for transactional data and can handle structured and semi-structured data efficiently.\nIn the given scenario, BigQuery can be used to store the user profiles, user account information, and order information. It offers flexible schema design, allowing you to store and query structured data. With BigQuery, you can easily perform complex analytics and run machine learning models on the data stored in the database.\nCloud SQL (option B) is a fully-managed relational database service, suitable for traditional SQL databases like MySQL and PostgreSQL. While it could handle the transactional data, BigQuery would be a better choice due to its scalability and analytic capabilities.\nCloud Bigtable (option C) is a NoSQL wide-column database that is optimized for large-scale, low-latency workloads. However, it might not be the best choice for the given scenario, as it is more suitable for high-throughput read\/write operations on large datasets, and the provided requirements don\u2018t explicitly indicate such needs.\nCloud Datastore (option D) is a NoSQL document database for web and mobile applications. It is designed for rapid development and scaling. However, BigQuery would be a better fit for the provided requirements as it offers more advanced analytics and querying capabilities.\nTherefore, the best choice among the given options is A. BigQuery."},{"label":"test_2","q_format":"single","q_text":"A bank wishes to predict that a given loan application will default in future. Given a dataset containing customer demographic information, loan application information, credit score and saving balance account information and a label containing default indicator (Y \u2013 Will Default, N \u2013 Will Not Default). Which type of Machine Learning algorithm is suited to achieve this?","answers":[{"ans":"Classification","val":true},{"ans":"Regression","val":false},{"ans":"Association","val":false},{"ans":"Clustering","val":false}],"q_expl":"The type of machine learning algorithm suited for predicting if a loan application will default or not is a classification algorithm.\nClassification\u00a0algorithms are used to predict the categorical class labels of instances based on the input attributes. In this case, the categorical class labels are whether the loan application will default or not. The algorithm will be trained on a dataset containing customer demographic information, loan application information, credit score, and saving balance account information to predict the default indicator for a new loan application.\nRegression algorithms are used to predict continuous numerical values, such as predicting the price of a house based on its features.\nAssociation algorithms are used to find relationships between variables or items in a dataset, such as identifying which products are frequently purchased together.\nClustering algorithms are used to group similar instances together based on their features.\nReference:\nhttps:\/\/developers.google.com\/machine-learning\/problem-framing\/types-of-ml\nhttps:\/\/towardsdatascience.com\/a-beginners-guide-to-machine-learning-algorithms-33c5f2fe44db"},{"label":"test_2","q_format":"single","q_text":"An application has the following data requirements:1. It requires strongly consistent transactions.2. Total data will be less than 500 GB.3. The data does not need to be streaming or real time.Which data technology would fit these requirements?","answers":[{"ans":"BigQuery","val":false},{"ans":"Cloud Bigtable","val":false},{"ans":"Cloud Spanner","val":true},{"ans":"Cloud Memorystore","val":false}],"q_expl":"Cloud Spanner would be the best fit for these requirements. It offers strongly consistent transactions, can handle large amounts of structured data, and can scale horizontally to meet demand. It also offers automatic sharding, so it can easily handle datasets up to petabyte-scale.\nWhile Cloud SQL and Cloud Memorystore offer strong consistency, they are not designed to handle large datasets or scale horizontally as easily as Cloud Spanner. BigQuery is a columnar data warehouse solution, which may not be the best fit for transactional data that requires strong consistency. Cloud Bigtable is a NoSQL wide-column store, but it is better suited for unstructured or semi-structured data that requires low latency access.\nReference:\nCloud Spanner:\u00a0https:\/\/cloud.google.com\/spanner\nCloud SQL:\u00a0https:\/\/cloud.google.com\/sql\nCloud Memorystore:\u00a0https:\/\/cloud.google.com\/memorystore\nBigQuery:\u00a0https:\/\/cloud.google.com\/bigquery\nCloud Bigtable:\u00a0https:\/\/cloud.google.com\/bigtable"},{"label":"test_2","q_format":"single","q_text":"You are deploying 10,000 new IoT devices to collect temperatures in your warehouses globally. However, the source data is streamed in bursts and is not periodical and must be transformed before it can be used. How should you design the system in Google Cloud?","answers":[{"ans":"Use Cloud Bigtable for fast input and cbt for ETL.","val":false},{"ans":"Ingest data to Cloud Storage. Use Cloud Dataproc for ETL.","val":false},{"ans":"Use Cloud Pub\/Sub to buffer the data, and then use BigQuery for ETL.","val":false},{"ans":"Use Cloud Pub\/Sub to buffer the data, and then use Cloud Dataflow for ETL.","val":true}],"q_expl":"Correct answer is D as unpredictable data would require a buffer which can be provided using Cloud Pub\/Sub and Cloud Dataflow can help perform transformations on the fly before the data is stored in any storage medium.\nCloud Pub\/Sub:-\u00a0https:\/\/cloud.google.com\/pubsub\/\nA, B & C are wrong as either the ingestion is not ideal for bursts or the data needs to be stored and the ETL performed as batch operation."},{"label":"test_2","q_format":"single","q_text":"An application that relies on Cloud SQL to read infrequently changing data is predicted to grow dramatically. How can you increase capacity for more read-only clients?","answers":[{"ans":"Configure high availability on the master node","val":false},{"ans":"Establish an external replica in the customer\u2018s data center","val":false},{"ans":"Use backups so you can restore if there\u2018s an outage","val":false},{"ans":"Configure read replicas.","val":true}],"q_expl":"Correct answer is D as read replicas can help handle the read traffic reducing the load from the primary database.\nCloud SQL Replication Options:-\u00a0https:\/\/cloud.google.com\/sql\/docs\/mysql\/replication\/\nCloud SQL provides the ability to replicate a master instance to one or more read replicas. A read replica is a copy of the master that reflects changes to the master instance in almost real time.\nA is wrong as high availability is for failover and not for performance.\nB is wrong as external replica is not recommended for scaling as it needs to be maintained and the network established for replication.\nC is wrong as backups are more to restore the database in case of any outage."},{"label":"test_2","q_format":"single","q_text":"You are planning to embed a online customer support service within your website. Which GCP Service would allow you to design and integrate a conversational user interface into a mobile app, web application, device, bot, interactive voice response systems, and so on?","answers":[{"ans":"Cloud Video Intelligence","val":false},{"ans":"Cloud Vision","val":false},{"ans":"Cloud Natural language","val":false},{"ans":"Dialogflow","val":true}],"q_expl":"Correct answer is D as Dialogflow can help provide conversational interfaces across various platforms.\nAI Products:-\u00a0https:\/\/cloud.google.com\/products\/ai\/\nDialogflow Enterprise Edition\u00a0is an end-to-end development suite for building conversational interfaces for websites, mobile applications, popular messaging platforms, and IoT devices. You can use it to build interfaces (e.g., chatbots) that are capable of natural and rich interactions between your users and your business. It is powered by machine learning to recognize the intent and context of what a user says, allowing your conversational interface to provide highly efficient and accurate responses."},{"label":"test_2","q_format":"single","q_text":"You have configured streaming data pipelines to ingest data from thousands of Internet of Things (IoT) devices, ingest it into BigQuery. The data is stored into ingestion-time partitioned table. You want to run SQL queries against your data for analysis. How would you query specific partitions in a BigQuery table?","answers":[{"ans":"Use the DATE column in the WHERE clause","val":false},{"ans":"Use the EXTRACT(DATE) clause","val":false},{"ans":"Use the _PARTITIONTIME pseudo-column in the WHERE clause","val":true},{"ans":"Use DATE BETWEEN in the WHERE clause","val":false}],"q_expl":"Correct answer is C as for ingestion-time partitioned tables, two columns\u00a0_PARTITIONTIME\u00a0and\u00a0_PARTITIONDATE\u00a0and available to query the partition.\nBigQuery Querying Partitioned Tables:-\u00a0https:\/\/cloud.google.com\/bigquery\/docs\/querying-partitioned-tables#ingestion-time_partitioned_table_pseudo_columns\nIngestion-time partitioned tables\nWhen you create a table partitioned by ingestion time, BigQuery automatically loads data into daily, date-based partitions that reflect the data\u2019s ingestion or arrival date. Pseudo column and suffix identifiers allow you to restate (replace) and redirect data to partitions for a specific day.\nIngestion-time partitioned tables include a pseudo column named _PARTITIONTIME that contains a date-based timestamp for data that is loaded into the table. Queries against time-partitioned tables can restrict the data read by supplying _PARTITIONTIME filters that represent a partition\u2019s location. All the data in the specified partition is read by the query, but the _PARTITIONTIME predicate filter restricts the number of partitions scanned.\nWhen you create an ingestion-time partitioned table, two pseudo columns are added to the table: a _PARTITIONTIME pseudo column and a _PARTITIONDATE pseudo column. The _PARTITIONTIME pseudo column contains a date-based timestamp for data that is loaded into the table. The _PARTITIONDATE pseudo column contains a date representation. Both pseudo column names are reserved, which means that you cannot create a column with either name in any of your tables.\n_PARTITIONTIME and _PARTITIONDATE are available only in ingestion-time partitioned tables. Partitioned tables do not have pseudo columns."},{"label":"test_2","q_format":"single","q_text":"Your company wants to develop a robotic car. The car needs to figure out a best way to traverse a path by it owns and the best way is the one with limited hurdles and shortest path. They aim to maximum a cumulative measure (say a reward) based on interactions with a given system. What type of machine learning needs to be applied?","answers":[{"ans":"Supervised learning","val":false},{"ans":"Unsupervised learning","val":false},{"ans":"Reinforcement learning","val":true},{"ans":"Dimensionality Reduction Technique","val":false}],"q_expl":"Correct answer is C as Reinforcement Learning can help design a model for car based on the reward technique.\nMachine Learning cases:-\u00a0https:\/\/developers.google.com\/machine-learning\/problem-framing\/cases\nAn additional branch of machine learning is reinforcement learning (RL). Reinforcement learning differs from other types of machine learning. In RL you don\u2019t collect examples with labels. Imagine you want to teach a machine to play a very basic video game and never lose. You set up the model (often called an agent in RL) with the game, and you tell the model not to get a \u201cgame over\u201d screen. During training, the agent receives a reward when it performs this task, which is called a reward function. With reinforcement learning, the agent can learn very quickly how to outperform humans."},{"label":"test_2","q_format":"single","q_text":"An organization wishes to automate data movement from Software as a Service (SaaS) applications such as Google Ads and Google Ad Manager on a scheduled, managed basis. This data is further needed for analytics and generate reports. How can the process be automated?","answers":[{"ans":"Use Storage Transfer Service to move the data to Cloud Storage","val":false},{"ans":"Use Storage Transfer Service to move the data to BigQuery","val":false},{"ans":"Use BigQuery Data Transfer Service to move the data to BigQuery","val":true},{"ans":"Use Transfer Appliance to move the data to Cloud Storage","val":false}],"q_expl":"BigQuery Data Service Transfer can help automate the data movement from data sources such as Google Ads and Google AD Manager.\nBigQuery Data Transfer Service:-\u00a0https:\/\/cloud.google.com\/bigquery\/docs\/transfer-service-overview\nA is wrong as Storage Transfer Service cannot move the data from Google Ads to BigQuery and Cloud Storage cannot help in analytics directly.\nB is wrong as Storage Transfer Service cannot move the data from Google Ads to BigQuery\nD is wrong as Transfer Appliance are for one time huge transfers."},{"label":"test_2","q_format":"single","q_text":"Your company is forecasting a sharp increase in the number and size of Apache Spark and Hadoop jobs being run on your local datacenter. You want to utilize the cloud to help you scale this upcoming demand with the least amount of operations work and code change. Which product should you use?","answers":[{"ans":"Google Cloud Dataflow","val":false},{"ans":"Google Cloud Dataproc","val":true},{"ans":"Google Compute Engine","val":false},{"ans":"Google Container Engine","val":false}],"q_expl":"Cloud Dataproc allows running of Apache Spark and Hadoop jobs.\nCloud Dataproc:-\u00a0https:\/\/cloud.google.com\/dataproc\/\nCloud Dataproc is a fast, easy-to-use, fully-managed cloud service for running Apache Spark and Apache Hadoop clusters in a simpler, more cost-efficient way. Operations that used to take hours or days take seconds or minutes instead, and you pay only for the resources you use (with per-second billing). Cloud Dataproc also easily integrates with other Google Cloud Platform (GCP) services, giving you a powerful and complete platform for data processing, analytics and machine learning\nA is wrong as Cloud Dataflow is a fully-managed service for transforming and enriching data in stream (real time) and batch (historical) modes with equal reliability and expressiveness \u2014 no more complex workarounds or compromises needed.\nC & D are wrong as they are not suited for big data processing."},{"label":"test_2","q_format":"multiple","q_text":"You are training a Tensorflow deep neural network model. The model should recognize different type of cars and return the brand and type of the car from the image input. While training, you decided to perform hyper-parameter tuning to optimize the model. Which of the variables are used for hyperparameter tuning? (select 2)","answers":[{"ans":"Number of nodes in hidden layers","val":true},{"ans":"Number of features","val":false},{"ans":"Number of hidden layers","val":true},{"ans":"Weight values","val":false}],"q_expl":"Hyperparameters are the variables govern the training process itself. For example, part of setting up a deep neural network is deciding how many hidden layers of nodes to use between the input layer and the output layer, and how many nodes each layer should use. These variables are not directly related to the training data. They are configuration variables. Note that parameters change during a training job, while hyperparameters are usually constant during a job.\nB is incorrect: Feature numbers are set by feature engineering, not hyperparameter tuning.\nD is incorrect: Weight values are set when training the model.\nReference:\u00a0https:\/\/cloud.google.com\/ml-engine\/docs\/tensorflow\/hyperparameter-tuning- overview"},{"label":"test_2","q_format":"multiple","q_text":"Your company wants to host confidential documents in Cloud Storage. Due to compliance requirements, there is a need for the data to be highly available and resilient even in case of a regional outage. Which 2 storage classes help meet the requirement?","answers":[{"ans":"Standard","val":false},{"ans":"Regional","val":false},{"ans":"Coldline","val":false},{"ans":"Dual-Regional","val":true},{"ans":"Multi-Regional","val":true}],"q_expl":"The storage classes that help meet the requirement of hosting highly available and resilient confidential documents in case of a regional outage are:\n1. Multi-Regional\n2. Dual-Regional\nExplanation:\nMulti-Regional\u00a0storage class also helps meet the requirement of high availability and resilience, even in case of a regional outage. It replicates data across multiple regions, providing increased resilience and availability. In the event of a regional outage, data stored in the Multi-Regional storage class remains accessible from other regions where the data is replicated.\nDual-Regional\u00a0storage class replicates data across two separate regions, offering increased resilience in the event of a regional outage. Data stored in this storage class remains accessible even if one of the regions experiences an outage. This makes it suitable for meeting the high availability and resilience requirements..\nExplanation of other options:\nStandard: The Standard storage class provides a balance between availability, performance, and cost. However, it does not specifically guarantee high availability and resilience in case of a regional outage.\nRegional: The Regional storage class replicates data within a single region, providing high availability within that region. However, it does not provide resilience in case of a regional outage.\nColdline: The Coldline storage class is designed for long-term archival storage with infrequent access. While it provides durability and low storage costs, it does not offer the same level of availability and resilience required for handling regional outages.\nTherefore, the correct options to meet the requirement are the Multi-Regional and Dual-Regional storage classes. These storage classes ensure the confidential documents hosted in Cloud Storage have high availability and resilience, even in the event of a regional outage.\nReferences:\nGoogle Cloud Storage classes:\u00a0https:\/\/cloud.google.com\/storage\/docs\/storage-classes\nGoogle Cloud Storage regional and dual-regional overview:\u00a0https:\/\/cloud.google.com\/storage\/docs\/regional-dual-regional-overview"},{"label":"test_2","q_format":"single","q_text":"A retailer wishes to identify the products, which are bought together. It already has a historical dataset containing a customer id, receipt id and the products bought. Which type of Machine Learning algorithm is suited to achieve this?","answers":[{"ans":"Classification","val":false},{"ans":"Regression","val":false},{"ans":"Association","val":true},{"ans":"Clustering","val":false}],"q_expl":"Correct answer is C as Association rules help identify the association from past events to define relations with the new ones.\nAssociation rule learning is a rule-based machine learning method for discovering interesting relations between variables in large databases. It is intended to identify strong rules discovered in databases using some measures of interestingness.[1]This rule-based approach also generates new rules as it analyzes more data. The ultimate goal, assuming a large enough dataset, is to help a machine mimic the human brain\u2019s feature extraction and abstract association capabilities from new uncategorized data."},{"label":"test_2","q_format":"single","q_text":"Your company is working on a multi-cloud initiative. The data processing pipelines requires creating workflows that connect data, transfer data, processing, and using services across clouds. What cloud native tool should be used for orchestration?","answers":[{"ans":"Cloud Scheduler","val":false},{"ans":"Cloud Dataflow","val":false},{"ans":"Cloud Composer","val":true},{"ans":"Cloud Dataproc","val":false}],"q_expl":"Cloud Composer can help create workflows that connect data, processing, and services across clouds, giving you a unified data environment.\nCloud Composer:-\u00a0https:\/\/cloud.google.com\/composer\/\nCloud Composer\u00a0is a fully managed workflow orchestration service that empowers you to author, schedule, and monitor pipelines that span across clouds and on-premises data centers. Built on the popular Apache Airflow open source project and operated using the Python programming language, Cloud Composer is free from lock-in and easy to use.\nCloud Composer gives you the ability to connect your pipeline through a single orchestration tool whether your workflow lives on-premises, in multiple clouds, or fully within GCP. The ability to author, schedule, and monitor your workflows in a unified manner means you can break down the silos in your environment and focus less on infrastructure.\nA is wrong as Cloud Scheduler is a fully managed enterprise-grade cron job scheduler. It is not an multi-cloud orchestration tool.\nB is wrong as Google Cloud Dataflow is a fully managed service for strongly consistent, parallel data-processing pipelines. It does not support multi-cloud handling.\nD is wrong as Google Cloud Dataproc is a fast, easy to use, managed Spark and Hadoop service for distributed data processing."},{"label":"test_2","q_format":"single","q_text":"You are designing a photo sharing mobile app. Users will upload pictures from their mobile device directly and will be able to share pictures with others. As a compliance requirement, no image with offensive content should be allowed to be uploaded. How would you design your application?","answers":[{"ans":"Use Cloud Vision API to identify image with offensive content and mark it for manual checks.","val":true},{"ans":"Use Cloud Natural Language API to identify image with offensive content and mark it for manual checks.","val":false},{"ans":"Use Cloud Image Intelligence API to identify image with offensive content and mark it for manual checks.","val":false},{"ans":"Use Cloud Video Intelligence API to identify image with offensive content and mark it for manual checks.","val":false}],"q_expl":"Cloud Vision can be used for image analysis to detect offensive or inappropriate content.\nCloud Vision:-\u00a0https:\/\/cloud.google.com\/vision\/\nCloud Vision API enables developers to understand the content of an image by encapsulating powerful machine learning models in an easy-to-use REST API. It quickly classifies images into thousands of categories (such as, \u201csailboat\u201d), detects individual objects and faces within images, and reads printed words contained within images. You can build metadata on your image catalog, moderate offensive content, or enable new marketing scenarios through image sentiment analysis.\nOption B is wrong as Cloud Natural Language is for text analysis.\nOption C is wrong as the Cloud Image Intelligence does not exist.\nOption D is wrong as Video Intelligence is for videos."},{"label":"test_2","q_format":"single","q_text":"Your company signed a contract with a retail chain store to handle its data processing applications and tech stack. One of the several applications to be implemented is building an ETL pipeline to ingest the chain store\u2019s daily purchase transaction logs to be processed and stored for analysis and reporting; visualize the chain\u2019s purchase details for the head management. Daily transaction logs will be available at 2 am when the day is over and logs are exported to a Google Storage bucket partitioned by date in format (yyyy-mm-dd). Dataflow pipeline should run every day at 3:00 am to ingest and process the logs. Which of the following Google products would help?","answers":[{"ans":"Cloud Function","val":false},{"ans":"Compute Engine","val":false},{"ans":"Cloud Scheduler","val":true},{"ans":"Kubernetes Engine","val":false}],"q_expl":"Cloud Scheduler\u00a0is a fully managed enterprise-grade cron job scheduler. It allows you to schedule any job virtually, including batch, big data jobs, cloud infrastructure operations, and more. You can automate everything, including retries in case of failure to reduce manual toil and intervention. Cloud Scheduler even acts as a single pane of glass, allowing you to manage all your automation tasks from one place.\nReference:\u00a0https:\/\/cloud.google.com\/scheduler\/"},{"label":"test_2","q_format":"single","q_text":"A banking system is linked to over 400 ATM machines distributed around a region. Each ATM machine sends event data about the machine\u2019s current state (active, standby, maintenance, ..), banknote balance, current activity type (withdrawal, check balance, ..) and other stats related to business and security purposes. Due to the dynamic attribute structure sent by ATM machines, JSON-formatted events are sent to the centralized system. The head office\u2019s chief data officer wants event data received from ATM machines to be stored in a data warehouse after required cleansing and transformation for the analytics team to fetch reports using SQL-syntax. Which of the following is the best to achieve this?","answers":[{"ans":"Store event data to Google Storage after converting data to ORC format. Launch a Dataproc cluster to create external tables using Apache Hive on data residing in Google Storage.","val":false},{"ans":"Load the data to BigQuery with enabling the \u201cauto-detect\u201d option.","val":true},{"ans":"Import the data to BigTable. Choose a key combination which allows the best performance while fetching event data based on Google\u2019s recommendations.","val":false},{"ans":"Build a Dataflow pipeline to read JSON data and transform it into a structured format like CSV. Then, load the data to BigQuery.","val":false}],"q_expl":"Schema Auto-detection: Schema auto-detection is available when you load data into BigQuery, and when you query an external data source. When auto-detection is enabled, BigQuery starts the inference process by selecting the file in the data source and scanning up to 100 rows of data to use as a representative sample. BigQuery then examines each field and attempts to assign a data type to that field based on the values in the sample. BigQuery makes the best-effort attempt to automatically infer the schema for CSV and JSON files.\nReference:\u00a0https:\/\/cloud.google.com\/bigquery\/docs\/schema-detect"},{"label":"test_2","q_format":"single","q_text":"As a data engineer, you are assigned to assist data analysts with data modeling to maintain and re-design the existing tables in BigQuery. Data analysts have provided you with the new table schema to be applied to existing datasets. From the list of to-do for modifying the schema, the most notable changes are renaming columns and changing the data type for others, as well as adding \u201cREQUIRED\u201d constraints to some critical columns. How would you achieve this?","answers":[{"ans":"Create a new dataset schema in BigQuery. Insert data from existing dataset to the new one using INSERT SELECT statement.","val":false},{"ans":"Create authorized views on BigQuery tables with new column names and data types.","val":false},{"ans":"You can modify columns names and data types and apply constraints using ALTER command.","val":true},{"ans":"Export data to Google Storage. Create new tables with the updated schema. Import data to the new schema from Google Storage.","val":false}],"q_expl":"As a data engineer, the best approach to modify the existing tables in BigQuery based on the new table schema provided by the data analysts would be to use the ALTER command. This command can be used to rename columns, change the data type for others, and add \u201cREQUIRED\u201c constraints to critical columns. \nThe ALTER command can be used to modify an existing table\u2018s schema without losing any data. This means that there is no need to create a new dataset schema, insert data from the existing dataset to the new one, or export data to Google Storage. \nHere is an example of how to use the ALTER command to modify a table\u2018s schema: \nsqlCopy code \nALTER TABLE my_dataset.my_table \nRENAME COLUMN old_column_name TO new_column_name, \nMODIFY COLUMN column_name new_data_type, \nALTER COLUMN critical_column SET OPTIONS(required=true); \nThis command renames a column from \u201cold_column_name\u201c to \u201cnew_column_name\u201c, changes the data type of \u201ccolumn_name\u201c to \u201cnew_data_type\u201c, and sets the \u201crequired\u201c constraint for the \u201ccritical_column\u201c. \nIn addition to using the ALTER command, it\u2018s important to ensure that any changes made to the table schema are compatible with the existing data in the table. For example, changing a column\u2018s data type to a narrower type could result in data loss if the new type cannot hold all of the values in the existing column. \nIn conclusion, the most appropriate approach to modify the existing tables in BigQuery based on the new table schema provided by the data analysts would be to use the ALTER command to modify the table\u2018s schema while ensuring that any changes made are compatible with the existing data in the table. \nThe correct option for modifying the existing tables in BigQuery based on the new table schema provided by the data analysts would be option C, which is to use the ALTER command to modify the table\u2018s schema by renaming columns, changing the data type for others, and adding \u201cREQUIRED\u201c constraints to some critical columns. \nOption A is not necessary since it involves creating a new dataset schema and inserting data from the existing dataset to the new one, which is not needed for this task. \nOption B is also not the best approach since creating authorized views would not modify the underlying table schema, but rather create a new view that can reference the modified table. \nOption D is not the most efficient approach since exporting data to Google Storage and importing data to a new schema can be time-consuming and could lead to additional storage costs. \nTherefore, using the ALTER command to modify the table\u2018s schema is the most appropriate and efficient approach for this task. \nReferences: \nALTER TABLE syntax:\u00a0https:\/\/cloud.google.com\/bigquery\/docs\/reference\/standard-sql\/data-definition-language#alter_table_statement \nModifying table schema:\u00a0https:\/\/cloud.google.com\/bigquery\/docs\/managing-table-schemas \nChecking schema compatibility:\u00a0https:\/\/cloud.google.com\/bigquery\/docs\/managing-table-schemas#checking_schema_compatibility"},{"label":"test_2","q_format":"single","q_text":"A system is expected to receive over 15,000 content delivery logs every minute from different web & mobile apps. Logs are received in JSON format. Due to logs being generated by different apps, each developed by a different team, logs do not have a fixed structure and may hold different attributes. Which of the following is a recommended storage option?","answers":[{"ans":"Cloud SQL","val":false},{"ans":"Cloud Spanner","val":false},{"ans":"BigTable","val":true},{"ans":"Datastore","val":false}],"q_expl":"Cloud BigTable is a petabyte-scale, fully managed NoSQL database service for large analytical and operational workloads. It provides flexible schema options.\nOptions A and B are incorrect: Cloud SQL & Spanner are relational database services. They are not recommended for JSON-format log data with a flexible schema.\nOption D is incorrect: Datastore can be a potential choice since it\u2019s a NoSQL database. However, Datastore is not built for storing huge data volumes as required in this scenario. Datastore is designed for web applications of a small scale.\nReference :\u00a0https:\/\/stackoverflow.com\/questions\/30085326\/google-cloud-bigtable-vs- google-cloud-datastore"},{"label":"test_2","q_format":"single","q_text":"You receive event data related to on-premise servers holding information about the servers CPU load, memory, disk space, I\/O reads and writes, and another application performance stats every 60 seconds. These events are stored in Google Storage. It is decided to use the data to monitor the on-premise architecture. Metrics should be extracted from these events in a time-series base for further calculations based on the timeline. Which of the following is best to achieve this?","answers":[{"ans":"Use Cloud SQL as a database. Move data from Google Storage to Cloud SQL.","val":false},{"ans":"Use Dataproc with Apache Hive to do required queries on data.","val":false},{"ans":"Move data to BigTable. Use tall & narrow tables when designing the schema and row key.","val":true},{"ans":"Move data to BigTable. Use short & wide tables when designing the schema and row key.","val":false}],"q_expl":"Storing time-series data in Cloud Bigtable is a natural fit for the given scenario. Cloud Bigtable stores data as unstructured columns in rows; each row has a row key, and row keys are sorted lexicographically.\nFor time series, you should generally use tall and narrow tables. This is for two reasons:\n1. Storing one event per row makes it easier to run queries against your data.\n2. Storing many events per row makes it more likely that the total row size will exceed the recommended maximum.\nOption A is incorrect: Cloud SQL is a relational database. Event data might require a flexible structure. Cloud SQL is not scalable to write thousands of rows in a given second.\nOption B is incorrect: For this scenario, using BigTable is preferred over storing data in Google Storage as further data partitioning and file formatting, both are required to use Dataproc with Apache Hive.\nOption D is incorrect: Wide & short table schema is not optimal for time-series event data.\nReference:\u00a0https:\/\/cloud.google.com\/bigtable\/docs\/schema-design-time-series"},{"label":"test_2","q_format":"single","q_text":"You receive payment transaction logs from e-wallet apps. Transaction logs have a dynamic structure that differs from the e-wallet apps received from. Logs are required to be stored for further security analysis. Transaction logs are critical and it is expected from data storage to have high performance in order to query the required security metrics to be updated in near-real-time. Which of the following approaches should you use?","answers":[{"ans":"Use BigTable as a database with HDD storage to store system logs.","val":false},{"ans":"Use BigTable as a database with SSD storage to store system logs.","val":true},{"ans":"Use Datastore as a database to store system logs.","val":false},{"ans":"Use Firebase as a database to store system logs.","val":false}],"q_expl":"When you create a Cloud Bigtable instance, you choose whether its clusters store data on solid-state drives (SSD) or hard disk drives (HDD):\nSSD is significantly faster and has a more predictable performance than HDD.\nHDD throughput is much more limited than SSD throughput. In a cluster that uses HDD storage, it\u2019s easy to reach the maximum throughput before CPU usage reaches 100%. To increase throughput, you must add more nodes, but the cost of the additional nodes can easily exceed your savings from using HDD storage. SSD storage does not have this limitation because it offers much more throughput per node.\nIndividual row reads on HDD are very slow. Because of disk seek time, HDD storage supports only 5% of the read rows per second of SSD storage.\nThe cost savings from HDD are minimal, relative to the cost of the nodes in your Cloud Bigtable cluster, unless you\u2019re storing very large amounts of data.\nReference:\nhttps:\/\/cloud.google.com\/bigtable\/docs\/choosing-ssd-hdd\nhttps:\/\/cloud.google.com\/bigquery\/external-data-bigtable"},{"label":"test_2","q_format":"single","q_text":"A company uses Apache Hive for querying data with a size of 200TB which resides in Google Storage. Apache Hive is installed in on-premise infrastructure. A decision was made to stop maintaining the on-premise Hive cluster since it costs outsourcing charges and instead of finding a solution on Google Cloud it replaces the Hive cluster as the data warehouse. The migration should be done in a short time period and cost should be considered. Which of the following approaches is the most appropriate for this?","answers":[{"ans":"Use BigQuery as the new data warehouse. Import data from Google Storage to BigQuery.","val":false},{"ans":"Use BigQuery as the new data warehouse. Create external tables referencing to data in Google Storage.","val":true},{"ans":"Use Dataproc as an alternative to on-premise Apache Hive cluster.","val":false},{"ans":"Use Bigtable as the new data warehouse. Import data from Google Storage to Bigtable.","val":false}],"q_expl":"Native tables in BigQuery are tables that import the full data inside Google BigQuery as you do in any other common database system. In contrast, external tables are tables that do not store the data in Google BigQuery, instead, they reference the data from an external source, such as a data lake. The advantages of creating external tables are that they are fast to create so you skip the part of importing data and no additional monthly billing storage costs are accrued to your account since you only get charged for the data that is stored in the data lake, which is comparatively cheaper than storing it in BigQuery.\nOption A is incorrect: Importing data into BigQuery from Google Storage may take more time compared to creating external tables on data. Additional storage costs by BigQuery are another issue that can be more expensive than Google Storage.\nOption C is incorrect: Using Dataproc requires maintenance since it\u2019s not fully managed by Google. This approach does not satisfy the company\u2019s decision to use a fully-managed product.\nOption D is incorrect: Bigtable is not recommended for this scenario.\nReference:\u00a0https:\/\/cloud.google.com\/bigquery\/external-data-sources"},{"label":"test_2","q_format":"single","q_text":"Your company uses BigQuery as its main data warehouse. There are different users with different departments who access BigQuery and run ad-hoc queries. The CTO noticed a hike in BigQuery costs due to running ad-hoc queries scan a high volume of data. Hence, she wants to limit the quota for the departments based on their requirements and business needs. How would you achieve this?","answers":[{"ans":"Allow access to department leads and managers only to control BigQuery access.","val":false},{"ans":"Set monthly flat-rate pricing for BigQuery.","val":false},{"ans":"Set custom quotas for each user with access on BigQuery based on their business requirements.","val":true},{"ans":"Set project-level quotas on BigQuery by setting a fixed size limit to be used monthly.","val":false}],"q_expl":"If you have multiple BigQuery projects and users, you can manage costs by requesting a custom quota that specifies a limit on the amount of query data processed per day.\nCreating a custom quota on query data allows you to control costs at the project-level or at the user- level.\n\u2022 Project-level custom quotas limit the aggregate usage of all users in that project.\n\u2022 User-level custom quotas are separately applied to each user or service account within a project.\nIn this scenario, the aim is to control user quotas. So, option C is the best approach.\nOption A is incorrect: This is not a sufficient solution to restrict access to management only. This is not the goal of CTO.\nOption B is incorrect: Flat-rate can be a possible approach. However, BigQuery does not provide flexible flat-rate pricing and the cheapest is $10,000 for 500 slots, which may not be a desirable option for small to medium businesses.\nOption D is incorrect: Setting project-level quota is not the best approach for this scenario because this will not set user limit quotas and when the project reaches the limit set, it will disallow all users to run queries.\nReferences:\nhttps:\/\/cloud.google.com\/bigquery\/docs\/custom-quotas#controlling_query_costs_using_bigquery_custom_quotas\nhttps:\/\/cloud.google.com\/bigquery\/pricing#monthly-flat-rate"},{"label":"test_2","q_format":"single","q_text":"You have Apache Spark jobs running on on-premise machines. The team decided to migrate all on-premises resources to Google Cloud and Dataproc was considered to be used to run Spark jobs on the cloud while data was migrated from on-premise HDFS to Google Storage. Dataproc will read and write data from and to Google Storage using the connector. After a while, you noticed that Spark jobs running on Dataproc are I\/O intensive and this is causing latencies reading and writing data in Storage. How would you solve this?","answers":[{"ans":"Increase persistent disk size for Dataproc cluster\u2019s nodes.","val":false},{"ans":"Increase RAM capacity of Dataproc cluster\u2019s worker nodes.","val":false},{"ans":"Use local HDFS storage of Dataproc cluster nodes instead of Google Storage.","val":true},{"ans":"Increase RAM capacity of Dataproc cluster\u2019s master node.","val":false}],"q_expl":"It\u2019s recommended to use Dataproc to run Apache Spark & Hadoop clusters When you want to move Hadoop & Spark workloads from an on-premises environment to Google Cloud Platform (GCP).\nLocal HDFS storage is a good option if you have workloads that involve heavy I\/O. For example, you have a lot of partitioned writes. It is a good option if you also have I\/O workloads that are especially sensitive to latency. For example, you require single-digit millisecond latency per storage operation.\nOption A is incorrect: Increasing disk size for worker nodes alone is not enough. You should move data to the local HDFS storage of Dataproc. Increasing size may help to increase HDFS storage.\nOptions B and D are incorrect: Increasing memory will not help fix the issue because the problem is because of intensive disk read\/write.\nReferences:\nhttps:\/\/cloud.google.com\/solutions\/migration\/hadoop\/migrating-apache-spark-jobs-to-cloud-dataproc"},{"label":"test_3","q_format":"multiple","q_text":"Your company\u2019s on-premises Spark jobs have been migrated to Cloud Dataproc. You are exploring the option to use Preemptible workers to increase the performance of the jobs, while cutting on costs. Which of these rules apply when you add preemptible workers to a Dataproc cluster? (select 2)","answers":[{"ans":"Preemptible workers cannot use persistent disk.","val":false},{"ans":"Preemptible workers cannot store data.","val":true},{"ans":"If a preemptible worker is reclaimed, then a replacement worker must be added manually.","val":false},{"ans":"A Dataproc cluster cannot have only preemptible workers.","val":true}],"q_expl":"Option B as Preemptible instances are disposable and should not be used to store data.\nOption D as a Dataproc cluster cannot be with only preemptible instances. It needs to have two non-preemptible worker nodes.\nDataproc Preemptible VMs:-\u00a0https:\/\/cloud.google.com\/dataproc\/docs\/concepts\/compute\/preemptible-vms\nOption A is wrong as preemptible nodes can have persistent disks.\nOption C is wrong as Dataproc handles the addition and removal of preemptible nodes."},{"label":"test_3","q_format":"single","q_text":"You have developed a Machine Learning model to categorize where the financial transaction was a fraud or not. Testing the Machine Learning model with validation data returns 100% correct answers. What can you infer from the results?","answers":[{"ans":"The model is working extremely well, indicating the hyperparameters are set correctly.","val":false},{"ans":"The model is overfit. There is a problem.","val":true},{"ans":"The model is underfit. There is a problem.","val":false},{"ans":"The model is perfectly fit. You do not need to continue training.","val":false}],"q_expl":"The 100% accuracy is an indicator that the validation data may have somehow gotten mixed in with the training data. You will need new validation data to generate recognizable error.\nOverfitting results when a model performs well on the training set, generating only a small error, but struggles with new or unknown data. In other words, the model overfits itself to the data. Instead of training a model to pick out general features in a given type of data, an overtrained model learns only how to pick out specific features found in the training set."},{"label":"test_3","q_format":"single","q_text":"Your company hosts its analytical data in a BigQuery dataset for analytics. They need to provide controlled access to certain tables and columns within the tables to a third party. How do you design the access with least privilege?","answers":[{"ans":"Grant only DATA VIEWER access to the third party team","val":false},{"ans":"Grant fine grained DATA VIEWER access to the tables and columns within the dataset","val":false},{"ans":"Create Authorized views for tables in a same project and grant access to the teams","val":false},{"ans":"Create Authorized views for tables in a separate project and grant access to the teams","val":true}],"q_expl":"The controlled access can be provided using Authorized views created in a separate project. \nBigQuery Authorized View:-\u00a0https:\/\/cloud.google.com\/bigquery\/docs\/share-access-views \nBigQuery is a petabyte-scale analytics data warehouse that you can use to run SQL queries over vast amounts of data in near realtime. \nGiving a view access to a dataset is also known as creating an authorized view in BigQuery. An authorized view allows you to share query results with particular users and groups without giving them access to the underlying tables. You can also use the view\u2019s SQL query to restrict the columns (fields) the users are able to query. \nWhen you create the view, it must be created in a dataset separate from the source data queried by the view. Because you can assign access controls only at the dataset level, if the view is created in the same dataset as the source data, your data analysts would have access to both the view and the data. \nOptions A & B are wrong as access cannot be controlled over table, but only projects and datasets. \nOption C is wrong as Authorized views should be created in a separate project. If they are created in the same project, the users would have access to the underlying tables as well."},{"label":"test_3","q_format":"single","q_text":"Your company is developing a next generation pet collar that collects biometric information to assist potential millions of families with promoting healthy lifestyles for their pets. Each collar will push 30kb of biometric data In JSON format every 2 seconds to a collection platform that will process and analyze the data providing health trending information back to the pet owners and veterinarians via a web portal. Management has tasked you to architect the collection platform ensuring the following requirements are met.\u00a0 \u00a01. Provide the ability for real-time analytics of the inbound biometric data\u00a0 \u00a02. Ensure processing of the biometric data is highly durable, elastic and parallel\u00a0 \u00a03. The results of the analytic processing should be persisted for data miningWhich architecture outlined below win meet the initial requirements for the platform?","answers":[{"ans":"Utilize Cloud Storage to collect the inbound sensor data, analyze data with Dataproc and save the results to BigQuery.","val":false},{"ans":"Utilize Cloud Pub\/Sub to collect the inbound sensor data, analyze the data with Dataflow and save the results to BigQuery.","val":true},{"ans":"Utilize Cloud Pub\/Sub to collect the inbound sensor data, analyze the data with Dataflow and save the results to Cloud SQL.","val":false},{"ans":"Utilize Cloud Pub\/Sub to collect the inbound sensor data, analyze the data with Dataflow and save the results to Bigtable.","val":false}],"q_expl":"Cloud Pub\/Sub provides elastic and scalable ingestion, Dataflow provides processing and BigQuery analytics.\nIoT:-\u00a0https:\/\/cloud.google.com\/solutions\/iot-overview\nGoogle Cloud Pub\/Sub provides a globally durable message ingestion service. By creating topics for streams or channels, you can enable different components of your application to subscribe to specific streams of data without needing to construct subscriber-specific channels on each device. Cloud Pub\/Sub also natively connects to other Cloud Platform services, helping you to connect ingestion, data pipelines, and storage systems.\nGoogle Cloud Dataflow provides the open Apache Beam programming model as a managed service for processing data in multiple ways, including batch operations, extract-transform-load (ETL) patterns, and continuous, streaming computation. Cloud Dataflow can be particularly useful for managing the high-volume data processing pipelines required for IoT scenarios. Cloud Dataflow is also designed to integrate seamlessly with the other Cloud Platform services you choose for your pipeline.\nGoogle BigQuery provides a fully managed data warehouse with a familiar SQL interface, so you can store your IoT data alongside any of your other enterprise analytics and logs. The performance and cost of BigQuery means you might keep your valuable data longer, instead of deleting it just to save disk space.\nOption A is wrong as Cloud Storage is not an ideal ingestion service for real time high frequency data. Also Dataproc is a fast, easy-to-use, fully-managed cloud service for running Apache Spark and Apache Hadoop clusters in a simpler, more cost-efficient way.\nOption C is wrong as Cloud SQL is a relational database and not suited for analytics data storage.\nOption D is wrong as Bigtable is not ideal for long term analytics data storage."},{"label":"test_3","q_format":"single","q_text":"A retailer has 1PB of historical purchase dataset, which is largely unlabeled. They want to categorize the customer into different groups as per their spend. Which type of Machine Learning algorithm is suited to achieve this?","answers":[{"ans":"Classification","val":false},{"ans":"Regression","val":false},{"ans":"Association","val":false},{"ans":"Clustering","val":true}],"q_expl":"The data is unlabelled, unsupervised learning technique of Clustering can be applied to categorize the data.\nMachine Learning:-\u00a0https:\/\/developers.google.com\/machine-learning\/problem-framing\/cases\nIn unsupervised learning, the goal is to identify meaningful patterns in the data. To accomplish this, the machine must learn from an unlabeled data set. In other words, the model has no hints how to categorize each piece of data and must infer its own rules for doing so.\nOptions A & B are wrong as they are supervised learning techniques.\nIn supervised machine learning, you feed the features and their corresponding labels into an algorithm in a process called training. During training, the algorithm gradually determines the relationship between features and their corresponding labels. This relationship is called the model. Often times in machine learning, the model is very complex.\nOption C is wrong as Association rules is mainly to identify relationship."},{"label":"test_3","q_format":"single","q_text":"A client is using Cloud SQL database to serve infrequently changing lookup tables that host data used by applications. The applications will not modify the tables. As they expand into other geographic regions they want to ensure good performance. What do you recommend?","answers":[{"ans":"Migrate to Cloud Spanner","val":false},{"ans":"Read replicas","val":true},{"ans":"Instance high availability configuration","val":false},{"ans":"Migrate to Cloud Storage","val":false}],"q_expl":"Read replica will increase the availability of the service and can be located closer to the users in the new geographies.\nCloud SQL Replication Options:-\u00a0https:\/\/cloud.google.com\/sql\/docs\/mysql\/replication\/\nCloud SQL provides the ability to replicate a master instance to one or more read replicas. A read replica is a copy of the master that reflects changes to the master instance in almost real time.\nOption A is wrong as Cloud Spanner is suitable for read\/write operations, as the requirement is mainly for read, read replicas would be best fit.\nOption C is wrong as high availability is for failover and not for performance.\nOption D is wrong as Cloud Storage is not ideal storage for relational data."},{"label":"test_3","q_format":"single","q_text":"A financial organization wishes to develop a global application to store transactions happening from different part of the world. The storage system must provide low latency transaction support and horizontal scaling. Which GCP service is appropriate for this use case?","answers":[{"ans":"Bigtable","val":false},{"ans":"Datastore","val":false},{"ans":"Cloud Storage","val":false},{"ans":"Cloud Spanner","val":true}],"q_expl":"Spanner provides Global scale, low latency and the ability to scale horizontally.\nStorage Options:-\u00a0https:\/\/cloud.google.com\/storage-options\/"},{"label":"test_3","q_format":"multiple","q_text":"Which of the following statements about the Wide & Deep Learning model are true? (select 2)","answers":[{"ans":"Wide model is used for memorization, while the deep model is used for generalization.","val":true},{"ans":"Wide model is used for generalization, while the deep model is used for memorization.","val":false},{"ans":"A good use for the wide and deep model is a recommender system.","val":true},{"ans":"A good use for the wide and deep model is a small-scale linear regression problem.","val":false}],"q_expl":"Wide learning model is good for memorization and a Deep learning model is generalization. Both Wide and Deep learning model can help build good recommendation engine.\nWide Deep learning together:-\u00a0https:\/\/ai.googleblog.com\/2016\/06\/wide-deep-learning-better-together-with.html\nThe human brain is a sophisticated learning machine, forming rules by memorizing everyday events (\u201csparrows can fly\u201d and \u201cpigeons can fly\u201d) and generalizing those learnings to apply to things we haven\u2019t seen before (\u201canimals with wings can fly\u201d). Perhaps more powerfully, memorization also allows us to further refine our generalized rules with exceptions (\u201cpenguins can\u2019t fly\u201d). As we were exploring how to advance machine intelligence, we asked ourselves the question\u2014can we teach computers to learn like humans do, by combining the power of memorization and generalization?\nIt\u2019s not an easy question to answer, but by jointly training a wide linear model (for memorization) alongside a deep neural network (for generalization), one can combine the strengths of both to bring us one step closer. At Google, we call it Wide & Deep Learning. It\u2019s useful for generic large-scale regression and classification problems with sparse inputs (categorical features with a large number of possible feature values), such as recommender systems, search, and ranking problems."},{"label":"test_3","q_format":"single","q_text":"Your company is developing an online video hosting platform. Users can upload their videos, which would be available for all the other users to view and share. As a compliance requirement, the videos need to undergo content moderation before it is available for all the users. How would you design your application?","answers":[{"ans":"Use Cloud Vision API to identify video with inappropriate content and mark it for manual checks.","val":false},{"ans":"Use Cloud Natural Language API to identify video with inappropriate content and mark it for manual checks.","val":false},{"ans":"Use Cloud Speech-to-Text API to identify video with inappropriate content and mark it for manual checks.","val":false},{"ans":"Use Cloud Video Intelligence API to identify video with inappropriate content and mark it for manual checks.","val":true}],"q_expl":"Cloud Video Intelligence can be used to perform content moderation. \nCloud Video Intelligence:-\u00a0https:\/\/cloud.google.com\/video-intelligence\/ \nGoogle Cloud Video Intelligence makes videos searchable, and discoverable, by extracting metadata with an easy to use REST API. You can now search every moment of every video file in your catalog. It quickly annotates videos stored in Google Cloud Storage, and helps you identify key entities (nouns) within your video; and when they occur within the video. Separate signals from noise, by retrieving relevant information within the entire video, shot-by-shot, -or per frame. \nIdentify when inappropriate content is being shown in a given video. You can instantly conduct content moderation across petabytes of data and more quickly and efficiently filter your content or user-generated content. \nOption A is wrong as Vision is for image analysis \nOption B is wrong as Natural Language is for text analysis \nOption C is wrong as Speech-to-Text is for audio to text conversion."},{"label":"test_3","q_format":"single","q_text":"You have a Dataflow job that you want to cancel. It is a streaming IoT pipeline, and you want to ensure that any data that is in-flight is processed and written to the output with no data loss. Which of the following commands can you use on the Dataflow monitoring console to stop the pipeline job?","answers":[{"ans":"Cancel","val":false},{"ans":"Drain","val":true},{"ans":"Stop","val":false},{"ans":"Pause","val":false}],"q_expl":"Drain command helps Dataflow process and complete in-flight messages and stops accepting any new ones.\nDataflow stopping a pipeline:-\u00a0https:\/\/cloud.google.com\/dataflow\/docs\/guides\/stopping-a-pipeline\nIf you need to stop a running Cloud Dataflow job, you can do so by issuing a command using either the Cloud Dataflow Monitoring Interface or the Cloud Dataflow Command-line Interface. There are two possible commands you can issue to stop your job: Cancel and Drain.\nNote: The Drain command is supported for streaming pipelines only.\nUsing the Drain option to stop your job tells the Cloud Dataflow service to finish your job in its current state. Your job will immediately stop ingesting new data from input sources. However, the Cloud Dataflow service will preserve any existing resources, such as worker instances, to finish processing and writing any buffered data in your pipeline. When all pending processing and write operations are complete, the Cloud Dataflow service will clean up the GCP resources associated with your job.\nNote: Your pipeline will continue to incur the cost of maintaining any associated GCP resources until all processing and writing has completed.\nUse the Drain option to stop your job if you want to prevent data loss as you bring down your pipeline.\nOption A is wrong as Cancel does not handle in-flight messages and it might result in data loss.\nOptions C & D are wrong as Stop and Pause option do not exist."},{"label":"test_3","q_format":"single","q_text":"You work for a large fast food restaurant chain with over 400,000 employees. You store employee information in Google BigQuery in a Users table consisting of a FirstName field and a LastName field. A member of IT is building an application and asks you to modify the schema and data in BigQuery, so the application can query a FullName field consisting of the value of the FirstName field concatenated with a space, followed by the value of the LastName field for each employee. How can you make that data available while minimizing cost?Create a view in BigQuery that concatenates the FirstName and LastName field values to produce the FullName.","answers":[{"ans":"Add a new column called FullName to the Users table.","val":false},{"ans":"Run an UPDATE statement that updates the FullName column for each user with the concatenation of the FirstName and LastName values.","val":false},{"ans":"Create a Google Cloud Dataflow job that queries BigQuery for the entire Users table, concatenates the FirstName value and LastName value for each user, and loads the proper values for FirstName, LastName, and FullName into a new table in BigQuery.","val":true},{"ans":"Use BigQuery to export the data for the table to a CSV file.","val":false},{"ans":"Create a Google Cloud Dataproc job to process the CSV file and output a new CSV file containing the proper values for FirstName, LastName and FullName. Run a BigQuery load job to load the new CSV file into BigQuery.","val":false}],"q_expl":"The best option is to create a new table with the updated columns. Dataflow provides a serverless NoOps option to convert data.\nOption A is wrong as it is better to create materialized tables instead of views as the query would be executed everytime. Refer BigQuery Best Practices\nBest practice: If possible, materialize your query results in stages.\nIf you create a large, multi-stage query, each time you run it, BigQuery reads all the data that is required by the query. You are billed for all the data that is read each time the query is run.\nInstead, break your query into stages where each stage materializes the query results by writing them to a destination table. Querying the smaller destination table reduces the amount of data that is read and lowers costs. The cost of storing the materialized results is much less than the cost of processing large amounts of data.\nOption B is wrong as DML are limited by quotas.\nMaximum number of combined UPDATE, DELETE, and MERGE statements per day per table \u2014 200\nOption D is wrong as Dataproc would need provisioning of servers and writing scripts."},{"label":"test_3","q_format":"single","q_text":"Your company is hosting its analytics data in BigQuery. All the Data analysts have been provided with the IAM owner role to their respective projects. As a compliance requirement, all the data access logs needs to be captured for audits. Also, the access to the logs needs to be limited to the Auditor team only. How can the access be controlled?","answers":[{"ans":"Export the data access logs using aggregated sink to Cloud Storage in an existing project and grant VIEWER access to the project to the Auditor team","val":false},{"ans":"Export the data access logs using project sink to BigQuery in an existing project and grant VIEWER access to the project to the Auditor team","val":false},{"ans":"Export the data access logs using project sink to Cloud Storage in a separate project and grant VIEWER access to the project to the Auditor team","val":false},{"ans":"Export the data access logs using aggregated sink to Cloud Storage in a separate project and grant VIEWER access to the project to the Auditor team","val":true}],"q_expl":"Data Analysts have OWNER roles to the projects, the logs need to be exported to a separate project which only the Auditor team has access to. Also, as there are multiple projects aggregated export sink can be used to export data access logs from all projects.\nBigQuery Auditing and Aggregated Exports:-\u00a0ttps:\/\/cloud.google.com\/logging\/docs\/export\/aggregated_exports\nYou can create an aggregated export sink that can export log entries from all the projects, folders, and billing accounts of an organization. As an example, you might use this feature to export audit log entries from an organization\u2019s projects to a central location.\nOptions A & B are wrong as the export needs to be in separate project.\nOption C is wrong as you need to use aggregated sink instead of project sink, as it would capture logs from all projects."},{"label":"test_3","q_format":"single","q_text":"A company\u2019s BigQuery data is currently stored in external CSV files in Cloud Storage. As the data has increased over the period of time, the query performance has dropped. What steps can help improve the query performance maintaining the cost-effectiveness?","answers":[{"ans":"Import the data into BigQuery for better performance.","val":true},{"ans":"Request more slots for greater capacity to improve performance.","val":false},{"ans":"Divide the data into partitions based on date.","val":false},{"ans":"Time to move to Cloud Bigtable; it is faster in all cases.","val":false}],"q_expl":"The performance issue is because the data is stored in a non-optimal format in an external storage medium. \nBigQuery External Data Sources:-\u00a0https:\/\/cloud.google.com\/bigquery\/external-data-sources \nQuery performance for external data sources may not be as high as querying data in a native BigQuery table. If query speed is a priority, load the data into BigQuery instead of setting up an external data source. The performance of a query that includes an external data source depends on the external storage type. For example, querying data stored in Cloud Storage is faster than querying data stored in Google Drive. In general, query performance for external data sources should be equivalent to reading the data directly from the external storage. \nOption B is wrong as there is feature to request more slots. \nOption C is wrong as partitioning of data at source would not improve query time for all use cases. \nOption D is wrong as Bigtable is more ideal for NoSQL data type and can get very expensive"},{"label":"test_3","q_format":"single","q_text":"Your company has a BigQuery dataset created, which is located near Tokyo. For efficiency reasons, the company now wants the dataset duplicated in Germany. How can be dataset be made available to the users in Germany?","answers":[{"ans":"Change the dataset from a regional location to multi-region location, specifying the regions to be included.","val":false},{"ans":"Export the data from BigQuery into a bucket in the new location, and import it into a new dataset at the new location.","val":false},{"ans":"Copy the data from the dataset in the source region to the dataset in the target region using BigQuery commands.","val":false},{"ans":"Export the data from BigQuery into nearby bucket in Cloud Storage. Copy to a new regional bucket in Cloud Storage in the new location and Import into the new dataset.","val":true}],"q_expl":"The dataset location cannot be changed once created. The dataset needs to be copied using Cloud Storage.\nBigQuery Exporting Data:-\u00a0https:\/\/cloud.google.com\/bigquery\/docs\/exporting-data\nYou cannot change the location of a dataset after it is created. Also, you cannot move a dataset from one location to another. If you need to move a dataset from one location to another, follow this process:\n1. Export the data from your BigQuery tables to a regional or multi-region Cloud Storage bucket in the same location as your dataset. For example, if your dataset is in the EU multi-region location, export your data into a regional or multi-region bucket in the EU.There are no charges for exporting data from BigQuery, but you do incur charges for storing the exported data in Cloud Storage. BigQuery exports are subject to the limits on export jobs.\n2. Copy or move the data from your Cloud Storage bucket to a regional or multi-region bucket in the new location. For example, if you are moving your data from the US multi-region location to the Tokyo regional location, you would transfer the data to a regional bucket in Tokyo. Note that transferring data between regions incurs network egress charges in Cloud Storage.\n3. After you transfer the data to a Cloud Storage bucket in the new location, create a new BigQuery dataset (in the new location). Then, load your data from the Cloud Storage bucket into BigQuery.You are not charged for loading the data into BigQuery, but you will incur charges for storing the data in Cloud Storage until you delete the data or the bucket. You are also charged for storing the data in BigQuery after it is loaded. Loading data into BigQuery is subject to the limits on load jobs."},{"label":"test_3","q_format":"single","q_text":"You are deploying 10,000 new Internet of Things devices to collect temperature data in your warehouses globally. You need to process, store and analyze these very large datasets in real time. How should you design the system in Google Cloud?","answers":[{"ans":"Send the data to Google Cloud Datastore and then export to BigQuery.","val":false},{"ans":"Send the data to Google Cloud Pub\/Sub, stream Cloud Pub\/Sub to Google Cloud Dataflow, and store the data in Google BigQuery.","val":true},{"ans":"Send the data to Cloud Storage and then spin up an Apache Hadoop cluster as needed in Google Cloud Dataproc whenever analysis is required.","val":false},{"ans":"Export logs in batch to Google Cloud Storage and then spin up a Google Cloud SQL instance, import the data from Cloud Storage, and run an analysis as needed.","val":false}],"q_expl":"The need to ingest it, transform and store the Cloud Pub\/Sub, Cloud Dataflow, BigQuery is ideal stack to handle the IoT data.\nIoT:-\u00a0https:\/\/cloud.google.com\/solutions\/iot-overview#ingestion\nGoogle Cloud Pub\/Sub provides a globally durable message ingestion service. By creating topics for streams or channels, you can enable different components of your application to subscribe to specific streams of data without needing to construct subscriber-specific channels on each device. Cloud Pub\/Sub also natively connects to other Cloud Platform services, helping you to connect ingestion, data pipelines, and storage systems.\nGoogle Cloud Dataflow provides the open Apache Beam programming model as a managed service for processing data in multiple ways, including batch operations, extract-transform-load (ETL) patterns, and continuous, streaming computation. Cloud Dataflow can be particularly useful for managing the high-volume data processing pipelines required for IoT scenarios. Cloud Dataflow is also designed to integrate seamlessly with the other Cloud Platform services you choose for your pipeline.\nGoogle BigQuery provides a fully managed data warehouse with a familiar SQL interface, so you can store your IoT data alongside any of your other enterprise analytics and logs. The performance and cost of BigQuery means you might keep your valuable data longer, instead of deleting it just to save disk space.\nSample Arch \u2013 Mobile Gaming Analysis Telemetry:-\u00a0https:\/\/cloud.google.com\/solutions\/mobile\/mobile-gaming-analysis-telemetry\nOption A is wrong as the Datastore is not an ideal ingestion service.\nOption C is wrong as Cloud Storage is not an ideal ingestion service and Dataproc is not a data warehousing solution.\nOption D is wrong as Cloud SQL is not a data warehousing solution."},{"label":"test_3","q_format":"single","q_text":"You have a table that includes a nested column called \u201ccity\u201d inside a column called \u201cperson\u201d, but when you try to submit the following query in BigQuery, it gives you an error:SELECT person FROM \u2032project1.example.table1\u2032 WHERE city = \u201cLondon\u201dHow would you correct the error?","answers":[{"ans":"Add \u201c, UNNEST(person)\u201c before the WHERE clause.","val":true},{"ans":"Change \u201cperson\u201c to \u201cperson.city\u201c.","val":false},{"ans":"Change \u201cperson\u201c to \u201ccity.person\u201c.","val":false},{"ans":"Add \u201c, UNNEST(city)\u201c before the WHERE clause.","val":false}],"q_expl":"The person column needs to be UNNEST for the nested city field to be used directly in the WHERE clause. Also, note this is standard SQL query by the reference of the table.\nBigQuery Nested Query:-\u00a0https:\/\/cloud.google.com\/bigquery\/docs\/reference\/standard-sql\/migrating-from-legacy-sql\n#standardSQL SELECT page.title FROM \u2032bigquery-public-data.samples.github_nested\u2032, UNNEST(payload.pages) AS page WHERE page.page_name IN (\u2018db_jobskill\u2019, \u2018Profession\u2019);"},{"label":"test_3","q_format":"single","q_text":"Which TensorFlow function can you use to configure a categorical column if you don\u2019t know all of the possible values for that column?","answers":[{"ans":"categorical_column_with_vocabulary_list","val":false},{"ans":"categorical_column_with_hash_bucket","val":true},{"ans":"categorical_column_with_unknown_values","val":false},{"ans":"sparse_column_with_keys","val":false}],"q_expl":"If you know the set of all possible feature values of a column and there are only a few of them, you can use categorical_column_with_vocabulary_list. Each key in the list will get assigned an auto-incremental ID starting from 0.\nWhat if we don\u2019t know the set of possible values in advance? Not a problem. We can use categorical_column_with_hash_bucket instead. What will happen is that each possible value in the feature column occupation will be hashed to an integer ID as we encounter them in training.\nReference:\u00a0https:\/\/www.tensorflow.org\/tutorials\/wide"},{"label":"test_3","q_format":"single","q_text":"You are writing highly-confidential data related to customers\u2019 personally identifiable information (PII). The security team is concerned about how secure the network connection between the instances and Google Storage buckets. Security team proposes to use encryption keys generated by security team. Those keys will be rotated every 30 days for more security. As a data engineer, what should you do to satisfy security team\u2019s requirement?","answers":[{"ans":"Upload encryption key provided by security team to Cloud Key Management Service (KMS) and use the key to encrypt data when writing to Google Storage.","val":false},{"ans":"Create symmetric keys using Cloud Key Management Service (KMS) and use those to encrypt data when writing to Google Storage. Create new keys every 30 days.","val":false},{"ans":"Create asymmetric keys using Cloud Key Management Service (KMS) and use those to encrypt data when writing to Google Storage. Create new keys every 30 days.","val":false},{"ans":"Supply the encryption key provided by security team and reference it as part of the API service calls to encrypt data in Cloud Storage.","val":true}],"q_expl":"Customer-Supplied Encryption Keys (CSEK) are a feature in Google Cloud Storage and Google Compute Engine. If you supply your own encryption keys, Google uses your key to protect the Google- generated keys used to encrypt and decrypt your data.\nWhen you use Customer-Supplied Encryption Keys in Cloud Storage, you provide a raw CSEK as part of an API call. This key is transmitted from the Google front end to the storage system\u2019s memory. This key is used as the key-encryption key in Google Cloud Storage for your data.\n\nThe raw CSEK is used to unwrap wrapped chunk keys, to create raw chunk keys in memory. These are used to decrypt data chunks stored in the storage systems. These keys are used as the data encryption keys (DEK) in Google Cloud Storage for your data.\nA is incorrect: the Security team does not recommend storing encryption keys in the cloud.\nB & C are incorrect: the Security team doesn\u2019t allow user-generated keys from KMS.\nReference:\u00a0https:\/\/cloud.google.com\/security\/encryption-at-rest\/customer- supplied-encryption-keys\/"},{"label":"test_3","q_format":"single","q_text":"A company is building an image tagging pipeline. Which service should be used in the icon with the question mark in the diagram?","answers":[{"ans":"Cloud Datastore","val":false},{"ans":"Cloud Dataflow","val":false},{"ans":"Cloud Pub\/Sub","val":true},{"ans":"Cloud Bigtable","val":false}],"q_expl":"Cloud Storage upload events can push Cloud Pub\/Sub to trigger a Cloud Function to ingest and process the image.\nCloud Storage Pub\/Sub Notifications:-\u00a0https:\/\/cloud.google.com\/storage\/docs\/pubsub-notifications\nCloud Pub\/Sub Notifications sends information about changes to objects in your buckets to Cloud Pub\/Sub, where the information is added to a Cloud Pub\/Sub topic of your choice in the form of messages. For example, you can track objects that are created and deleted in your bucket. Each notification contains information describing both the event that triggered it and the object that changed.\nCloud Pub\/Sub Notifications are the recommended way to track changes to objects in your Cloud Storage buckets because they\u2019re faster, more flexible, easier to set up, and more cost-effective.\nOptions A, B & D are wrong as they cannot be configured for notifications from Cloud Storage."},{"label":"test_3","q_format":"single","q_text":"A company has a new IoT pipeline. Which services will make this design work?\n    Select the services that should be used to replace the icons with the number \u201c1\u201d and number \u201c2\u201d in the diagram.","answers":[{"ans":"Cloud IoT Core, Cloud Datastore","val":false},{"ans":"Cloud Pub\/Sub, Cloud Storage","val":false},{"ans":"Cloud IoT Core, Cloud Pub\/Sub","val":true},{"ans":"App Engine, Cloud IoT Core","val":false}],"q_expl":"Device data captured by Cloud IoT Core gets published to Cloud Pub\/Sub, which can then trigger Dataflow and Cloud Functions.\nCloud IoT Core:-\u00a0https:\/\/cloud.google.com\/iot-core\/\nCloud IoT Core is a fully managed service that allows you to easily and securely connect, manage, and ingest data from millions of globally dispersed devices. Cloud IoT Core, in combination with other services on Cloud IoT platform, provides a complete solution for collecting, processing, analyzing, and visualizing IoT data in real time to support improved operational efficiency.\nCloud IoT Core, using Cloud Pub\/Sub underneath, can aggregate dispersed device data into a single global system that integrates seamlessly with Google Cloud data analytics services. Use your IoT data stream for advanced analytics, visualizations, machine learning, and more to help improve operational efficiency, anticipate problems, and build rich models that better describe and optimize your business."},{"label":"test_3","q_format":"single","q_text":"You are working on a sensitive project involving private user data. You have set up a project on Google Cloud Platform to house your work internally. An external consultant is going to assist with coding a complex transformation in a Google Cloud Dataflow pipeline for your project. How should you maintain users\u2019 privacy?","answers":[{"ans":"Grant the consultant the Viewer role on the project.","val":false},{"ans":"Grant the consultant the Cloud Dataflow Developer role on the project.","val":false},{"ans":"Create a service account and allow the consultant to log on with it.","val":false},{"ans":"Create an anonymized sample of the data for the consultant to work with in a different project.","val":true}],"q_expl":"The best option to maintain user privacy while collaborating with an external consultant on a Dataflow pipeline is:\nD. Create an anonymized sample of the data for the consultant to work with in a different project.\nHere\u2019s why the other options are not ideal for user privacy:\n\nA. Grant the consultant the Viewer role:\u00a0This role allows viewing of all resources in the project, including potentially sensitive user data.\nB. Grant the consultant the Cloud Dataflow Developer role:\u00a0This role allows creating and modifying Dataflow pipelines. While the consultant might not directly access user data, they could potentially write code that exposes it.\nC. Create a service account:\u00a0While a service account can limit access, the consultant would still need permissions to access the user data within the pipeline.\n\nExplanation of Option D:\nBy creating an anonymized sample of the data, you provide the consultant with the necessary information to work on the transformation logic without exposing real user data. This ensures:\n\nPrivacy Protection:\u00a0User data remains secure, as the consultant never interacts with the real data.\nFunctionality:\u00a0The anonymized sample preserves the data structure and allows the consultant to develop the transformation logic effectively.\n\nAdditional Considerations:\n\nYou can define the level of anonymization based on the complexity of the transformation.\nEnsure the anonymized data doesn\u2019t contain any personally identifiable information (PII).\nConsider using Google Cloud Data Loss Prevention (DLP) to help anonymize the data.\n\nRemember: User data privacy is paramount. This approach minimizes the risk of exposing sensitive information while enabling collaboration on the Dataflow pipeline."},{"label":"test_3","q_format":"multiple","q_text":"Select all true statements about Cloud Datastore entities. (select 2)","answers":[{"ans":"Entity values for a given property must be of the same data type.","val":true},{"ans":"Entity keys can have manually generated numeric ids.","val":false},{"ans":"Entities of the same kind can have different properties.","val":true},{"ans":"Entities of the same kind must have the same properties.","val":false}],"q_expl":"Entities of the same kind can have different properties. This means that entities with the same kind can have different sets of properties, allowing for flexibility in data modeling.\nEntity values for a given property must be of the same data type. For a specific property within an entity, all values must be of the same data type. This ensures consistency and data integrity.\n\nThe other statements are incorrect:\n\nEntity keys can have manually generated numeric ids. While you can manually specify the entity key for a new entity, Cloud Datastore will automatically generate a unique numeric ID for the entity if you don\u2019t provide one.\nEntities of the same kind must have the same properties. This is not true. Entities of the same kind can have different properties, allowing for flexibility in data modeling.\n\n[1] Cloud Datastore documentation: https:\/\/cloud.google.com\/datastore\/docs\/concepts\/entities\n[2] Creating and retrieving entities documentation:\u00a0https:\/\/cloud.google.com\/datastore\/docs\/quickstart#create_entities"},{"label":"test_3","q_format":"single","q_text":"You are using Cloud SQL with MySQL database and replication for better read performance. Sometimes a read replica becomes unavailable for a short time once or twice a month. No administrative operation is performed when an incident occurs. What could be the cause of this?","answers":[{"ans":"A backup is being performed on this read replica.","val":false},{"ans":"Maintenance updates. Read replicas can receive updates at any time.","val":true},{"ans":"The read replica is being promoted to a standalone instance.","val":false},{"ans":"The primary instance is failing over to the read replica.","val":false}],"q_expl":"The most likely cause of a read replica becoming unavailable for a short time in Cloud SQL with MySQL database is:\nB. Maintenance updates. Read replicas can receive updates at any time.\nCloud SQL performs maintenance updates on read replicas to keep them up-to-date with the latest features and security patches. These updates can temporarily affect the availability of the read replica.\nThe other options are less likely:\n\nA backup is being performed on this read replica: While backups can temporarily impact the performance of a read replica, they usually don\u2019t cause it to become completely unavailable.\nThe read replica is being promoted to a standalone instance: This is a rare operation and would typically involve a planned outage.\nThe primary instance is failing over to the read replica: This would result in the read replica becoming the primary instance, not becoming unavailable.\n\nTherefore, maintenance updates are the most likely cause of the observed behavior."},{"label":"test_3","q_format":"single","q_text":"Your company is migrating to the Google cloud and looking for HBase alternative. Current solution uses a lot of custom code using the observer coprocessor. You are required to find the best alternative for migration while using managed services, is possible?","answers":[{"ans":"Dataflow","val":false},{"ans":"HBase on Dataproc","val":false},{"ans":"Bigtable","val":true},{"ans":"BigQuery","val":false}],"q_expl":"The best alternative for HBase on Google Cloud Platform (GCP) that supports custom code using the observer coprocessor is Bigtable.\nHere\u2019s a breakdown of the options and why Bigtable is the best fit:\nDataflow:\n\nPros:\nScalable and flexible data processing framework\n\nCons:\nNot designed for real-time, low-latency data access\nDoesn\u2019t support observer coprocessors directly\n\n\nHBase on Dataproc:\n\nPros:\nManaged HBase cluster with familiar API\n\nCons:\nRequires managing and scaling the cluster\nMay not have all the features and performance of native Bigtable\n\n\nBigtable:\n\nPros:\nFully managed, scalable, and high-performance NoSQL database\nSupports custom code using Cloud Dataflow or Cloud Functions for custom processing\nCan be used with a variety of data access patterns, including real-time, batch, and streaming\n\nCons:\nMay have a slightly different API than HBase\n\n\nWhile Dataflow is a great option for data processing, it\u2019s not designed for real-time, low-latency data access. HBase on Dataproc provides a managed HBase cluster but requires more management and may not have all the features of native Bigtable. Bigtable offers a fully managed, scalable, and high-performance NoSQL database with support for custom code and is the best fit for migrating from HBase while using managed services."},{"label":"test_3","q_format":"single","q_text":"A machine learning startup is planning to expand the use of BigQuery. You need to investigate whether the company should invest in Flat-rate billing for BigQuery. What tools and data should you use?","answers":[{"ans":"You should use Cloud Logging and CPU utilization metrics.","val":false},{"ans":"You should use Cloud Logging and audit log data.","val":false},{"ans":"You should use Cloud Monitoring and utilization metrics.","val":false},{"ans":"You should use Cloud Monitoring and CPU utilization metrics.","val":true}],"q_expl":"The correct option is: \u201cYou should use Cloud Monitoring and CPU utilization metrics.\u201c\nTo determine whether investing in flat-rate billing for BigQuery is a wise decision, monitoring and analyzing the utilization of the service is necessary. CPU utilization metrics can help in understanding how much computing power is being used by the service and if it is experiencing any performance issues. Cloud Monitoring is a tool provided by Google Cloud Platform (GCP) that can be used to monitor and analyze the performance of GCP services such as BigQuery.\nCloud Logging is another GCP tool used for collecting, analyzing, and monitoring log data. Although it can provide useful information for troubleshooting, it is not the best tool for monitoring the utilization of BigQuery.\nAudit log data can provide information about who has accessed BigQuery and what actions were taken, but it does not provide information on utilization and performance.\nReference:\nGoogle Cloud Platform documentation on Cloud Monitoring:\u00a0https:\/\/cloud.google.com\/monitoring\nGoogle Cloud Platform documentation on BigQuery monitoring:\u00a0https:\/\/cloud.google.com\/bigquery\/docs\/monitoring"},{"label":"test_3","q_format":"single","q_text":"You are building storage for files for a data pipeline on Google Cloud. You want to support JSON files. The schema of these files will occasionally change. Your analyst teams will use running aggregate ANSI SQL queries on this data. What should you do?","answers":[{"ans":"Use BigQuery for storage. Provide format files for data load. Update the format files as needed.","val":false},{"ans":"Use BigQuery for storage. Select \u201cAutomatically detect\u201c in the Schema section.","val":true},{"ans":"Use Cloud Storage for storage. Link data as temporary tables in BigQuery and turn on the \u201cAutomatically detect\u201c option in the Schema section of BigQuery.","val":false},{"ans":"Use Cloud Storage for storage. Link data as permanent tables in BigQuery and turn on the \u201cAutomatically detect\u201c option in the Schema section of BigQuery.","val":false}],"q_expl":"Correct answer is B as the requirement is to support occasionally (schema) changing JSON files and aggregate ANSI SQL queries: you need to use BigQuery, and it is quickest to use \u2018Automatically detect\u2019 for schema changes.\nBigQuery Auto-Detection:-\u00a0https:\/\/cloud.google.com\/bigquery\/docs\/schema-detect\nSchema auto-detection is available when you load data into BigQuery, and when you query an external data source.\nWhen auto-detection is enabled, BigQuery starts the inference process by selecting a random file in the data source and scanning up to 100 rows of data to use as a representative sample. BigQuery then examines each field and attempts to assign a data type to that field based on the values in the sample.\nA is not correct because you should not provide format files: you can simply turn on the \u2018Automatically detect\u2019 schema changes flag.\nC and D are not correct as Cloud Storage is not ideal for this scenario; it is cumbersome, adds latency and doesn\u2019t add value."},{"label":"test_3","q_format":"single","q_text":"You are developing an application on Google Cloud that will label famous landmarks in users\u2019 photos. You are under competitive pressure to develop the predictive model quickly. You need to keep service costs low. What should you do?","answers":[{"ans":"Build an application that calls the Cloud Vision API.Inspect the generated MID values to supply the image labels.","val":false},{"ans":"Build an application that calls the Cloud Vision API.Pass landmark locations as base64-encoded strings.","val":true},{"ans":"Build and train a classification model with TensorFlow.Deploy the model using Cloud Machine Learning Engine.","val":false},{"ans":"Pass landmark locations as base64-encoded strings.Build and train a classification model with TensorFlow.","val":false},{"ans":"Deploy the model using Cloud Machine Learning Engine. Inspect the generated MID values to supply the image labels.","val":false}],"q_expl":"correct answer is B as the requirement is to quickly develop a model that generates landmark labels from photos, it can be easily supported by Cloud Vision API.\nCloud Vision:-\u00a0https:\/\/cloud.google.com\/vision\/docs\/\nCloud Vision\u00a0offers both pretrained models via an API and the ability to build custom models using AutoML Vision to provide flexibility depending on your use case.\nCloud Vision API\u00a0enables developers to understand the content of an image by encapsulating powerful machine learning models in an easy-to-use REST API. It quickly classifies images into thousands of categories (such as, \u201csailboat\u201d), detects individual objects and faces within images, and reads printed words contained within images. You can build metadata on your image catalog, moderate offensive content, or enable new marketing scenarios through image sentiment analysis.\nOption A is wrong as you should not inspect the generated MID values; instead, you should simply pass the image locations to the API and use the labels, which are output.\nOptions C & D are wrong as you should not build a custom classification TF model for this scenario, as it would require time."},{"label":"test_3","q_format":"single","q_text":"Your company is in a highly regulated industry. One of your requirements is to ensure external users have access only to the non PII fields information required to do their jobs. You want to enforce this requirement with Google BigQuery. Which access control method would you use?","answers":[{"ans":"Use Primitive role on the dataset","val":false},{"ans":"Use Predefined role on the dataset","val":false},{"ans":"Use Authorized view with the same dataset with proper permissions","val":false},{"ans":"Use Authorized view with the different dataset with proper permissions","val":true}],"q_expl":"The controlled access can be granted using Authorized view. The Authorized view needs to be in a different dataset than the source.\nBigQuery Authorized Views:-\u00a0https:\/\/cloud.google.com\/bigquery\/docs\/share-access-views\nGiving a view access to a dataset is also known as creating an authorized view in BigQuery. An authorized view allows you to share query results with particular users and groups without giving them access to the underlying tables. You can also use the view\u2019s SQL query to restrict the columns (fields) the users are able to query.\nWhen you create the view, it must be created in a dataset separate from the source data queried by the view. Because you can assign access controls only at the dataset level, if the view is created in the same dataset as the source data, your users would have access to both the view and the data.\nOptions A, B & C are wrong as they would provide access to the complete datasets with the source included."},{"label":"test_3","q_format":"single","q_text":"You are building a data pipeline on Google Cloud. You need to select services that will host a deep neural network machine-learning model also hosted on Google Cloud. You also need to monitor and run jobs that could occasionally fail. What should you do?","answers":[{"ans":"Use Cloud Machine Learning to host your model. Monitor the status of the Operation object for \u2018error\u2018 results.","val":false},{"ans":"Use Cloud Machine Learning to host your model. Monitor the status of the Jobs object for \u2018failed\u2018 job states.","val":true},{"ans":"Use a Kubernetes Engine cluster to host your model. Monitor the status of the Jobs object for \u2018failed\u2018 job states.","val":false},{"ans":"Use a Kubernetes Engine cluster to host your model. Monitor the status of Operation object for \u2018error\u2018 results.","val":false}],"q_expl":"The requirement is to host an Machine Learning Deep Neural Network job it is ideal to use the Cloud Machine Learning service. Monitoring works on Jobs object.\nML Engine Managing Jobs:-\u00a0https:\/\/cloud.google.com\/ml-engine\/docs\/tensorflow\/managing-models-jobs\nOption A is wrong as monitoring should not be on Operation object to monitor failures.\nOptions C & D are wrong as you should not use a Kubernetes Engine cluster for Machine Learning jobs."},{"label":"test_3","q_format":"single","q_text":"Your company has data stored in BigQuery in Avro format. You need to export this Avro formatted data from BigQuery into Cloud Storage. What is the best method of doing so from the web console?","answers":[{"ans":"Convert the data to CSV format the BigQuery export options, then make the transfer.","val":false},{"ans":"Use the BigQuery Transfer Service to transfer Avro data to Cloud Storage.","val":false},{"ans":"Click on Export Table in BigQuery, and provide the Cloud Storage location to export to","val":true},{"ans":"Create a Dataflow job to manage the conversion of Avro data to CSV format, then export to Cloud Storage.","val":false}],"q_expl":"BigQuery can export Avro data natively to Cloud Storage. \nBigQuery Exporting Data:-\u00a0https:\/\/cloud.google.com\/bigquery\/docs\/exporting-data \nAfter you\u2019ve loaded your data into BigQuery, you can export the data in several formats. BigQuery can export up to 1 GB of data to a single file. If you are exporting more than 1 GB of data, you must export your data to multiple files. When you export your data to multiple files, the size of the files will vary. \nYou cannot export data to a local file or to Google Drive, but you can save query results to a local file. The only supported export location is Google Cloud Storage. \nFor Export format, choose the format for your exported data: CSV, JSON (Newline Delimited), or Avro. \nOption A is wrong as BigQuery can export Avro data natively to Cloud Storage and does not need to be converted to CSV format. \nOption B is wrong as BigQuery Transfer Service is for moving BigQuery data to Google SaaS applications (AdWords, DoubleClick, etc.). You will want to do a normal export of data, which works with Avro formatted data. \nOption D is wrong as Google Cloud Dataflow can be used to read data from BigQuery instead of manually exporting it, but doesn\u2019t work through console."},{"label":"test_3","q_format":"single","q_text":"You need to take streaming data from thousands of Internet of Things (IoT) devices, ingest it, run it through a processing pipeline, and store it for analysis. You want to run SQL queries against your data for analysis. What services in which order should you use for this task?","answers":[{"ans":"Cloud Dataflow, Cloud Pub\/Sub, BigQuery","val":false},{"ans":"Cloud Pub\/Sub, Cloud Dataflow, Cloud Dataproc","val":false},{"ans":"Cloud Pub\/Sub, Cloud Dataflow, BigQuery","val":true},{"ans":"App Engine, Cloud Dataflow, BigQuery","val":false}],"q_expl":"The need to ingest it, transform and store the Cloud Pub\/Sub, Cloud Dataflow, BigQuery is ideal stack to handle the IoT data. \nIoT:-\u00a0https:\/\/cloud.google.com\/solutions\/iot-overview#ingestion \nGoogle Cloud Pub\/Sub provides a globally durable message ingestion service. By creating topics for streams or channels, you can enable different components of your application to subscribe to specific streams of data without needing to construct subscriber-specific channels on each device. Cloud Pub\/Sub also natively connects to other Cloud Platform services, helping you to connect ingestion, data pipelines, and storage systems. \nGoogle Cloud Dataflow provides the open Apache Beam programming model as a managed service for processing data in multiple ways, including batch operations, extract-transform-load (ETL) patterns, and continuous, streaming computation. Cloud Dataflow can be particularly useful for managing the high-volume data processing pipelines required for IoT scenarios. Cloud Dataflow is also designed to integrate seamlessly with the other Cloud Platform services you choose for your pipeline. \nGoogle BigQuery provides a fully managed data warehouse with a familiar SQL interface, so you can store your IoT data alongside any of your other enterprise analytics and logs. The performance and cost of BigQuery means you might keep your valuable data longer, instead of deleting it just to save disk space. \nSample Arch \u2013 Mobile Gaming Analysis Telemetry:-\u00a0https:\/\/cloud.google.com\/solutions\/mobile\/mobile-gaming-analysis-telemetry \nProcessing game client and game server events in real time \nOption A is wrong as the stack is correct, however the order is not correct. \nOption B is wrong as Dataproc is not an ideal tool for analysis. Cloud Dataproc is a fast, easy-to-use, fully-managed cloud service for running Apache Spark and Apache Hadoop clusters in a simpler, more cost-efficient way. \nOption D is wrong as App Engine is not an ideal ingestion tool to handle IoT data."},{"label":"test_3","q_format":"single","q_text":"You have 250,000 devices which produce a JSON device status event every 10 seconds. You want to capture this event data for outlier time series analysis. What should you do?","answers":[{"ans":"Ship the data into BigQuery.Develop a custom application that uses the BigQuery API to query the dataset and displays device outlier data based on your business requirements.","val":false},{"ans":"Ship the data into BigQuery.Use the BigQuery console to query the dataset and display device outlier data based on your business requirements.","val":false},{"ans":"Ship the data into Cloud Bigtable.Use the Cloud Bigtable cbt tool to display device outlier data based on your business requirements.","val":true},{"ans":"Ship the data into Cloud Bigtable.Install and use the HBase shell for Cloud Bigtable to query the table for device outlier data based on your business requirements.","val":false}],"q_expl":"The time series data with its data type, volume, and query pattern best fits BigTable capabilities.\nBigtable Time Series data and CBT:-\u00a0https:\/\/cloud.google.com\/bigtable\/docs\/schema-design-time-series\nOptions A & B are wrong as BigQuery is not suitable for the query pattern in this scenario.\nOption D is wrong as you can use the simpler method of \u2018cbt tool\u2019 to support this scenario."},{"label":"test_3","q_format":"single","q_text":"Your company is planning the infrastructure for a new large-scale application that will need to store over 100 TB or a petabyte of data in NoSQL format for Low-latency read\/write and High-throughput analytics. Which storage option should you use?","answers":[{"ans":"Cloud Bigtable","val":true},{"ans":"Cloud Spanner","val":false},{"ans":"Cloud SQL","val":false},{"ans":"Cloud Datastore","val":false}],"q_expl":"Bigtable is an ideal solution to provide low latency, high throughput data processing storage option with analytics\nStorage Options:-\u00a0https:\/\/cloud.google.com\/storage\/docs\/\nOptions B & C are wrong as they are relational databases\nOption D is wrong as Cloud Datastore is not ideal for analytics."},{"label":"test_3","q_format":"single","q_text":"Your customer is moving their corporate applications to Google Cloud Platform. The security team wants detailed visibility of all projects in the organization. You provision the Google Cloud Resource Manager and set up yourself as the org admin. What Google Cloud Identity and Access Management (Cloud IAM) roles should you give to the security team?","answers":[{"ans":"Org viewer, project owner","val":false},{"ans":"Org viewer, project viewer","val":true},{"ans":"Org admin, project browser","val":false},{"ans":"Project owner, network admin","val":false}],"q_expl":"The security team only needs visibility to the projects, project viewer provides the same with the best practice of least privilege.\nOrganization & Project access control:-\u00a0https:\/\/cloud.google.com\/resource-manager\/docs\/access-control-org\nOption A is wrong as project owner will provide access however it does not align with the best practice of least privilege.\nOption C is wrong as org admin does not align with the best practice of least privilege.\nOption D is wrong as the user needs to be provided organization viewer access to see the organization."},{"label":"test_3","q_format":"single","q_text":"Your company uses BigQuery as the main data warehouse. Data warehouse is divided into several datasets based on data origin and profile. Data analysts want to access certain data resides in a dataset considered sensitive and should not be openly available to all users. Security team allows only certain tables with limited columns for data analysts to read from. Which of the following actions will you take?","answers":[{"ans":"Create a new dataset in BigQuery.Create authorized views on tables data analysts want to read from.Grant viewer role to data analysts on new dataset.","val":true},{"ans":"Create authorized views on tables data analysts want to read from on the same dataset tables reside in.Grant viewer role to marketing team on the views.Grant data analysts viewer role on these specific tables with specifying what columns to be read from.","val":false},{"ans":"Create a new dataset in BigQuery.Grant viewer role to data analysts on the new dataset.Copy the tables from the current dataset to the new one with only columns allowed.","val":false}],"q_expl":"For BigQuery roles, the lowest permission available is the dataset level. You CANNOT set permissions on the table level.\nTo restrict access to a table, you may use authorized views. An authorized view allows you to share query results with particular users and groups without giving them access to the underlying tables. You can also use the view\u2019s SQL query to restrict the columns (fields) the users are able to query.\nWhen you create the view, it must be created in a dataset separate from the source data queried by the view. Because you can assign access controls only at the dataset level, if the view is created in the same dataset as the source data, your users would have access to both the view and the data.\nAnswer B is incorrect: Authorized views should be created in a separate dataset from tables. You cannot grant permissions on only authorized views as the lowest permission level is data set.\nAnswer C is incorrect: You cannot grant permissions on table level in BigQuery. The lowest level in BigQuery is the dataset level.\nAnswer D is incorrect: No need to create tables when you can use authorized views.\nReference:\u00a0https:\/\/cloud.google.com\/bigquery\/docs\/authorized-views"},{"label":"test_3","q_format":"single","q_text":"You launched a Dataproc cluster to perform some Apache Spark jobs. You are looking for a method to securely transfer web traffic data between your machine\u2019s web browser and Dataproc cluster.How can you achieve this?","answers":[{"ans":"FTP connection","val":false},{"ans":"SSH tunnel","val":true},{"ans":"VPN connection","val":false},{"ans":"Incognito mode","val":false}],"q_expl":"clusters, such as Apache Hadoop and Apache Spark, provide web interfaces. These interfaces can be used to manage and monitor cluster resources and facilities, such as the YARN resource manager, the Hadoop Distributed File System (HDFS), MapReduce, and Spark. Other components or applications that you install on your cluster may also provide web interfaces.\nIt is recommended to create an SSH tunnel for a secure connection between your web browser and Dataproc\u2019s master node. SSH tunnel supports traffic proxying using the SOCKS protocol. To configure your browser to use the proxy, start a new browser session with proxy server parameters.\nReference:\u00a0Dataproc \u2013 Cluster Web Interfaces:\u00a0https:\/\/cloud.google.com\/dataproc\/docs\/concepts\/accessing\/cluster- web-interfaces#connecting_to_the_web_interfaces"},{"label":"test_3","q_format":"single","q_text":"A company uses Airflow to orchestrate its data pipelines and DAGs (Directed Acyclic Graphs), installed and maintained on-premise by DevOps team. The company wants to migrate the data pipelines managed in Airflow to Google Cloud. The company is looking for a migration method which can make DAGs available and migrated without extra code modifications so the data pipelines can be available once migrated. Which service should you use?","answers":[{"ans":"App Engine","val":false},{"ans":"Cloud Function","val":false},{"ans":"Dataflow","val":false},{"ans":"Cloud Composer","val":true}],"q_expl":"Cloud Composer is a fully managed workflow orchestration service built on Apache Airflow. Cloud composer is built specifically to schedule and monitor workflows and take required actions. You can use Cloud Composer to orchestrate dataflow pipeline and create a custom sensor to detect file\u2019s condition if any changes occurred, then it triggers the dataflow pipeline to run again.\nReference:\u00a0Cloud Composer: https:\/\/cloud.google.com\/composer\/"},{"label":"test_3","q_format":"single","q_text":"You regularly use prefetch caching with a Data Studio report to visualize the results of BigQuery queries. You want to minimize service costs. What should you do?","answers":[{"ans":"Set up the report to use the Owner\u2018s credentials to access the underlying data in BigQuery, and direct the users to view the report only once per business day (24-hour period).","val":false},{"ans":"Set up the report to use the Owner\u2018s credentials to access the underlying data in BigQuery, and verify that the \u2018Enable cache\u2018 checkbox is selected for the report.","val":true},{"ans":"Set up the report to use the Viewer\u2018s credentials to access the underlying data in BigQuery, and also set it up to be a \u2018view-only\u2018 report.","val":false},{"ans":"Set up the report to use the Viewer\u2018s credentials to access the underlying data in BigQuery, and verify that the \u2018Enable cache\u2018 checkbox is not selected for the report.","val":false}],"q_expl":"You must set Owner credentials to use the \u2018enable cache\u2019 option in BigQuery. It is also a Google best practice to use the \u2018enable cache\u2019 option when the business scenario calls for using prefetch caching.\nDatastudio data caching:-\u00a0https:\/\/support.google.com\/datastudio\/answer\/7020039?hl=en\nThe prefetch cache is only active for data sources that use owner\u2019s credentials to access the underlying data.\nOptions A, C, & D are wrong as cache auto-expires every 12 hours; a prefetch cache is only for data sources that use the Owner\u2019s credentials and not the Viewer\u2019s credentials"},{"label":"test_3","q_format":"single","q_text":"You want to optimize the performance of an accurate, real-time, weather-charting application. The data comes from 50,000 sensors sending 10 readings a second, in the format of a timestamp and sensor reading. Where should you store the data?","answers":[{"ans":"Google BigQuery","val":false},{"ans":"Google Cloud SQL","val":false},{"ans":"Google Cloud Bigtable","val":true},{"ans":"Google Cloud Storage","val":false}],"q_expl":"Bigtable is a ideal solution for storing time series data. Storing time-series data in Cloud Bigtable is a natural fit. Cloud Bigtable stores data as unstructured columns in rows; each row has a row key, and row keys are sorted lexicographically.\nStorage Options:-\u00a0https:\/\/cloud.google.com\/storage\/docs\/\nOption A is wrong as Google BigQuery is a scalable, fully-managed Enterprise Data Warehouse (EDW) with SQL and fast response times. It is for analytics and OLAP workload, though it also provides storage capacity and price similar to GCS. It cannot handle the required real time ingestion of data.\nOption B is wrong as Google Cloud SQL is a fully-managed MySQL and PostgreSQL relational database service for Structured data and OLTP workloads. It also won\u2019t stand for this type of high ingesting rate in real time.\nOption D is wrong as Google Cloud Storage is a scalable, fully-managed, highly reliable, and cost-efficient object \/ blob store. It cannot stand for this amount of data streaming ingestion rate in real-time."},{"label":"test_3","q_format":"single","q_text":"You have hundreds of IoT devices that generate 1 TB of streaming data per day. Due to latency, messages will often be delayed compared to when they were generated. You must be able to account for data arriving late within your processing pipeline. How can the data processing system be designed?","answers":[{"ans":"Use Cloud SQL to process the delayed messages.","val":false},{"ans":"Enable your IoT devices to generate a timestamp when sending messages. Use Cloud Dataflow to process messages, and use windows, watermarks (timestamp), and triggers to process late data.","val":true},{"ans":"Use SQL queries in BigQuery to analyze data by timestamp.","val":false},{"ans":"Enable your IoT devices to generate a timestamp when sending messages. Use Cloud Pub\/Sub to process messages by timestamp and fix out of order issues.","val":false}],"q_expl":"Cloud Pub\/Sub can help handle the streaming data. However, Cloud Pub\/Sub does not handle the ordering, which can be done using Dataflow and adding watermarks to the messages from the source.\nCloud Pub\/Sub ordering & Subscriber:-\u00a0https:\/\/cloud.google.com\/pubsub\/docs\/ordering\nHow do you assign an order to messages published from different publishers? Either the publishers themselves have to coordinate, or the message delivery service itself has to attach a notion of order to every incoming message. Each message would need to include the ordering information. The order information could be a timestamp (though it has to be a timestamp that all servers get from the same source in order to avoid issues of clock drift), or a sequence number (acquired from a single source with ACID guarantees). Other messaging systems that guarantee ordering of messages require settings that effectively limit the system to multiple publishers sending messages through a single server to a single subscriber.\nTypically, Cloud Pub\/Sub delivers each message once and in the order in which it was published. However, messages may sometimes be delivered out of order or more than once. In general, accommodating more-than-once delivery requires your subscriber to be idempotent when processing messages. You can achieve exactly once processing of Cloud Pub\/Sub message streams using Cloud Dataflow PubsubIO. PubsubIO de-duplicates messages on custom message identifiers or those assigned by Cloud Pub\/Sub. You can also achieve ordered processing with Cloud Dataflow by using the standard sorting APIs of the service. Alternatively, to achieve ordering, the publisher of the topic to which you subscribe can include a sequence token in the message.\nOptions A & C are wrong as SQL and BigQuery do not support ingestion and ordering of IoT data and would need other services like Pub\/Sub.\nOption D is wrong as Cloud Pub\/Sub does not perform ordering of messages."},{"label":"test_3","q_format":"single","q_text":"Your company has its input data hosted in BigQuery. They have existing Spark scripts for performing analysis which they want to reuse. The output needs to be stored in BigQuery for future analysis. How can you set up your Dataproc environment to use BigQuery as an input and output source?","answers":[{"ans":"Use the Bigtable syncing service built into Dataproc.","val":false},{"ans":"Manually use a Cloud Storage bucket to import and export to and from both BigQuery and Dataproc","val":false},{"ans":"Install the BigQuery connector on your Dataproc cluster","val":true},{"ans":"You can only use Cloud Storage or HDFS for your Dataproc input and output.","val":false}],"q_expl":"Dataproc has a BigQuery connector library which allows it directly interface with BigQuery.\nDataproc BigQuery Connector:-\u00a0https:\/\/cloud.google.com\/dataproc\/docs\/concepts\/connectors\/bigquery\nYou can use a BigQuery connector to enable programmatic read\/write access to BigQuery. This is an ideal way to process data that is stored in BigQuery. No command-line access is exposed. The BigQuery connector is a Java library that enables Hadoop to process data from BigQuery using abstracted versions of the Apache Hadoop InputFormat and OutputFormat classes.\nOption A is wrong Bigtable syncing service does not exist.\nOptions B & D are wrong as Dataproc can directly interface with BigQuery."},{"label":"test_3","q_format":"single","q_text":"A client has been developing a pipeline based on PCollections using local programming techniques and is ready to scale up to production. What should they do?","answers":[{"ans":"They should use the Cloud Dataflow Cloud Runner.","val":true},{"ans":"They should upload the pipeline to Cloud Dataproc.","val":false},{"ans":"They should use the local version of runner.","val":false},{"ans":"Import the pipeline into BigQuery.","val":false}],"q_expl":"PCollection indicates it is a Cloud Dataflow pipeline. And the Cloud Runner will enable the pipeline to scale to production levels.\nDataflow Cloud Runner:-\u00a0https:\/\/cloud.google.com\/dataflow\/docs\/\nOptions B & D are wrong as PCollections are related to Dataflow\nOption C is wrong as Local runner is execute the pipeline locally."},{"label":"test_3","q_format":"single","q_text":"You currently have a Bigtable instance you\u2019ve been using for development running a development instance type, using HDD\u2019s for storage. You are ready to upgrade your development instance to a production instance for increased performance. You also want to upgrade your storage to SSD\u2019s as you need maximum performance for your instance. What should you do?","answers":[{"ans":"Upgrade your development instance to a production instance, and switch your storage type from HDD to SSD.","val":false},{"ans":"Export your Bigtable data into a new instance, and configure the new instance type as production with SSD\u2018s","val":true},{"ans":"Run parallel instances where one instance is using HDD and the other is using SSD.","val":false},{"ans":"Use the Bigtable instance sync tool in order to automatically synchronize two different instances, with one having the new storage configuration.","val":false}],"q_expl":"The storage for the cluster cannot be updated. You need to define the new cluster and copy or import the data to it.\nBigtable Choosing HDD vs SSD:-\u00a0https:\/\/cloud.google.com\/bigtable\/docs\/choosing-ssd-hdd\nSwitching between SSD and HDD storage\nWhen you create a Cloud Bigtable instance and cluster, your choice of SSD or HDD storage for the cluster is permanent. You cannot use the Google Cloud Platform Console to change the type of storage that is used for the cluster.\nIf you need to convert an existing HDD cluster to SSD, or vice-versa, you can export the data from the existing instance and import the data into a new instance. Alternatively, you can use a Cloud Dataflow or Hadoop MapReduce job to copy the data from one instance to another. Keep in mind that migrating an entire instance takes time, and you might need to add nodes to your Cloud Bigtable clusters before you migrate your instance.\nOption A is wrong as storage type cannot be changed.\nOptions C & D are wrong as it would have two clusters running at the same time with same data, thereby increasing cost."},{"label":"test_3","q_format":"single","q_text":"Your company receives streaming data from IoT sensors capturing various parameters. You need to calculate a running average for each of the parameter on streaming data, taking into account the data that can arrive late and out of order. How would you design the system?","answers":[{"ans":"Use Cloud Pub\/Sub and Cloud Dataflow with Sliding Time Windows.","val":true},{"ans":"Use Cloud Pub\/Sub and Google Data Studio.","val":false},{"ans":"Cloud Pub\/Sub can guarantee timely arrival and order.","val":false},{"ans":"Use Cloud Dataflow\u2018s built-in timestamps for ordering and filtering.","val":false}],"q_expl":"Cloud Pub\/Sub does not maintain message order and Dataflow can be used to order the messages and as well as calculate average using Sliding Time window.\nPub\/Sub Subscriber:-\u00a0https:\/\/cloud.google.com\/pubsub\/docs\/subscriber\nCloud Pub\/Sub delivers each message once and in the order in which it was published. However, messages may sometimes be delivered out of order or more than once. In general, accommodating more-than-once delivery requires your subscriber to be idempotent when processing messages. You can achieve exactly once processing of Cloud Pub\/Sub message streams using Cloud Dataflow PubsubIO. PubsubIO de-duplicates messages on custom message identifiers or those assigned by Cloud Pub\/Sub. You can also achieve ordered processing with Cloud Dataflow by using the standard sorting APIs of the service. Alternatively, to achieve ordering, the publisher of the topic to which you subscribe can include a sequence token in the message.\nOption B is wrong as Data Studio is more of a visualization tool and does not help in analysis or ordering of messages.\nOption C is wrong as Cloud Pub\/Sub does not guarantee order and arrival.\nOption D is wrong as Dataflow does not provide built-in timestamps for ordering and filtering. It needs to use the watermark\/timestamp introduced either by the publisher source or Cloud Pub\/Sub."},{"label":"test_3","q_format":"single","q_text":"You have spent a few days loading data from comma-separated values (CSV) files into the Google BigQuery table CLICK_STREAM. The column DT stores the epoch time of click events. For convenience, you chose a simple schema where every field is treated as the STRING type. Now, you want to compute web session durations of users who visit your site, and you want to change its data type to the\u00a0TIMESTAMP. You want to minimize the migration effort without making future queries computationally expensive. What should you do?Delete the table CLICK_STREAM, and then re-create it such that the column DT is of the\u00a0TIMESTAMP\u00a0type.Reload the data.Add a column TS of the\u00a0TIMESTAMP\u00a0type to the table CLICK_STREAM, and populate the numeric values from the column DT for each row.","answers":[{"ans":"Reference the column TS instead of the column DT from now on.","val":false},{"ans":"Create a view CLICK_STREAM_V, where strings from the column DT are cast into\u00a0TIMESTAMP\u00a0values. Reference the view CLICK_STREAM_V instead of the table CLICK_STREAM from now on.","val":false},{"ans":"Construct a query to return every row of the table CLICK_STREAM, while using the built-in function to cast strings from the column DT into\u00a0TIMESTAMP\u00a0values. Run the query into a destination table NEW_CLICK_STREAM, in which the column TS is the\u00a0TIMESTAMP\u00a0type.","val":false},{"ans":"Reference the table NEW_CLICK_STREAM instead of the table CLICK_STREAM from now on. In the future, new data is loaded into the table NEW_CLICK_STREAM.","val":true}],"q_expl":"The column type cannot be changed and the column needs to casting loaded into a new table using either SQL Query or import\/export.\nBigQuery Changing Schema:-\u00a0https:\/\/cloud.google.com\/bigquery\/docs\/manually-changing-schemas#changing_a_columns_data_type\nOption A is wrong as with this approach all the data would be lost and needs to be reloaded\nOption B is wrong as numeric values cannot be used directly and would need casting.\nOption C is wrong as view is not materialized views, so the future queries would always be taxed as the casting would be done always."},{"label":"test_3","q_format":"single","q_text":"A company has loaded its complete financial data for last year for analytics into BigQuery. A Data Analyst is concerned that a BigQuery query could be too expensive. Which methods can be used to reduce the number of rows processed by BigQuery?","answers":[{"ans":"Use the\u00a0LIMIT\u00a0clause to limit the number of values in the results.","val":false},{"ans":"Use the\u00a0SELECT\u00a0clause to limit the amount of data in the query.Partition data by date so the query can be more focused.","val":true},{"ans":"Set the Maximum Bytes Billed, which will limit the number of bytes processed but still run the query if the number of bytes requested goes over the limit.","val":false},{"ans":"Use\u00a0GROUP BY\u00a0so the results will be grouped into fewer output values.","val":false}],"q_expl":"SELECT\u00a0with partition would limit the data for querying.\nBigQuery Cost Best Practices:-\u00a0https:\/\/cloud.google.com\/bigquery\/docs\/best-practices-costs\nOption C is wrong as the query would fail and would not execute if the Maximum bytes limit is exceeded by the query.\nOption D is wrong as GROUP BY would return less output, but would still query the entire data."},{"label":"test_3","q_format":"single","q_text":"You company\u2019s on-premises Hadoop and Spark jobs have been migrated to Cloud Dataproc. When using Cloud Dataproc clusters, you can access the YARN web interface by configuring a browser to connect through which proxy?","answers":[{"ans":"HTTPS","val":false},{"ans":"VPN","val":false},{"ans":"SOCKS","val":true},{"ans":"HTTP","val":false}],"q_expl":"The internal services can be accessed using the SOCKS proxy server.\nDataproc \u2013 Connecting to web interfaces:-\u00a0https:\/\/cloud.google.com\/dataproc\/docs\/concepts\/accessing\/cluster-web-interfaces\nYou can connect to web interfaces running on a Cloud Dataproc cluster using your project\u2019s Cloud Shell or the Cloud SDK gcloud command-line tool:\nCloud Shell: The Cloud Shell in the Google Cloud Platform Console has the Cloud SDK commands and utilities pre-installed, and it provides a Web Preview feature that allows you to quickly connect through an SSH tunnel to a web interface port on a cluster. However, a connection to the cluster from Cloud Shell uses local port forwarding, which opens a connection to only one port on a cluster web interface\u2014multiple commands are needed to connect to multiple ports. Also, Cloud Shell sessions automatically terminate after a period of inactivity (30 minutes).\ngcloud command-line tool: The gcloud compute ssh command with dynamic port forwarding allows you to establish an SSH tunnel and run a SOCKS proxy server on top of the tunnel. After issuing this command, you must configure your local browser to use the SOCKS proxy. This connection method allows you to connect to multiple ports on a cluster web interface."},{"label":"test_3","q_format":"single","q_text":"A company has migrated their Hadoop cluster to the cloud and is now using Cloud Dataproc with the same settings and same methods as in the data center. What would you advise them to do to make better use of the cloud environment?","answers":[{"ans":"Upgrade to the latest version of HDFS.","val":false},{"ans":"Change the settings in Hadoop components to optimize for the different kinds of work in the mix.","val":false},{"ans":"Find more jobs to run so the cluster utilizations will cost-justify the expense.","val":false},{"ans":"Store persistent data off-cluster.","val":false},{"ans":"Start a cluster for one kind of work then shut it down when it is not processing data.","val":true},{"ans":"Migrate from Cloud Dataproc to an open source Hadoop Cluster hosted on Compute Engine, because this is the only way to get all the Hadoop customizations needed for efficiency.","val":false}],"q_expl":"Storing persistent data off the cluster allows the cluster to be shut down when not processing data. And it allows separate clusters to be started per job or per kind of work, so tuning is less important.\nDataproc Cloud Storage:-\u00a0https:\/\/cloud.google.com\/dataproc\/docs\/concepts\/connectors\/cloud-storage\nDirect data access \u2013 Store your data in Cloud Storage and access it directly, with no need to transfer it into HDFS first.\nHDFS compatibility \u2013 You can easily access your data in Cloud Storage using the gs:\/\/ prefix instead of hdfs:\/\/.\nInteroperability \u2013 Storing data in Cloud Storage enables seamless interoperability between Spark, Hadoop, and Google services.\nData accessibility \u2013 When you shut down a Hadoop cluster, you still have access to your data in Cloud Storage, unlike HDFS.\nHigh data availability \u2013 Data stored in Cloud Storage is highly available and globally replicated without a loss of performance.\nNo storage management overhead \u2013 Unlike HDFS, Cloud Storage requires no routine maintenance such as checking the file system, upgrading or rolling back to a previous version of the file system, etc.\nQuick startup \u2013 In HDFS, a MapReduce job can\u2019t start until the NameNode is out of safe mode\u2014a process that can take from a few seconds to many minutes depending on the size and state of your data. With Cloud Storage, you can start your job as soon as the task nodes start, leading to significant cost savings over time."},{"label":"test_4","q_format":"single","q_text":"If you\u2019re running a performance test that depends upon Cloud Bigtable, all the choices except one below are recommended steps. Which is NOT a recommended step to follow?","answers":[{"ans":"Do not use a production instance.","val":true},{"ans":"Run your test for at least 10 minutes.","val":false},{"ans":"Before you test, run a heavy pre-test for several minutes.","val":false},{"ans":"Use at least 300 GB of data","val":false}],"q_expl":"If you\u2019re running a performance test that depends upon Cloud Bigtable, be sure to follow these steps as you plan and execute your test:\nUse a production instance. A development instance will not give you an accurate sense of how a production instance performs under load.\nUse at least 300 GB of data. Cloud Bigtable performs best with 1 TB or more of data. However, 300 GB of data is enough to provide reasonable results in a performance test on a 3-node cluster. On larger clusters, use 100 GB of data per node.\nBefore you test, run a heavy pre-test for several minutes. This step gives Cloud Bigtable a chance to balance data across your nodes based on the access patterns it observes.\nRun your test for at least 10 minutes. This step lets Cloud Bigtable further optimize your data, and it helps ensure that you will test reads from disk as well as cached reads from memory.\nReference:\u00a0https:\/\/cloud.google.com\/bigtable\/docs\/performance"},{"label":"test_4","q_format":"single","q_text":"Dataproc clusters contain many configuration files. To update these files, you will need to use the \u2013properties option. The format for the option is: file_prefix:property=_____.","answers":[{"ans":"details","val":false},{"ans":"value","val":true},{"ans":"null","val":false},{"ans":"id","val":false}],"q_expl":"To make updating files and properties easy, the \u2013properties command uses a special format to specify the configuration file and the property and value within the file that should be updated. The formatting is as follows: file_prefix:property=value.\nReference:\u00a0https:\/\/cloud.google.com\/dataproc\/docs\/concepts\/cluster-properties#formatting"},{"label":"test_4","q_format":"single","q_text":"A marketing company wants to build a solution for processing large amounts of customer data to predict customer behavior. They want to be able to run complex machine learning algorithms on this data and scale their processing power as needed. What solution would you recommend for this use case to process large amounts of customer data, run complex machine learning algorithms, and scale processing power as needed?","answers":[{"ans":"Use Cloud Dataflow to process the customer data and run the machine learning algorithms.","val":false},{"ans":"Use BigQuery to process the customer data and run the machine learning algorithms.","val":false},{"ans":"Use Apache Spark ML on Google Cloud Dataproc to process the customer data and run the machine learning algorithms.","val":true},{"ans":"Use Compute Engine instances to process the customer data and run the machine learning algorithms.","val":false}],"q_expl":"Use Apache Spark ML on Google Cloud Dataproc to process the customer data and run the machine learning algorithms. -> Correct.\nThe BigQuery Connector for Apache Spark allows Data Scientists to blend the power of BigQuery\u2018s seamlessly scalable SQL engine with Apache Spark\u2019s Machine Learning capabilities. In this tutorial, we show how to use Dataproc, BigQuery and Apache Spark ML to perform machine learning on a dataset.\nReference:\u00a0\u00a0https:\/\/cloud.google.com\/dataproc\/docs\/tutorials\/bigquery-sparkml"},{"label":"test_4","q_format":"single","q_text":"Cloud Dataproc charges you only for what you really use with _____ billing.","answers":[{"ans":"month-by-month","val":false},{"ans":"minute-by-minute","val":true},{"ans":"week-by-week","val":false},{"ans":"hour-by-hour","val":false}],"q_expl":"One of the advantages of Cloud Dataproc is its low cost. Dataproc charges for what you really use with minute-by-minute billing and a low, ten-minute-minimum billing period.\nReference:\u00a0https:\/\/cloud.google.com\/dataproc\/docs\/concepts\/overview"},{"label":"test_4","q_format":"single","q_text":"The YARN ResourceManager and the HDFS NameNode interfaces are available on a Cloud Dataproc cluster ____.","answers":[{"ans":"application node","val":false},{"ans":"conditional node","val":false},{"ans":"master node","val":true},{"ans":"worker node","val":false}],"q_expl":"The YARN ResourceManager and the HDFS NameNode interfaces are available on a Cloud Dataproc cluster master node. The cluster master-host-name is the name of your Cloud Dataproc cluster followed by an -m suffixfor example, if your cluster is named \u201cmy-cluster\u201d, the master-host-name would be \u201cmy-cluster-m\u201d.\nReference:\u00a0https:\/\/cloud.google.com\/dataproc\/docs\/concepts\/cluster-web-interfaces#interfaces"},{"label":"test_4","q_format":"single","q_text":"In order to securely transfer web traffic data from your computer\u2019s web browser to the Cloud Dataproc cluster you should use a(n) _____.","answers":[{"ans":"VPN connection","val":false},{"ans":"Special browser","val":false},{"ans":"SSH tunnel","val":true},{"ans":"FTP connection","val":false}],"q_expl":"To connect to the web interfaces, it is recommended to use an SSH tunnel to create a secure connection to the master node.\nReference:\u00a0https:\/\/cloud.google.com\/dataproc\/docs\/concepts\/cluster-web-interfaces#connecting_to_the_web_interfaces"},{"label":"test_4","q_format":"single","q_text":"What is the general recommendation when designing your row keys for a Cloud Bigtable schema?","answers":[{"ans":"Include multiple time series values within the row key","val":false},{"ans":"Keep the row keep as an 8 bit integer","val":false},{"ans":"Keep your row key reasonably short","val":true},{"ans":"Keep your row key as long as the field permits","val":false}],"q_expl":"A general guide is to, keep your row keys reasonably short. Long row keys take up additional memory and storage and increase the time it takes to get responses from the Cloud Bigtable server.\nReference:\u00a0https:\/\/cloud.google.com\/bigtable\/docs\/schema-design#row-keys"},{"label":"test_4","q_format":"single","q_text":"Which of the following statements is NOT true regarding Bigtable access roles?","answers":[{"ans":"Using IAM roles, you cannot give a user access to only one table in a project, rather than all tables in a project.","val":false},{"ans":"To give a user access to only one table in a project, grant the user the Bigtable Editor role for that table.","val":true},{"ans":"You can configure access control only at the project level.","val":false},{"ans":"To give a user access to only one table in a project, you must configure access through your application.","val":false}],"q_expl":"For Cloud Bigtable, you can configure access control at the project level. For example, you can grant the ability to:\nRead from, but not write to, any table within the project.\nRead from and write to any table within the project, but not manage instances.\nRead from and write to any table within the project, and manage instances.\nReference:\u00a0https:\/\/cloud.google.com\/bigtable\/docs\/access-control"},{"label":"test_4","q_format":"single","q_text":"For the best possible performance, what is the recommended zone for your Compute Engine instance and Cloud Bigtable instance?","answers":[{"ans":"Have the Compute Engine instance in the furthest zone from the Cloud Bigtable instance.","val":false},{"ans":"Have both the Compute Engine instance and the Cloud Bigtable instance to be in different zones.","val":false},{"ans":"Have both the Compute Engine instance and the Cloud Bigtable instance to be in the same zone.","val":true},{"ans":"Have the Cloud Bigtable instance to be in the same zone as all of the consumers of your data.","val":false}],"q_expl":"It is recommended to create your Compute Engine instance in the same zone as your Cloud Bigtable instance for the best possible performance,\nIf it\u2019s not possible to create a instance in the same zone, you should create your instance in another zone within the same region. For example, if your Cloud\nBigtable instance is located in us-central1-b, you could create your instance in us-central1-f. This change may result in several milliseconds of additional latency for each Cloud Bigtable request.\nIt is recommended to avoid creating your Compute Engine instance in a different region from your Cloud Bigtable instance, which can add hundreds of milliseconds of latency to each Cloud Bigtable request.\nReference:\u00a0https:\/\/cloud.google.com\/bigtable\/docs\/creating-compute-instance"},{"label":"test_4","q_format":"multiple","q_text":"Which row keys are likely to cause a disproportionate number of reads and\/or writes on a particular node in a Bigtable cluster (select 2)?","answers":[{"ans":"A sequential numeric ID","val":true},{"ans":"A timestamp followed by a stock symbol","val":true},{"ans":"A non-sequential numeric ID","val":false},{"ans":"A stock symbol followed by a timestamp","val":false}],"q_expl":"Using a timestamp as the first element of a row key can cause a variety of problems.\nIn brief, when a row key for a time series includes a timestamp, all of your writes will target a single node; fill that node; and then move onto the next node in the cluster, resulting in hotspotting.\nSuppose your system assigns a numeric ID to each of your application\u2019s users. You might be tempted to use the user\u2019s numeric ID as the row key for your table.\nHowever, since new users are more likely to be active users, this approach is likely to push most of your traffic to a small number of nodes. [https:\/\/ cloud.google.com\/bigtable\/docs\/schema-design]\nReference:\u00a0https:\/\/cloud.google.com\/bigtable\/docs\/schema-design-time-series#ensure_that_your_row_key_avoids_hotspotting"},{"label":"test_4","q_format":"single","q_text":"Which is not a valid reason for poor Cloud Bigtable performance?","answers":[{"ans":"The workload isn\u2018t appropriate for Cloud Bigtable.","val":false},{"ans":"The table\u2018s schema is not designed correctly.","val":false},{"ans":"The Cloud Bigtable cluster has too many nodes.","val":true},{"ans":"There are issues with the network connection.","val":false}],"q_expl":"The Cloud Bigtable cluster doesn\u2019t have enough nodes. If your Cloud Bigtable cluster is overloaded, adding more nodes can improve performance. Use the monitoring tools to check whether the cluster is overloaded.\nReference:\u00a0https:\/\/cloud.google.com\/bigtable\/docs\/performance"},{"label":"test_4","q_format":"single","q_text":"Which is the preferred method to use to avoid hotspotting in time series data in Bigtable?","answers":[{"ans":"Field promotion","val":true},{"ans":"Randomization","val":false},{"ans":"Salting","val":false},{"ans":"Hashing","val":false}],"q_expl":"By default, prefer field promotion. Field promotion avoids hotspotting in almost all cases, and it tends to make it easier to design a row key that facilitates queries.\nField promotion involves promoting a time-related field, such as the timestamp, from a nested data structure to a top-level column in Bigtable. By promoting the time-related field, you ensure that writes are distributed across different column qualifiers rather than concentrated on a single column qualifier. This helps to avoid hotspotting and evenly distribute the data across Bigtable, improving performance and scalability for time series data.\nReference:\u00a0https:\/\/cloud.google.com\/bigtable\/docs\/schema-design-time-series#ensure_that_your_row_key_avoids_hotspotting"},{"label":"test_4","q_format":"single","q_text":"When you design a Google Cloud Bigtable schema it is recommended that you _________.","answers":[{"ans":"Avoid schema designs that are based on NoSQL concepts","val":false},{"ans":"Create schema designs that are based on a relational database design","val":false},{"ans":"Avoid schema designs that require atomicity across rows","val":true},{"ans":"Create schema designs that require atomicity across rows","val":false}],"q_expl":"All operations are atomic at the row level. For example, if you update two rows in a table, it\u2019s possible that one row will be updated successfully and the other update will fail. Avoid schema designs that require atomicity across rows.\nReference:\u00a0https:\/\/cloud.google.com\/bigtable\/docs\/schema-design#row-keys"},{"label":"test_4","q_format":"single","q_text":"Cloud Bigtable is Google\u2019s ______ Big Data database service.","answers":[{"ans":"Relational","val":false},{"ans":"mySQL","val":false},{"ans":"NoSQL","val":true},{"ans":"SQL Server","val":false}],"q_expl":"Cloud Bigtable is Google\u2019s NoSQL Big Data database service. It is the same database that Google uses for services, such as Search, Analytics, Maps, and Gmail.\nIt is used for requirements that are low latency and high throughput including Internet of Things (IoT), user analytics, and financial data analysis.\nReference:\u00a0https:\/\/cloud.google.com\/bigtable\/"},{"label":"test_4","q_format":"single","q_text":"When you store data in Cloud Bigtable, what is the recommended minimum amount of stored data?","answers":[{"ans":"500 TB","val":false},{"ans":"1 GB","val":false},{"ans":"1 TB","val":true},{"ans":"500 GB","val":false}],"q_expl":"Cloud Bigtable is not a relational database. It does not support SQL queries, joins, or multi-row transactions. It is not a good solution for less than 1 TB of data.\nReference:\u00a0https:\/\/cloud.google.com\/bigtable\/docs\/overview#title_short_and_other_storage_options"},{"label":"test_4","q_format":"single","q_text":"Cloud Dataproc is a managed Apache Hadoop and Apache _____ service.","answers":[{"ans":"Blaze","val":false},{"ans":"Spark","val":true},{"ans":"Fire","val":false},{"ans":"Ignite","val":false}],"q_expl":"Cloud Dataproc is a managed Apache Spark and Apache Hadoop service that lets you use open source data tools for batch processing, querying, streaming, and machine learning.\nReference:\u00a0https:\/\/cloud.google.com\/dataproc\/docs\/"},{"label":"test_4","q_format":"single","q_text":"An environment safety facility receives thousands of events every 60 seconds from its sensors assembled in different sectors monitoring air pollution in the region. Scientists want to access and query the data for observation and daily reporting. Due to current funding state, their budget is limited and they seek a cost-effective, highly available and ACID-compliant solution supports SQL querying. Which approach would you recommend for such a scenario?","answers":[{"ans":"Use BigQuery to store and query the event data. Enable streaming on BigQuery for data to be loaded in real-time.","val":false},{"ans":"Batch-load data into BigTable with launching 10 nodes to allow high performance.","val":false},{"ans":"Use Cloud SQL to load events into a relational database and allow access to scientists to query.","val":false},{"ans":"Use BigQuery to store and query event data. Batch load the data to BigQuery using its API.","val":true}],"q_expl":"BigQuery supports both batch & streaming data. However, due to the mentioned budget restrictions, the solution would choose the cheaper approach, which is batching data to BigQuery. Batching data to BigQuery is free of charge. Streaming data, on the other hand, is charged by size.\nSo, answer D is correct.\nAnswer A is incorrect: Enabling streaming on BigQuery is not free. Since the budget is limited and the scenario seeks a cost-effective solution, this approach is not recommended over batching.\nAnswer B is incorrect: BigTable does not support SQL querying.\nAnswer C is incorrect: Cloud SQL needs administration and not easily scalable. Cloud SQL does not provide batching tools. BigQuery is a better approach to such a scenario.\nReferences:\nhttps:\/\/cloud.google.com\/bigquery\/streaming-data-into-bigquery\nhttps:\/\/cloud.google.com\/bigquery\/batch\nhttps:\/\/cloud.google.com\/bigquery\/pricing"},{"label":"test_4","q_format":"single","q_text":"Data analysts are using Google Data Studio to build dashboards reading data from BigQuery as a data source. The CTO wants to minimize the costs of BigQuery queries run by dashboards. You suggested enabling predictive (pre-fetch) caching. Which of the following will minimize the costs?","answers":[{"ans":"Restrict data fetch to be once every 24 hours and make sure Data Studio report has view credentials on the BigQuery dataset.","val":false},{"ans":"Enable pre-fetch caching for the report and make sure Data Studio report has view credentials on the BigQuery dataset.","val":false},{"ans":"Enable pre-fetch caching for the report and make sure Data Studio report is an owner on the BigQuery dataset.","val":true},{"ans":"Restrict data fetch to be once every 24 hours and make sure Data Studio report is an owner on the BigQuery dataset.","val":false}],"q_expl":"The predictive (pre-fetch) cache analyzes the dimensions, metrics, and filter controls contained in the report, and predicts the possible queries. Data Studio then executes those queries in the background and stores the responses in the predictive cache. When a query can\u2019t be answered by the responsive cache, Data Studio tries to answer it using this predicted data. The predictive cache is limited in size, so it\u2019s possible your report can issue queries not already contained in the cache. If the query can\u2019t be answered by the predictive cache, Data Studio requests the data from the underlying data set.\nLimitation: The predictive (pre-fetch) cache is only active for data sources that use the owner\u2019s credentials to access the underlying data.\nAnswer A is incorrect: Data Studio caching maximum period is 12 hours.\nAnswer B is incorrect: As stated in the description, owner credentials should be granted on data sets.\nAnswer D is incorrect: Same as A, data fetch caching maximum period is 12 hours.\nReference:\u00a0https:\/\/support.google.com\/datastudio\/answer\/7020039?hl=en"},{"label":"test_4","q_format":"single","q_text":"You have the following BigQuery legacy SQL query :SELECT SUM (amount)FROM TABLE_DATE_RANGE ([some-dataset.orders_],TIMESTAMP (\u20182017-06-01\u2019),TIMESTAMP (\u20182017-09-01\u2019);How can you convert it to standard SQL?","answers":[{"ans":"SELECT SUM(amount)FROM \u2032some-dataset.orders_*\u2032WHERE _TABLE_DATE_RANGE BETWEEN \u201820170601\u2019 AND \u201820170901\u2019;","val":false},{"ans":"SELECT SUM(amount)FROM \u2032some-dataset.orders_*\u2032WHERE _TABLE_SUFFIX BETWEEN \u201820170601\u2019 AND \u201820170901\u2019;","val":true},{"ans":"SELECT SUM(amount)FROM \u2032some-dataset.orders_*\u2032WHERE TABLE_DATE_RANGE BETWEEN \u201820170601\u2019 AND \u201820170901\u2019;","val":false},{"ans":"SELECT SUM(amount)FROM \u2032some-dataset.orders_\u2032WHERE _TABLE_SUFFIX BETWEEN \u201820170601\u2019 AND \u201820170901\u2019;","val":false}],"q_expl":"To restrict the query so that it scans an arbitrary set of tables, use the\u00a0_TABLE_SUFFIX\u00a0pseudo column in the WHERE clause. The\u00a0_TABLE_SUFFIX\u00a0pseudo column contains the values matched by the table wildcard.\nTABLE_DATE_RANGE queries daily tables that overlap with the time range between and . This function is a\u00a0LEGACY\u00a0SQL function."},{"label":"test_4","q_format":"single","q_text":"You have the following Dataproc cluster configuration:Master Node:* number in cluster: 1* n1-standard-8* 500 GB attached persistent diskWorker Nodes:* number in cluster: 3* n1-standard-8* 500 GB attached persistent diskHow many virtual CPUs does your cluster have?","answers":[{"ans":"32","val":true},{"ans":"16","val":false},{"ans":"8","val":false},{"ans":"24","val":false}],"q_expl":"This Dataproc cluster has 32 virtual CPUs, 8 for the master and 24 spread across the workers"},{"label":"test_4","q_format":"single","q_text":"In a Cloud Storage bucket you have an object that you want to share with an external company. This object contains sensitive data. You want access to the content to be removed after four hours. The external company doesn\u2018t have a Google account to which you can grant specific access. You want to use the most secure method that requires the fewest steps. What should you do?","answers":[{"ans":"You should create a new Cloud Storage bucket specifically for the external company to access. Copy the object to that bucket. Delete the bucket after four hours have passed.","val":false},{"ans":"You should create a signed URL with a four-hour expiration and share the URL with the company.","val":true},{"ans":"You should set object access to public and remove the object after four hours manually.","val":false},{"ans":"You should set object access to public and use object lifecycle management to remove the object after four hours.","val":false}],"q_expl":"signed URLs give time-limited access to a specific Cloud Storage resource. Anyone in possession of the signed URL can use it while it\u2018s active, regardless of whether they have a Google account\nReference:\u00a0https:\/\/cloud.google.com\/storage\/docs\/access-control\/signed-urls"},{"label":"test_4","q_format":"single","q_text":"An e-commerce company uses Cloud Spanner for storing relational data. They want to optimize this solution for queries on non-key columns and also want to maintain the ability to scale horizontally. What should you recommend?","answers":[{"ans":"They should rewrite queries to Dataproc jobs for analytics.","val":false},{"ans":"They should create the appropriate indexes to support query patterns.","val":true},{"ans":"They should move data to Cloud SQL and create the appropriate indexes to support query patterns.","val":false},{"ans":"They should move data to Cloud BigQuery.","val":false}],"q_expl":"When you create a Cloud Spanner instance, you choose the number of compute capacity nodes or processing units to serve your data. However, if the workload of an instance changes, Cloud Spanner doesn\u2018t automatically adjust the size of the instance. This document introduces the Autoscaler tool for Cloud Spanner (Autoscaler), an open source tool that you can use as a companion tool to Cloud Spanner. This tool lets you automatically increase or reduce the number of nodes or processing units in one or more Spanner instances based on how their capacity is being used.\nYou can also create secondary indexes for other columns. Adding a secondary index on a column makes it more efficient to look up data in that column."},{"label":"test_4","q_format":"single","q_text":"company is collecting terabytes of real-time data that is growing rapidly. They want to preprocess the data and collect feature data that is needed for the machine learning model. What should you recommend to use?","answers":[{"ans":"They should use Dataflow to preprocess the data and Bigtable to store feature data.","val":true},{"ans":"They should use on-premise Apache Hadoop cluster to preprocess the data and Bigtable to store feature data.","val":false},{"ans":"They should use Dataflow to preprocess the data and Cloud SQL to store feature data.","val":false},{"ans":"They should use Dataproc to preprocess the data and Bigtable to store feature data.","val":false}],"q_expl":"Make sure you\u2018re reading and writing many different rows in your table. Bigtable performs best when reads and writes are evenly distributed throughout your table, which helps Bigtable distribute the workload across all of the nodes in your cluster. If reads and writes cannot be spread across all of your Bigtable nodes, performance will suffer.\nReference:\u00a0https:\/\/cloud.google.com\/dataflow\/docs https:\/\/cloud.google.com\/bigtable\/docs"},{"label":"test_4","q_format":"single","q_text":"You have a dataflow pipeline reads a CSV file daily at 6 am, applies the needed cleansing & transformation on it, then loads it to BigQuery. Occasionally, the CSV file might be modified within the day due to human error or incomplete data. This causes you to manually re-run the dataflow pipeline again. Is there a way to fix this by automatically re-run the pipeline if the file has been modified?","answers":[{"ans":"Use Cloud Scheduler to re-run dataflow after 6am. Check what is the average time the file is modified and schedule based on it.","val":false},{"ans":"Use Dataproc to reprocess the file after 6am. You can use Cloud Functions to launch a Dataproc cluster.","val":false},{"ans":"Use Cloud Composer to rerun dataflow and reprocess the file. Create a custom sensor to detect file condition if changed.","val":true},{"ans":"Use a compute engine to schedule a cron job to run every 10 minutes to check if the file was modified to rerun dataflow.","val":false}],"q_expl":"Cloud Composer\u00a0is a fully managed workflow orchestration service built on Apache Airflow. Cloud composer is built specifically to schedule and monitor workflows and take required actions. You can use Cloud Composer to orchestrate the dataflow pipeline and create a custom sensor to detect the file\u2019s condition if any changes occurred, then it triggers the dataflow pipeline to run again.\nAnswer A is incorrect: Guessing what time scheduler should rerun dataflow is not efficient.\nAnswer B is incorrect: Dataproc is unnecessary in this scenario.\nAnswer D is incorrect: Using a Compute Engine VM is unnecessary because Cloud Composer can orchestrate the Dataflow pipeline\u2019s failure.\nReference:\u00a0https:\/\/cloud.google.com\/composer\/"},{"label":"test_4","q_format":"single","q_text":"Data scientists are testing a TensorFlow model on Google Cloud using four NVIDIA Tesla P100 GPUs to test a TensorFlow model. After experimenting with several use cases, they decide to scale up by using a different machine type for testing. As a data engineer, you are responsible of assisting with choosing the right machine type to reach a better model performance. What should you do?","answers":[{"ans":"Use TPU machine type for testing the TensorFlow on.","val":true},{"ans":"Scale up machine type by using NVIDIA Tesla V100 GPUs.","val":false},{"ans":"Use 8 NVIDIA Tesla K80 GPUs instead of the current 4 P100 GPUs.","val":false},{"ans":"Increase number of Tesla P100 GPUs used until test results return satisfactory performance.","val":false}],"q_expl":"Google built the Tensor Processing Unit (TPU) in order to make it possible for data scientists to achieve business and research breakthroughs ranging from network security to medical diagnoses. Cloud TPU is the custom-designed machine learning ASIC that powers Google products like Translate, Photos, Search, Assistant, and Gmail.\n\nAs for cost, below is a benchmark for cost comparison between TPU & GPU types:\nSo, for this scenario, using TPU machine type is the recommended type to build Tensorflow models on.\nReferences:\u00a0TPU Machine Type:\u00a0https:\/\/cloud.google.com\/tpu\/\nGPU Machine Type:\u00a0https:\/\/cloud.google.com\/gpu\/\nhttps:\/\/cloud.google.com\/blog\/products\/ai-machine-learning\/what-makes-tpus-fine-tuned-for-deep- learning\nUsing GPUs for training models in the cloud:\u00a0https:\/\/cloud.google.com\/ml-engine\/docs\/tensorflow\/using-gpus"},{"label":"test_4","q_format":"single","q_text":"You have deployed a Tensorflow machine learning model using Cloud Machine Learning Engine. The model should be able to handle a high volume of instances in a job to run complex models. The model should also write the output to Google Storage. Which of the following approaches is recommended?","answers":[{"ans":"Use online prediction when using the model. Batch prediction supports asynchronous requests.","val":false},{"ans":"Use batch prediction when using the model. Batch prediction supports asynchronous requests.","val":true},{"ans":"Use batch prediction when using the model to return the results as soon as possible.","val":false},{"ans":"Use online prediction when using the model to return the results as soon as possible.","val":false}],"q_expl":"AI Platform provides two ways to get predictions from trained models: online prediction (sometimes called HTTP prediction), and batch prediction. In both cases, you pass input data to a cloud-hosted machine-learning model and get inferences for each data instance. The differences are shown in the following table:\nBatch prediction can handle a high volume of instances in a job to run complex models. It also writes the output to Google Storage by specified location.\nAnswer A & D are incorrect: Online prediction doesn\u2019t support handling a high volume of instances per job and doesn\u2019t write output to Google Storage.\nAnswer C is incorrect: Batch prediction doesn\u2019t return the output as soon as possible, it supports asynchronous requests.\nReference:\u00a0https:\/\/cloud.google.com\/ml-engine\/docs\/tensorflow\/online-vs-batch- prediction"},{"label":"test_4","q_format":"single","q_text":"You have a Dataflow pipeline that streams data to be stored to BigTable after it has been transformed and enriched. The development team needs to modify the transformation code based on the client\u2019s needs. The pipeline is in production which keeps streaming and any interruption to the pipeline may lead to data loss or unexpected output. How can you make sure the pipeline can be stopped without any consequences?","answers":[{"ans":"Turn off Dataflow pipeline with \u2018cancel\u2019 option.","val":false},{"ans":"Create a new Dataflow pipeline with the new transformation code, then switch data stream to the new pipeline.","val":false},{"ans":"Transfer Dataflow pipeline to write data to Google Storage. Perform the needed changes then transfer pipeline back to write to BigTable and re-process the data written in Google Storage.","val":false},{"ans":"Turn off Dataflow pipeline with \u2018drain\u2019 option.","val":true}],"q_expl":"Using the Drain option to stop your job tells the Cloud Dataflow service to finish your job in its current state. Your job will immediately stop ingesting new data from input sources. When all pending processing and write operations are complete, the Cloud Dataflow service will clean up the GCP resources associated with your job.\nBecause Cancel option immediately halts processing, you may lose any \u201cin-flight\u201d data. \u201cIn-flight\u201d data refers to data that has been read but is still being processed by your pipeline.\nAnswer A is incorrect: Using \u2018Cancel\u2019 option will lead to losing in-flight data.\nAnswer B & C are incorrect: Those options are not necessary.\nReference:\u00a0https:\/\/cloud.google.com\/dataflow\/docs\/guides\/stopping-a- pipeline"},{"label":"test_4","q_format":"single","q_text":"A data science team has collected CSV files with a total size of 80 GB. They plan to store this data on Google Cloud using managed service. They also want to analyze the data using SQL. Which GCP storage service should you recommend for storing these CSV files?","answers":[{"ans":"BigQuery","val":true},{"ans":"Bigtable","val":false},{"ans":"Cloud SQL or Cloud Spanner","val":false},{"ans":"Cloud Storage","val":false}],"q_expl":"Scalability: BigQuery is designed to handle large datasets and can easily scale to accommodate your growing data needs.\nSQL Analysis: BigQuery supports standard SQL, making it easy for data scientists to analyze the data using familiar tools and techniques.\nManaged Service: BigQuery is a fully managed service, so you don\u2019t have to worry about managing infrastructure or maintaining the storage system.\nCost-Effective: BigQuery is a pay-as-you-go service, so you only pay for the resources you use.\n\nWhile Cloud Storage can also store CSV files, it doesn\u2019t provide built-in SQL analysis capabilities. Cloud SQL and Cloud Spanner are more suitable for storing and managing relational databases, but they might not be the most cost-effective or scalable option for a large dataset like 80GB of CSV files.\nTherefore, BigQuery is the ideal choice for storing and analyzing your 80GB CSV files on Google Cloud."},{"label":"test_4","q_format":"single","q_text":"When creating a new Cloud Dataproc cluster with the projects.regions.clusters.create operation, these four values are required: project, region, name, and ____.","answers":[{"ans":"zone","val":false},{"ans":"node","val":false},{"ans":"label","val":false},{"ans":"type","val":false},{"ans":"clusterConfig","val":true}],"q_expl":"The missing value when creating a new Cloud Dataproc cluster with the projects.regions.clusters.create operation is clusterConfig.\nHere are the four required values:\nproject: The ID of the project where the cluster will be created.\nregion: The region where the cluster will be located.\nname: A unique name for the cluster.\nclusterConfig: A configuration object that specifies the details of the cluster, such as the number of workers, the type of machines to use, and the software configuration.\nThe other options (zone, node, label, and type) are not directly required for creating a new Cloud Dataproc cluster using the projects.regions.clusters.create operation. However, they may be included within the clusterConfig object to specify additional details about the cluster.\n\n\nReference: https:\/\/cloud.google.com\/dataproc\/docs\/tutorials\/python-library-example#create_a_new_cloud_dataproc_cluste"},{"label":"test_4","q_format":"multiple","q_text":"Which of the following job types are supported by Cloud Dataproc (select 3 )?","answers":[{"ans":"Hive","val":true},{"ans":"Pig","val":true},{"ans":"YARN","val":false},{"ans":"Spark","val":true}],"q_expl":"Cloud Dataproc provides out-of-the box and end-to-end support for many of the most popular job types, including Spark, Spark SQL, PySpark, MapReduce, Hive, and Pig jobs.\nReference:\u00a0https:\/\/cloud.google.com\/dataproc\/docs\/resources\/faq#what_type_of_jobs_can_i_run"},{"label":"test_4","q_format":"multiple","q_text":"Which of these are examples of a value in a sparse vector? (select 2 )","answers":[{"ans":"[0, 5, 0, 0, 0, 0]","val":false},{"ans":"[0, 0, 0, 1, 0, 0, 1]","val":false},{"ans":"[0, 1]","val":true},{"ans":"[1, 0, 0, 0, 0, 0, 0]","val":true}],"q_expl":"Categorical features in linear models are typically translated into a sparse vector in which each possible value has a corresponding index or id. For example, if there are only three possible eye colors you can represent \u2018eye_color\u2019 as a length 3 vector: \u2018brown\u2019 would become [1, 0, 0], \u2018blue\u2019 would become [0, 1, 0] and \u2018green\u2019 would become [0, 0, 1]. These vectors are called \u201csparse\u201d because they may be very long, with many zeros, when the set of possible values is very large (such as all English words).\n[0, 0, 0, 1, 0, 0, 1] is not a sparse vector because it has two 1s in it. A sparse vector contains only a single 1.\n[0, 5, 0, 0, 0, 0] is not a sparse vector because it has a 5 in it. Sparse vectors only contain 0s and 1s.\nReference:\u00a0https:\/\/www.tensorflow.org\/tutorials\/linear#feature_columns_and_transformations"},{"label":"test_4","q_format":"multiple","q_text":"Which of the following are examples of hyperparameters? (select 2)","answers":[{"ans":"Number of hidden layers","val":true},{"ans":"Number of nodes in each hidden layer","val":true},{"ans":"Biases","val":false},{"ans":"Weights","val":false}],"q_expl":"If model parameters are variables that get adjusted by training with existing data, your hyperparameters are the variables about the training process itself. For example, part of setting up a deep neural network is deciding how many \u201chidden\u201d layers of nodes to use between the input layer and the output layer, as well as how many nodes each layer should use. These variables are not directly related to the training data at all. They are configuration variables. Another difference is that parameters change during a training job, while the hyperparameters are usually constant during a job.\nWeights and biases are variables that get adjusted during the training process, so they are not hyperparameters.\nReferences:\u00a0https:\/\/cloud.google.com\/ml-engine\/docs\/hyperparameter-tuning-overview\nhttps:\/\/cloud.google.com\/ai-platform\/training\/docs\/hyperparameter-tuning-overview#whats_a_hyperparameter"},{"label":"test_4","q_format":"multiple","q_text":"Which of the following are feature engineering techniques? (select 2)","answers":[{"ans":"Hidden feature layers","val":false},{"ans":"Feature prioritization","val":false},{"ans":"Crossed feature columns","val":true},{"ans":"Bucketization of a continuous feature","val":true}],"q_expl":"Selecting and crafting the right set of feature columns is key to learning an effective model.\nBucketization is a process of dividing the entire range of a continuous feature into a set of consecutive bins\/buckets, and then converting the original numerical feature into a bucket ID (as a categorical feature) depending on which bucket that value falls into.\nUsing each base feature column separately may not be enough to explain the data. To learn the differences between different feature combinations, we can add crossed feature columns to the model.\nReference:\nhttps:\/\/www.tensorflow.org\/tutorials\/wide#selecting_and_engineering_features_for_the_model"},{"label":"test_4","q_format":"single","q_text":"What Dataflow concept determines when a Window\u2019s contents should be output based on certain criteria being met?","answers":[{"ans":"Sessions","val":false},{"ans":"OutputCriteria","val":false},{"ans":"Windows","val":false},{"ans":"Triggers","val":true}],"q_expl":"Triggers control when the elements for a specific key and window are output. As elements arrive, they are put into one or more windows by a Window transform and its associated WindowFn, and then passed to the associated Trigger to determine if the Windows contents should be output.\nReference:\u00a0https:\/\/cloud.google.com\/dataflow\/java-sdk\/JavaDoc\/com\/google\/cloud\/dataflow\/sdk\/transforms\/windowing\/Trigger"},{"label":"test_4","q_format":"single","q_text":"What are the minimum permissions needed for a service account used with Google Dataproc?","answers":[{"ans":"Execute to Google Cloud Storage; write to Google Cloud Logging","val":false},{"ans":"Write to Google Cloud Storage; read to Google Cloud Logging","val":false},{"ans":"Execute to Google Cloud Storage; execute to Google Cloud Logging","val":false},{"ans":"Read and write to Google Cloud Storage; write to Google Cloud Logging","val":true}],"q_expl":"Service accounts authenticate applications running on your virtual machine instances to other Google Cloud Platform services. For example, if you write an application that reads and writes files on Google Cloud Storage, it must first authenticate to the Google Cloud Storage API. At a minimum, service accounts used with Cloud Dataproc need permissions to read and write to Google Cloud Storage, and to write to Google Cloud Logging.\nReference:\u00a0https:\/\/cloud.google.com\/dataproc\/docs\/concepts\/service-accounts#important_notes"},{"label":"test_4","q_format":"single","q_text":"A company has over 25TB of data in Avro format stored in on-premise disks. You are migrating the tech stack used to Google Cloud. The current data pipeline built on-premise does the required data transformation and enrichment using Apache Spark. You decide to use Dataproc for data processing. When the migration was approved by the management, one of the base requirements was for data to be highly available and cross-zone durability should be guaranteed. What should you do?","answers":[{"ans":"Use Google Storage to store data. Allow Dataproc cluster to access data from Google Storage.","val":true},{"ans":"Use BigQuery to store data. Install Dataproc-BigQuery connector to access data.","val":false},{"ans":"Use Dataproc cluster\u2019s HDFS namenodes to store data.","val":false},{"ans":"Use BigTable to store data. Use Dataproc-BigTable connector to access data.","val":false}],"q_expl":"When you want to move Hadoop & Spark workloads from an on-premises environment to Google Cloud Platform (GCP), It\u2019s recommended to use Dataproc to run Apache Spark & Hadoop clusters.\nCloud Storage is a good option if:\n1. Your data in ORC, Parquet, Avro, or any other format will be used by different clusters or jobs, and you need data persistence if the cluster terminates.\n2. You need high throughput and your data is stored in files larger than 128 MB.\n3. You need cross-zone durability for your data.\n4. You need data to be highly available\u2014for example, you want to eliminate HDFS NameNode as a single point of failure.\nReference:\u00a0https:\/\/cloud.google.com\/solutions\/migration\/hadoop\/migrating-apache-spark-jobs-to-cloud-dataproc"},{"label":"test_4","q_format":"single","q_text":"You want to build a system that uses a machine learning, image recognition model to detect customers\u2019 faces entering a retail shop, and based on the knowledge base it will return whether the customer is a new, returning or loyal customer. You are building the model using AutoML Vision. After training the model and testing it, you find the model\u2019s accuracy is lower due to overfitting. How can you solve this?","answers":[{"ans":"Images used should be taken from the same exact angle and resolution.","val":false},{"ans":"Instead of manually splitting samples to training and testing sets, allow AutoML Vision to split the sample set.","val":false},{"ans":"Samples used for training should be covering true positives only.","val":false},{"ans":"Images used should be taken from different angles, resolutions and points of view","val":true}],"q_expl":"Google Cloud provies a machine learning service called AutoML to quickly build models for you. AutoML Vision is one of its products which you can start with a training set as little as a dozen photo samples and AutoML takes care of the rest.\nWhile iterating on your model, if the model\u2019s quality levels are not up to expectations, you can go back to earlier steps to improve the quality:\nAutoML Vision allows you to sort the images by how \u201cconfused\u201d the model is, by the true label and its predicted label. Look through these images and make sure they\u2019re labeled correctly.\nConsider adding more images to any labels with low quality.\nYou may need to add different types of images (e.g. wider angle, higher or lower resolution, different points of view).\nConsider removing labels altogether if you don\u2019t have enough training images.\nRemember that machines can\u2019t read your label name; it\u2019s just a random string of letters to them. If you have one label that says \u201cdoor\u201d and another that says \u201cdoor_with_knob\u201d the machine has no way of figuring out the nuance other than the images you provide it.\nAugment your data with more examples of true positives and negatives. Especially important examples are the ones that are close to the decision boundary (i.e. likely to produce confusion but still correctly labeled).\nSpecify your own TRAIN, TEST, VALIDATION split. The tool randomly assigns images, but near-duplicates may end up in TRAIN and VALIDATION which could lead to overfitting and then poor performance on the TEST set.\nOnce you\u2019ve made changes, train and evaluate a new model until you reach a high enough quality level.\nReference:\u00a0https:\/\/cloud.google.com\/vision\/automl\/docs\/evaluate"},{"label":"test_4","q_format":"single","q_text":"You are preparing a dataset as a training set for a machine learning model. You have the following columns chosen as features for the model:Zip codeIncomeAgeWhich feature type each column is?","answers":[{"ans":"3 continuous.","val":false},{"ans":"2 categorical, 1 continuous.","val":false},{"ans":"2 continuous, 1 categorical.","val":true},{"ans":"3 categorical.","val":false}],"q_expl":"In machine learning, features are two types: Categorical & Continuous.\nCategorical features are features with finite values. For example: Country, education level and marital status.\nContinuous features are features with numeric values in a continuous range. For example: Income, latitude & longitude and time.\nFor zip code, while it\u2019s represented as numeric values, it\u2019s considered categorical because it represents regions, means, it marks each region with a number."},{"label":"test_4","q_format":"multiple","q_text":"Your company handles data processing for a number of different clients. Each client prefers to use their own suite of analytics tools, with some allowing direct query access via Google BigQuery. You need to secure the data so that clients cannot see each other\u2018s data. You want to ensure appropriate access to the data.Which three steps should you take (select 3)?","answers":[{"ans":"Load data into different partitions.","val":false},{"ans":"Load data into a different dataset for each client.","val":true},{"ans":"Put each client\u2018s BigQuery dataset into a different table.","val":false},{"ans":"Restrict a client\u2018s dataset to approved users.","val":true},{"ans":"Only allow a service account to access the datasets.","val":false},{"ans":"Use the appropriate identity and access management (IAM) roles for each client\u2018s users.","val":true}],"q_expl":"To ensure appropriate access to data and prevent clients from seeing each other\u2018s data, you should take the following three steps:\nB- Load data into a different dataset for each client: This step helps to keep data separated by client, ensuring that each client has their own data store within BigQuery. By creating different datasets for each client, you can ensure that only authorized users have access to that client\u2018s data.\nD- Restrict a client\u2018s dataset to approved users: This step ensures that only authorized users can access a client\u2018s data. By assigning appropriate access controls to each dataset, you can restrict access to only those users who are approved to access that client\u2018s data.\nF- Use the appropriate identity and access management (IAM) roles for each client\u2018s users: IAM roles are used to grant or restrict access to BigQuery resources. By assigning appropriate roles to each user, you can ensure that they have access to the data they need, while also preventing unauthorized access to other clients\u2018 data.\nOverall, these three steps help to ensure appropriate access to data and prevent clients from seeing each other\u2018s data, which is crucial for maintaining the confidentiality and security of your clients\u2018 data.\nA) Load data into different partitions: This is not the most appropriate solution for ensuring client data is secure from other clients. Partitioning is useful for managing and querying large datasets, but it does not prevent other clients from accessing the data.\nC) Put each client\u2018s BigQuery dataset into a different table: This is not necessary, as each dataset already acts as a container for multiple tables.\nE) Only allow a service account to access the datasets: This is not necessary if appropriate IAM roles are used to control access to the datasets.\nReference:\nGoogle Cloud. (n.d.). Creating and managing BigQuery datasets.\u00a0https:\/\/cloud.google.com\/bigquery\/docs\/datasets#create-dataset\nGoogle Cloud. (n.d.). BigQuery access control.\u00a0https:\/\/cloud.google.com\/bigquery\/docs\/access-control"},{"label":"test_4","q_format":"single","q_text":"Your company is performing data preprocessing for a learning algorithm in Google Cloud Dataflow. Numerous data logs are being are being generated during this step, and the team wants to analyze them. Due to the dynamic nature of the campaign, the data is growing exponentially every hour.The data scientists have written the following code to read the data for a new key features in the logs.BigQueryIO.Read \u2013.named(\u201cReadLogData\u201c).from(\u201cclouddataflow-readonly:samples.log_data\u201c)You want to improve the performance of this data read. What should you do?","answers":[{"ans":"Specify the TableReference object in the code.","val":false},{"ans":"Use .fromQuery operation to read specific fields from the table.","val":true},{"ans":"Use of both the Google BigQuery TableSchema and TableFieldSchema classes.","val":false},{"ans":"Call a transform that returns TableRow objects, where each element in the PCollection represents a single row in the table.","val":false}],"q_expl":"BigQueryIO.read.from()\u00a0directly reads the whole table from BigQuery. This function exports the whole table to temporary files in Google Cloud Storage, where it will later be read from. This requires almost no computation, as it only performs an export job, and later Dataflow reads from GCS (not from BigQuery). BigQueryIO.read.fromQuery() executes a query and then reads the results received after the query execution. Therefore, this function is more time-consuming, given that it requires that a query is first executed (which will incur in the corresponding economic and computational costs)."},{"label":"test_4","q_format":"single","q_text":"A company is using Bigtable in an IoT application. As a Data Engineer, you are investigating long latencies in query response time. The Key Visualizer heatmap shows two areas with hotspots. What could be the cause of hotspots?","answers":[{"ans":"Improperly created queries.","val":false},{"ans":"Improperly used primary index.","val":false},{"ans":"Improperly designed row key.","val":true},{"ans":"Improperly used secondary index.","val":false}],"q_expl":"In Bigtable, the row key design is critical for efficient data access and to avoid hotspots, which can cause increased latency and poor performance. Hotspots occur when many queries or updates access a small number of rows or tablets, which can overload the nodes responsible for those rows or tablets.\nTo avoid hotspots, it is recommended to design row keys that distribute the data evenly across the cluster, use a combination of time-based and unique identifiers, and avoid sequential or monotonically increasing keys.\nSecondary indexes are not natively supported in Bigtable, and queries that rely on secondary indexes can be inefficient and lead to hotspots. Reference: [1]\nReferences:\n[1] Designing Your Schema \u2013 Google Cloud Bigtable Documentation:\u00a0https:\/\/cloud.google.com\/bigtable\/docs\/schema-design"},{"label":"test_4","q_format":"multiple","q_text":"You are building a convolutional neural network (CNN) to solve image multi-class classification problem. Your model is overfitting. Which of the following method would be helpful? (select 3)","answers":[{"ans":"L2 regularization","val":true},{"ans":"Data re-sampling","val":false},{"ans":"Dropout technique","val":true},{"ans":"Reducing the number of images from the training set.","val":false},{"ans":"L1 regularization","val":true}],"q_expl":"The following methods would be helpful to reduce overfitting in a convolutional neural network: \nL2 regularization: This method adds a penalty term to the loss function, which helps to prevent overfitting by reducing the magnitude of the weights in the network. Reference: [1] \nDropout technique: This method randomly drops out a proportion of the neurons in the network during training, which helps to prevent over-reliance on any one feature and encourages the network to learn more robust features. Reference: [2] \nData augmentation: This method generates additional training data by applying transformations such as rotation, scaling, and flipping to the existing images. This can help to reduce overfitting by increasing the diversity of the training set. Reference: [3] \nReducing the number of images from the training set would not be helpful, as this would likely increase overfitting by reducing the amount of data available for training. \nL1 regularization could also be used to prevent overfitting, but it is less commonly used in CNNs compared to L2 regularization. \nReferences: \n[1] Regularization \u2013 TensorFlow Documentation:\u00a0https:\/\/www.tensorflow.org\/tutorials\/keras\/overfit_and_underfit#add_weight_regularization \n[2] Dropout: A Simple Way to Prevent Neural Networks from Overfitting \u2013 Srivastava et al., 2014:\u00a0https:\/\/www.cs.toronto.edu\/~hinton\/absps\/JMLRdropout.pdf \n[3] Data augmentation \u2013 TensorFlow Documentation:\u00a0https:\/\/www.tensorflow.org\/tutorials\/images\/data_augmentation"},{"label":"test_4","q_format":"single","q_text":"Your team is running Redis instance using Cloud Memorystore. You received a notification that memory usage exceeds 80 percent. You don\u2018t want to scale your instance up, but you need to reduce the amount of memory used. What can you do?","answers":[{"ans":"You can switch from Basic Tier to Standard Tier.","val":false},{"ans":"You can set shorter TTLs (time-to-live) and try a different eviction policy.","val":true},{"ans":"You can export the cache.","val":false},{"ans":"You must scale the instance. There is no other option.","val":false}],"q_expl":"The correct option is: \u201cYou can set shorter TTLs (time-to-live) and try a different eviction policy.\u201c\nRedis uses memory as its primary storage mechanism, so it is common to experience high memory usage when running Redis instances. When the memory usage exceeds 80 percent, it is important to take action to prevent potential issues, such as slow queries or even crashes.\nOne way to reduce memory usage is to set shorter TTLs (time-to-live) for the Redis keys. TTLs determine how long a key will be stored in memory before it is automatically removed. By setting shorter TTLs, Redis will automatically remove keys that are no longer needed, freeing up memory.\nAnother option is to try a different eviction policy. Redis offers several eviction policies that determine which keys to remove when memory usage exceeds a certain threshold. By choosing a more aggressive eviction policy, Redis will be more likely to remove keys from memory and reduce memory usage.\nSwitching from Basic Tier to Standard Tier or exporting the cache will not directly reduce memory usage. Upgrading to Standard Tier may provide more memory capacity, but it will not solve the underlying issue of high memory usage. Exporting the cache is a way to create a backup or migrate the data, but it will not reduce memory usage either.\nTherefore, setting shorter TTLs and trying a different eviction policy are the best options to reduce memory usage without scaling up the instance.\nReference:\nGoogle Cloud Memorystore documentation on Redis eviction policies:\u00a0https:\/\/cloud.google.com\/memorystore\/docs\/redis\/redis-configs#eviction-policies"},{"label":"test_4","q_format":"single","q_text":"Your application allows users to publish images. You wan to be careful and prevent from adding images with credit card numbers or other sensitive information. Which GCP service can you use to enable this?","answers":[{"ans":"Cloud SDK","val":false},{"ans":"Data Loss Prevention API","val":true},{"ans":"Cloud KMS","val":false},{"ans":"Cloud Armor","val":false}],"q_expl":"To prevent users from publishing images that contain sensitive information like credit card numbers, you can use the Data Loss Prevention (DLP) API. The DLP API is a GCP service that helps you detect and classify sensitive data within text or images. You can use it to analyze the content of images and automatically redact any sensitive information.\nThe DLP API uses machine learning models to analyze images and identify sensitive data patterns. You can configure the API to detect specific types of sensitive data, such as credit card numbers, social security numbers, or other personally identifiable information (PII). Once the API detects sensitive information, you can use it to redact the information or apply other actions, such as masking or tokenization.\nCloud SDK is a command-line interface tool used for managing GCP resources and applications. Cloud KMS is a service used for managing encryption keys, and Cloud Armor is a service used for protecting applications against Distributed Denial of Service (DDoS) attacks. Neither of these services is directly related to detecting and redacting sensitive information in images.\nTherefore, the best option to prevent users from publishing images with sensitive information is to use the Data Loss Prevention API.\nReference:\nGoogle Cloud Data Loss Prevention API documentation:\u00a0https:\/\/cloud.google.com\/dlp"},{"label":"test_4","q_format":"single","q_text":"Your company has a hybrid cloud initiative. You have a complex data pipeline that moves data between cloud provider services and leverages services from each of the cloud providers. Which cloud-native service should you use to orchestrate the entire pipeline?","answers":[{"ans":"A. Cloud Dataflow","val":false},{"ans":"B. Cloud Composer","val":true},{"ans":"C. Cloud Dataprep","val":false},{"ans":"D. Cloud Dataproc","val":false}],"q_expl":"Based on the given scenario, the cloud-native service that would be most suitable for orchestrating the entire data pipeline, considering a hybrid cloud environment and the need to leverage services from multiple cloud providers, is Cloud Composer (Option B).\nCloud Composer is a fully managed workflow orchestration service provided by Google Cloud Platform (GCP). It is built on Apache Airflow and offers a flexible and scalable platform for managing and executing complex workflows. With Cloud Composer, you can easily define, schedule, and monitor workflows using Python, making it suitable for orchestrating data pipelines.\nCloud Composer\u00a0supports integration with various cloud provider services, including those from GCP and other major cloud providers like Amazon Web Services (AWS) and Microsoft Azure. This makes it a good fit for the given scenario where the data pipeline involves moving data between different cloud provider services.\nReferences:\n1. Cloud Composer Overview:\u00a0https:\/\/cloud.google.com\/composer\n2. Cloud Composer Documentation:\u00a0https:\/\/cloud.google.com\/composer\/docs\n3. Cloud Composer and Apache Airflow:\u00a0https:\/\/cloud.google.com\/blog\/products\/application-development\/cloud-composer-an-airflow-based-orchestration-service"},{"label":"test_4","q_format":"single","q_text":"Which Google Cloud Platform service is an alternative to Hadoop with Hive?","answers":[{"ans":"Cloud Dataflow","val":false},{"ans":"Cloud Bigtable","val":false},{"ans":"BigQuery","val":true},{"ans":"Cloud Datastore","val":false}],"q_expl":"Apache Hive is a data warehouse software project built on top of Apache Hadoop for providing data summarization, query, and analysis.\nGoogle BigQuery is an enterprise data warehouse.\nReference:\u00a0https:\/\/en.wikipedia.org\/wiki\/Apache_Hive"},{"label":"test_5","q_format":"single","q_text":"As a Data Engineer, you need to write a script to upload objects to Cloud Storage bucket. Before that, you need to configure the IAM access to enable the script running in a Compute Engine virtual machine instance to upload objects to Cloud Storage. How should you do this?","answers":[{"ans":"You should grant\u00a0Storage Object Viewer\u00a0role to the service account used by the virtual machine.","val":false},{"ans":"You should create a new service account and grant\u00a0Storage Object Creator\u00a0role to the service account.","val":false},{"ans":"You should grant\u00a0Storage Object Creator\u00a0role to the service account used by the virtual machine.","val":true},{"ans":"You should grant\u00a0Storage Object Admin\u00a0role to the service account used by the virtual machine.","val":false}],"q_expl":"https:\/\/cloud.google.com\/storage\/docs\/access-control\/iam-roles#standard-roles"},{"label":"test_5","q_format":"single","q_text":"As a Data Engineer, you have three tables in Cloud SQL with identical schema and you need to combine these tables into a single table using SQL query. You want to remove duplicate rows from the result. What should you do?","answers":[{"ans":"You should use the JOIN operator to combine these tables.","val":false},{"ans":"You should use the UNION operator to combine these tables.","val":true},{"ans":"You should use the UNION ALL operator to combine these tables.","val":false},{"ans":"You should use nested WITH statements to combine these tables.","val":false}],"q_expl":"The only difference between Union and Union All is that Union All will not removes duplicate rows or records, instead, it just selects all the rows from all the tables which meets the conditions of your specifics query and combines them into the result table."},{"label":"test_5","q_format":"single","q_text":"As a Data Engineer, you want to automate execution of a multi-step data pipeline running on Google Cloud. This pipeline includes Dataproc and Dataflow jobs that have multiple dependencies on each other. The pipeline will run every day. You want to use managed services where possible, which tool should you use?","answers":[{"ans":"Cloud Composer","val":true},{"ans":"cron","val":false},{"ans":"Dataproc Workflow Templates","val":false},{"ans":"Cloud Scheduler","val":false}],"q_expl":"Cloud Composer is a fully managed workflow orchestration service that empowers you to author, schedule, and monitor pipelines that span across clouds and on-premises data centres.\nReference:\u00a0https:\/\/cloud.google.com\/composer\/"},{"label":"test_5","q_format":"single","q_text":"As a Data Engineer, you need to design and integrate a conversational user interface into your interactive voice response system to support interaction with your services and products. Which GCP service should you use?","answers":[{"ans":"Speech-to-text API & Text-to-speech API","val":false},{"ans":"Dialogflow","val":true},{"ans":"AutoML","val":false},{"ans":"Natural Language API","val":false}],"q_expl":"Dialogflow is a natural language understanding platform that makes it easy to design and integrate a conversational user interface into your mobile app, web application, device, bot, interactive voice response system, and so on. Using Dialogflow, you can provide new and engaging ways for users to interact with your product. Dialogflow can analyze multiple types of input from your customers, including text or audio inputs (like from a phone or voice recording). It can also respond to your customers in a couple of ways, either through text or with synthetic speech."},{"label":"test_5","q_format":"single","q_text":"In Bigtable, you want to separate the workload so that some nodes respond to read requests and others to write requests. How would you do this to minimize your operational load?","answers":[{"ans":"You should create multiple clusters in the Bigtable instance and use Bigtable replication to synchronize the clusters.","val":true},{"ans":"You should create two instances (read\/write) and develop your own replication program to keep the clusters synchronized.","val":false},{"ans":"It\u2018s not possible to do this in Bigtable.","val":false},{"ans":"You should create two instances (read\/write) and separate the workload at the application level.","val":false}],"q_expl":"Replication for Cloud Bigtable lets you increase the availability and durability of your data by copying it across multiple regions or multiple zones within the same region. You can also isolate workloads by routing different types of requests to different clusters."},{"label":"test_5","q_format":"single","q_text":"Which GCP service should you use to create, schedule, monitor and manage Apache Airflow workflows?","answers":[{"ans":"Cloud Composer","val":true},{"ans":"Cloud Schedule","val":false},{"ans":"Cloud Dataflow","val":false},{"ans":"Cloud Dataproc","val":false}],"q_expl":"Cloud Composer is a fully managed workflow orchestration service, enabling you to create, schedule, monitor, and manage workflows that span across clouds and on-premises data centers. Cloud Composer is built on the popular Apache Airflow open source project and operates using the Python programming language."},{"label":"test_5","q_format":"single","q_text":"You are working on an IoT application that will process huge amounts of sensor data. Which cloud storage is the best choice for your application?","answers":[{"ans":"Cloud Datastore","val":false},{"ans":"Memorystore","val":false},{"ans":"Cloud Bigtable","val":true},{"ans":"BigQuery","val":false}],"q_expl":"Cloud Bigtable\u00a0is a sparsely populated table that can scale to billions of rows and thousands of columns, enabling you to store terabytes or even petabytes of data. A single value in each row is indexed; this value is known as the row key. Bigtable is ideal for storing very large amounts of single-keyed data with very low latency. It supports high read and write throughput at low latency, and it is an ideal data source for MapReduce operations."},{"label":"test_5","q_format":"single","q_text":"A weather forecasting facility receives events from its 25,000 sensors every 10 seconds. Those events are stored in Google Storage in JSON format. Events can have different attributes based on purpose, location and brand. Data Science team wants to apply their SQL-queries on this data for further transformation and forecasting analysis.Which of the following approaches is best to satisfy Data Scientists request?","answers":[{"ans":"Load the data directly to BigQuery with enabling \u201cauto-detect\u201d option.","val":true},{"ans":"Use Dataproc cluster and create Hive external clusters on the data for data scientists to query data.","val":false},{"ans":"Import the data to BigTable. Choose combination #eventType-location-brand to differentiate between different events.","val":false},{"ans":"Build a dataflow pipeline to read JSON data and transform it to a structured format like CSV. Then, load the data to BigQuery.","val":false}],"q_expl":"When schema auto-detection is enabled, BigQuery makes a best-effort attempt to automatically infer the schema for CSV and JSON files. The auto-detection logic infers the schema field types by reading up to the first 500 rows of data. Leading lines are skipped if the\u00a0\u2013skip_leading_rows\u00a0flag is present. The field types are based on the rows having the most fields. Therefore, auto-detection should work as expected as long as there is at least one row of data that has values in every column\/field.\nSchema auto-detection is not used with Avro files, Parquet files, ORC files, Firestore export files, or Datastore export files. When you load these files into BigQuery, the table schema is automatically retrieved from the self-describing source data"},{"label":"test_5","q_format":"single","q_text":"What is the keyword in BigQuery standard SQL used when selecting from multiple tables with wildcard by their suffices?","answers":[{"ans":"_TABLES_SUFFIX","val":false},{"ans":"_TABLE_SUFFIX","val":true},{"ans":"_WILDCARD_SUFFIX","val":false},{"ans":"_SUFFIX","val":false}],"q_expl":"To restrict the query so that it scans an arbitrary set of tables, use the\u00a0_TABLE_SUFFIX\u00a0pseudo column in the WHERE clause. The\u00a0_TABLE_SUFFIX\u00a0pseudo column contains the values matched by the table wildcard."},{"label":"test_5","q_format":"multiple","q_text":"Standard methods for REST Google APIs (also known as REST methods) are\u00a0List, Get, Create, Update,\u00a0and\u00a0Delete. Select all true statements about standard methods for REST Google APIs. (select 2)","answers":[{"ans":"The\u00a0List\u00a0standard method takes a collection name and zero or more parameters as input, and returns a list of resources that match the input.","val":true},{"ans":"The\u00a0Delete\u00a0standard method is mapped to GET HTTP method.","val":false},{"ans":"The\u00a0Update\u00a0method takes a parent resource name, a resource, and zero or more parameters. It creates a new resource under the specified parent, and returns the newly created resource.","val":false},{"ans":"The\u00a0Create\u00a0standard method is mapped to POST HTTP method.","val":true}],"q_expl":"Google APIs follow the REST architectural style, which relies on a standard set of HTTP methods to perform CRUD (Create, Read, Update, Delete) operations on resources. These methods are commonly referred to as REST methods, and they include: \n1. List: This method is used to retrieve a list of resources. It takes a collection name and zero or more parameters as input, and returns a list of resources that match the input. \n2. Get: This method is used to retrieve a specific resource. It takes a resource name or ID as input, and returns the details of the specified resource. \n3. Create: This method is used to create a new resource. It takes a resource as input, and adds it to the collection of resources. \n4. Update: This method is used to update an existing resource. It takes a resource name or ID and a resource as input, and updates the details of the specified resource. \n5. Delete: This method is used to delete a resource. It takes a resource name or ID as input, and removes the specified resource from the collection of resources. \nRegarding the given statements, Statement A is true. The List method is used to retrieve a collection of resources based on the specified input parameters. For example, the Google Drive API\u2018s Files List method can be used to retrieve a list of files that match certain criteria such as file type or owner. \nStatement B is false. The Delete method is mapped to the HTTP DELETE method, not the GET method. The HTTP DELETE method is used to delete a resource from the server. \nStatement C is false. The Update method does not create a new resource; instead, it updates an existing resource. To create a new resource, the Create method is used. \nStatement D is true. The Create method is mapped to the HTTP POST method. The HTTP POST method is used to submit a new resource to the server. \nReferences: \nAbout API requests:\u00a0 \u00a0https:\/\/cloud.google.com\/apis\/docs\/overview\/requests \nAPI design guide:\u00a0\u00a0https:\/\/cloud.google.com\/apis\/design \nREST Resource: Method: list:\u00a0https:\/\/developers.google.com\/drive\/api\/v3\/reference\/files\/list"},{"label":"test_5","q_format":"single","q_text":"A company wants to migrate on-premises Hadoop Hbase database to Google Cloud. They want to choose managed service. Which solution should you advise them?","answers":[{"ans":"Cloud Memorystore","val":false},{"ans":"Cloud Datastore","val":false},{"ans":"Bigtable","val":true},{"ans":"Managed Instance Group (MIG) with Hbase","val":false}],"q_expl":"Bigtable is a fully managed wide-column and key-value NoSQL database service for large analytical and operational workloads as part of the Google Cloud portfolio\nReference:\u00a0https:\/\/cloud.google.com\/bigtable\/docs\/overview"},{"label":"test_5","q_format":"single","q_text":"Which storage service provides a MySQL, PostgreSQL or SQL Server database engines to Google Cloud customers?","answers":[{"ans":"Cloud Spanner","val":false},{"ans":"Cloud Bigtable.","val":false},{"ans":"BigQuery","val":false},{"ans":"Cloud SQL","val":true}],"q_expl":"Cloud SQL\u00a0is a fully-managed database service that helps you set up, maintain, manage, and administer your relational databases on Google Cloud Platform. You can use Cloud SQL with MySQL, PostgreSQL, or SQL Server."},{"label":"test_5","q_format":"single","q_text":"As a Data Engineer, you need to implement an IoT application that requires data storage up to 30 petabytes. Your application must support fast reads and writes. Your data schema is rather simple and you want to use the most economical solution for this. What should you do?","answers":[{"ans":"You should use BigQuery, and implement the business logic in SQL.","val":false},{"ans":"You should store the data in Cloud Spanner, and add an in-memory cache for speed.","val":false},{"ans":"You should store the data in Cloud Storage.","val":false},{"ans":"You should store the data in Cloud Bigtable.","val":true}],"q_expl":"Bigtable\u00a0is correct because it provides high-speed reads and writes, supports a simple schema, and is cost-effective. Cloud Spanner is not correct because it would not be the most economical solution. Cloud Storage is not correct because blob-oriented storage is not a good fit for reading and writing small pieces of data. BigQuery is not correct because it doesn\u2018t provide the high-speed reads and writes required by IoT.\nReference:\u00a0https:\/\/cloud.google.com\/bigtable\/docs"},{"label":"test_5","q_format":"single","q_text":"You want to set up a streaming data insert into a Redis cluster running on Compute Engine instances. Because you have PII data you need to encrypt data at rest with encryption keys that you can create, rotate and destroy as needed. What should you do?","answers":[{"ans":"You should create encryption keys in Cloud KMS. Then, reference those keys in your API service calls when accessing the data in your Compute Engine cluster instances.","val":false},{"ans":"You should create encryption keys locally. Then, upload your encryption keys to Cloud KMS and use those keys to encrypt your data in all of the Compute Engine cluster instances.","val":false},{"ans":"You should create encryption keys in Cloud KMS. Then, use those keys to encrypt your data in all of the Compute Engine cluster instances.","val":true},{"ans":"You should create a dedicated service account, and use encryption at rest to reference your data stored in your Compute Engine cluster instances as part of your API service calls.","val":false}],"q_expl":"Cloud KMS\u00a0stored on cloud helps in creating, rotating and destroying keys as needed and also it can be used to encrypt Compute Engine instances."},{"label":"test_5","q_format":"single","q_text":"What kind of cloud computing service provides raw compute, storage, and network organized in ways that are familiar from physical data centers?","answers":[{"ans":"Software as a Service (SaaS), iaas","val":false},{"ans":"Platform as a Service (PaaS).","val":false},{"ans":"Database as a Service.","val":false},{"ans":"Container as a Service.","val":false},{"ans":"Infrastructure as a Service (IaaS).","val":true}],"q_expl":"IaaS provides users access to raw computing resources such processing power, data storage capacity, and networking, in the context of a secure data center. Platform as a Service (PaaS)."},{"label":"test_5","q_format":"multiple","q_text":"As a Data Engineer, you need to know what storage and database options you should use in different use cases. Select all true statements. (select 2)","answers":[{"ans":"If you need to store highly structured objects in a document database, with support for ACID transactions and SQL-like queries, you should use Bigtable.","val":false},{"ans":"If you need interactive querying in an Online Analytical Processing (OLAP) system, you should use BigQuery.","val":true},{"ans":"For in-memory data storage with low latency, you should use Cloud Storage.","val":false},{"ans":"If you need full SQL support for an Online Transaction Processing (OLTP) system, you should use Cloud Spanner or Cloud SQL.","val":true}],"q_expl":"If you need to store highly structured objects in a document database, with support for ACID transactions and SQL-like queries, you should use Firestore.\nFor in-memory data storage with low latency, you should use Memorystore."},{"label":"test_5","q_format":"single","q_text":"In BigQuery, your team wants to run an important query that can return a lot of records. You want to find out how much it will cost to run this query. You are using on-demand pricing. What should you do?","answers":[{"ans":"You should run a\u00a0SELECT COUNT(*)\u00a0to get an idea of how many records your query will look through. Then convert that number of rows to dollars using the Pricing Calculator.","val":false},{"ans":"You can use query validator in the Google Console or use\u00a0--dry_run\u00a0flag in the\u00a0bq\u00a0command-line tool. Then using the Pricing Calculator convert estimated bytes to dollars.","val":true},{"ans":"You cannot make an estimate with the information provided.","val":false},{"ans":"You should arrange to switch to\u00a0Flat-rate\u00a0pricing model for this query, then move back to on-demand.","val":false}],"q_expl":"https:\/\/cloud.google.com\/bigquery\/docs\/estimate-costs"},{"label":"test_5","q_format":"single","q_text":"You are deploying a Tensorflow model built by data science team to the cloud. Based on the requirements provided by data scientists, the model should be able to return the output as soon as possible to minimize the latency of serving predictions. Input will be passed as JSON. Which of the following approaches are best for this scenario?","answers":[{"ans":"Use Google Kubernetes Engine to deploy the model. Use online prediction to pass input data to the model hosted in cloud.","val":false},{"ans":"Use Google Kubernetes Engine to deploy the model. Use batch prediction to pass input data to the model hosted in cloud.","val":false},{"ans":"Use Cloud Machine Learning Engine to deploy the model. Use batch prediction to pass input data to the model hosted in cloud.","val":false},{"ans":"Use Cloud Machine Learning Engine to deploy the model. Use online prediction to pass input data to the model hosted in cloud.","val":true}],"q_expl":"AI Platform provides two ways to get predictions from trained models: online prediction (sometimes called HTTP prediction), and batch prediction. In both cases, you pass input data to a cloud-hosted machine-learning model and get inferences for each data instance.\nOnline prediction passes input as a JSON string and returns the output as soon as possible.\nAnswer A & B are incorrect: GKE is not a recommended option to deploy the model.\nAnswer C is incorrect: Batch prediction doesn\u2019t support returning the output as soon as possible. Input is passed indirectly as one or more URIs of files in Google Storage.\nReference:\u00a0\u00a0https:\/\/cloud.google.com\/ai-platform\/prediction\/docs\/online-vs-batch-prediction"},{"label":"test_5","q_format":"single","q_text":"You have a massively multiplayer online (MMO) game which sends events from each player every 10 seconds. Events contain stats about the player session\u2019s state (play, idle, off) as well as ping duration. You want to use Dataflow for windowing. The purpose is to aggregate events and extracting stats to detect how many players are currently online and what is the average ping duration for each server in a time window of 30 seconds. Which windowing function you should choose to design the pipeline?","answers":[{"ans":"Fixed-time window.","val":false},{"ans":"Sliding-time window.","val":true},{"ans":"Per-session window.","val":false},{"ans":"Single global window.","val":false}],"q_expl":"A sliding time window uses time intervals in the data stream to define bundles of data. However, with sliding time windowing, the windows overlap. Each window might capture five minutes\u2019 worth of data, but a new window starts every ten seconds. The frequency with which sliding windows begin is called the period. Therefore, our example would have a window size of five minutes and a period of ten seconds.\nThe sliding-time window is the windowing function recommended for this scenario.\nReference:\u00a0https:\/\/cloud.google.com\/dataflow\/model\/windowing#windowing-functions"},{"label":"test_5","q_format":"single","q_text":"An e-payment service allows users to purchase online and transfer money securely. They log into the website to perform the transactions and they log out. The website needs to check if their sessions are idle for 10 minutes, means they did not perform any action or they opened a new link within the website. In case of idle session, the website ends their session for security purposes. You need to build a Dataflow pipeline to aggregate session events received from the website and detect sessions idle more than 10 minutes to get their sessions expired. Which windowing function you should choose to design the pipeline?","answers":[{"ans":"Fixed-time window with duration of 10 minutes.","val":false},{"ans":"Sliding-time window with duration of 10 minutes.","val":false},{"ans":"Per-session window with time gap duration of 10 minutes.","val":true},{"ans":"Single global window with time-based trigger of 10 minutes.","val":false}],"q_expl":"A session window function defines windows around areas of concentration in the data. Session windowing is useful for data that is irregularly distributed with respect to time; for example, a data stream representing user mouse activity may have long periods of idle time interspersed with high concentrations of clicks. Session windowing groups the high concentrations of data into separate windows and filters out the idle sections of the data stream. Note that session windowing applies on a per-key basis: That is, grouping into sessions only takes into account data that has the same key. Each key in your data collection will, therefore, be grouped into disjoint windows of differing sizes.\nFor this scenario, the per-session window is the function to choose to build a Dataflow pipeline.\nReference:\u00a0https:\/\/cloud.google.com\/dataflow\/model\/windowing#windowing-functions"},{"label":"test_5","q_format":"single","q_text":"A company specializes in monitoring and distributing data about road traffic for more than 60 cities. Data is used by navigation apps to notify users of traffic congestion on their destination routes and alert them of road accidents. There are thousands of queries running to write new events and read events for analysis and get the latest stats on road traffic. Which of the following is the best option for this scenario?","answers":[{"ans":"Cloud Spanner","val":false},{"ans":"BigQuery","val":false},{"ans":"Datastore","val":false},{"ans":"BigTable","val":true}],"q_expl":"Cloud BigTable is a petabyte-scale, fully managed NoSQL database service for large analytical and operational workloads. Under a typical workload, Cloud BigTable delivers highly predictable performance. When everything is running smoothly, a typical workload can achieve the following performance for each node in the Cloud Bigtable cluster, depending on which type of storage the cluster uses:\n\nIn general, a cluster\u2019s performance increases linearly as you add nodes to the cluster. For example, if you create an SSD cluster with 10 nodes, the cluster can support up to 100,000 rows per second for a typical read-only or write-only workload, with 6 ms latency for each read or write operation.\nAnswer A is incorrect: Cloud Spanner does not guarantee the same performance and low latency as BigTable.\nAnswer B is incorrect: While BigQuery is a potential choice, BigQuery doesn\u2019t provide high throughput and low latency as powerful as BigTable.\nAnswer C is incorrect: Datastore can be a potential choice since it\u2019s a NoSQL database. The issue is, Datastore is not built for storing and reading huge data volumes as in this scenario. Datastore is designed for web applications of a small scale.\nReference:\u00a0https:\/\/cloud.google.com\/bigtable\/docs\/performance"},{"label":"test_5","q_format":"single","q_text":"A Kafka cluster is receiving event data from outsourced sensors. The cluster is installed in a Compute Engine instance and it writes events to Google Storage. Due to the new security rules in the company, data written to Google Storage should be encrypted. Security team wants to be sure the encryption key used is provided by them using on-premise vault and no keys generated by third-parties are used. What should you do to follow the security team\u2019s rules?","answers":[{"ans":"Reference the encryption key provided by security team when calling API service when writing data to Google Storage to encrypt the data.","val":false},{"ans":"Store the encryption key provided by security team in Compute Engine instance and reference it when calling API service when writing data to Google Storage to encrypt the data.","val":false},{"ans":"Store the encryption key provided by security team in Cloud Key Management Service (KMS) and reference it when calling API service when writing data to Google Storage to encrypt the data.","val":true},{"ans":"Create encryption keys using Cloud Key Management Service (KMS) and reference it when calling API service when writing data to Google Storage to encrypt the data.","val":false}],"q_expl":"The best option to follow the security team\u2019s rules is:\nC. Store the encryption key provided by security team in Cloud Key Management Service (KMS) and reference it when calling API service when writing data to Google Storage to encrypt the data.\nHere\u2019s why this option is the most secure:\n\nSecurity Team\u2019s Key:\u00a0This option adheres to the requirement of using the encryption key provided by the security team.\nCloud KMS Storage:\u00a0Cloud KMS is a Google Cloud service specifically designed for managing encryption keys. It provides a secure environment for storing and controlling access to the keys.\nAPI call reference:\u00a0Referencing the key stored in KMS during the API call ensures the data gets encrypted with the designated key.\n\nWhy other options are not ideal:\n\nA. Storing key nowhere (referencing directly):\u00a0This approach bypasses key management altogether, making it difficult to control access and rotate keys securely.\nB. Storing key in Compute Engine:\u00a0This poses a security risk. If the compute engine instance is compromised, the encryption key could be exposed.\nD. Using Cloud KMS generated keys:\u00a0This option wouldn\u2019t utilize the key provided by the security team as required."},{"label":"test_5","q_format":"single","q_text":"A multinational company has multiple Google Storage buckets in different regions around the world. Each branch has its own set of buckets in the region nearest to them to avoid latencies. However, this led to a problem for the analytics team to reach and do the necessary reports on the data using BigQuery since they need to create the tables in the same region either to import the data or create external tables to access the data in different regions. The head of data decided to sync the data daily from different Google Storage buckets scattered in different regions to a single multi-regional bucket to do the necessary data analysis and reporting. Which service could help with this approach?","answers":[{"ans":"Appliance Transfer Service","val":false},{"ans":"gsutil","val":false},{"ans":"Storage Transfer Service","val":true},{"ans":"Dataflow","val":false}],"q_expl":"The most suitable service for syncing data daily from scattered Google Storage buckets to a single multi-regional bucket for analytics is:\nC. Storage Transfer Service\nHere\u2019s why Storage Transfer Service is the ideal choice for this scenario:\n\nDesigned for Data Movement:\u00a0Storage Transfer Service is a Google Cloud service specifically built to automate transfers between various storage systems, including Google Cloud Storage buckets across different regions.\nScheduled Transfers:\u00a0It allows you to configure scheduled transfers, ensuring daily synchronization of data from source buckets to the destination multi-regional bucket.\nScalability:\u00a0Storage Transfer Service can handle large-scale data transfers efficiently.\n\nWhy other options are not as effective:\n\nA. Appliance Transfer Service:\u00a0This service is designed for migrating data from physical appliances to Google Cloud Storage, not ideal for data movement between buckets within the cloud.\nB. gsutil:\u00a0While gsutil is a command-line tool for managing Google Cloud Storage, it\u2019s less suitable for setting up automated and scheduled data transfer workflows between multiple buckets across regions.\nD. Dataflow:\u00a0Dataflow is a streaming data processing service, which might be an overkill for this use case that primarily involves scheduling data transfers."},{"label":"test_5","q_format":"single","q_text":"You have the following legacy SQL query in BigQuery:\n    # legacy SQL\n    SELECTorder_date,\n    COUNT(DISTINCT customer_id)) AS customers\n    FROM\n    [my-project:orders.orders_2023]\n    GROUP BY\n    order_date\n    ORDER BY\n    order_date;\n    How can you convert this query to standard SQL?","answers":[{"ans":"Change table syntax to \u2018my-project:orders.orders_2023\u2018","val":false},{"ans":"Change table syntax to [my-project.orders.orders_2023]","val":false},{"ans":"Change table syntax to \u2032my-project.orders.orders_2023\u2032","val":true},{"ans":"No change required. This works fine if standard SQL is enabled.","val":false}],"q_expl":"Correct Option:\nC. Change table syntax to\u00a0my-project.orders.orders_2023\n\nExplanation: In standard SQL, BigQuery uses the dot notation for table references. Therefore, the correct way to reference the table in standard SQL is\u00a0my-project.orders.orders_2023.\n\nIncorrect Options:\nA. Change table syntax to \u2018my-project:orders.orders_2023\u2018\n\nExplanation: This option uses the legacy SQL syntax with a colon (:) to separate the project and dataset. This is not valid in standard SQL.\n\nB. Change table syntax to [my-project.orders.orders_2023]\n\nExplanation: This option uses square brackets ([]), which is also a legacy SQL syntax. Standard SQL does not use square brackets for table references.\n\nD. No change required. This works fine if standard SQL is enabled.\n\nExplanation: This statement is incorrect because the given query uses legacy SQL syntax. Standard SQL requires changes to the table reference format."},{"label":"test_5","q_format":"multiple","q_text":"Your team is planning to perform tests on Cloud BigTable instance to ensure the performance quality of the BigTable instance to be used in production. Which of the following conditions should be met to consider the performance testing valid? (select 3)","answers":[{"ans":"Use development instance for testing.","val":false},{"ans":"Run a heavy pre-test for several minutes before the test starts.","val":true},{"ans":"Scale up the instance just before the test starts.","val":false},{"ans":"Use at least 300GB of data.","val":true},{"ans":"Do not scale up the instance just before the test starts.","val":true},{"ans":"Test should take no longer than 10 minutes.","val":false}],"q_expl":"If you\u2019re running a performance test that depends upon Cloud Bigtable, be sure to follow these steps as you plan and execute your test:\n\u2013 Use a production instance. A development instance will not give you an accurate sense of how a\nproduction instance performs under load.\n\u2013 Use at least 300 GB of data. Cloud Bigtable performs best with 1 TB or more of data. However, 300\nGB of data is enough to provide reasonable results in a performance test on a 3-node cluster. On\nlarger clusters, use at least 100 GB of data per node.\n\u2013 Stay below the recommended storage utilization per node.\n\u2013 Before you test, run a heavy pre-test for several minutes. This step gives Cloud Bigtable a chance to\nbalance data across your nodes based on the access patterns it observes.\n\u2013 Run your test for at least 10 minutes. This step lets Cloud Bigtable further optimize your data, and it\nhelps ensure that you will test reads from disk as well as cached reads from memory.\nReference:\u00a0\u00a0https:\/\/cloud.google.com\/bigtable\/docs\/performance"},{"label":"test_5","q_format":"single","q_text":"Your company uses BigQuery as their main data warehouse. Recently, data team has proposed a new schema for existing tables: They want to modify some column names to a more meaningful ones. You are tasked to help data team apply the necessary changes on BigQuery\u2019s tables schemas. How can you achieve this?","answers":[{"ans":"Modify column names for each table using ALTER command.","val":true},{"ans":"Export data for tables with schema changes to Google Storage. Create a new table with the new schema. Import data to new table from Google Storage.","val":false},{"ans":"Create views on BigQuery tables with new column names.","val":false},{"ans":"Create a new table in BigQuery with the new schema. Insert data from existing tables to the new tables using INSERT SELECT statement.","val":false}],"q_expl":"The most efficient and straightforward way to modify column names in BigQuery tables is to modify column names for each table using the ALTER command.\nHere\u2019s how you can achieve this:\n\nIdentify the tables that need schema changes.\n\nUse the ALTER TABLE command to modify the column names. For example:\nSQL\n\nALTER TABLE `your_dataset.your_table` RENAME COLUMN `old_column_name` TO `new_column_name`;\n\n\n\nVerify the changes by querying the table and checking the column names.\n\nThis approach is the most direct and requires minimal effort. Other options, such as exporting data to Google Storage and creating new tables, involve more steps and can be time-consuming, especially for large datasets. Creating views with new column names is not a permanent solution, as it doesn\u2019t modify the underlying table schema."},{"label":"test_5","q_format":"multiple","q_text":"You want to build a machine learning model to recognize images for Thai cuisine restaurant. You are provided with several image samples for each dish and its name. You used AutoML Vision to build the model. You split the samples into training and test sets. You uploaded the training set to Google Cloud with labels and build the model on AutoML vision. When you tested the newly built model with the test, the confusion matrix shows high false positives with the model confuses between different labels. How can you fix the model\u2019s accuracy? (select 2)","answers":[{"ans":"Remove all images with bad quality.","val":false},{"ans":"Sort images by how \u201cconfused\u201c the model is and check if they are labeled correctly.","val":true},{"ans":"If you have a very low training set, consider removing the labels altogether.","val":true},{"ans":"Let AutoML Vision decide which images to be considered for training or testing.","val":false}],"q_expl":"Google Cloud provides a machine learning service called AutoML to quickly build models for you. AutoML Vision is one of its products which you can start with a training set as little as a dozen photo samples and AutoML takes care of the rest.\nWhile iterating on your model, if the model\u2019s quality levels are not up to expectations, you can go back to earlier steps to improve the quality:\nAutoML Vision allows you to sort the images by how \u201cconfused\u201d the model is, by the true label and its predicted label. Look through these images and make sure they\u2019re labeled correctly.\nConsider adding more images to any labels with low quality.\nYou may need to add different types of images (e.g. wider angle, higher or lower resolution, different points of view).\nConsider removing labels altogether if you don\u2019t have enough training images.\nRemember that machines can\u2019t read your label name; it\u2019s just a random string of letters to them. If you have one label that says \u201cdoor\u201d and another that says \u201cdoor_with_knob\u201d the machine has no way of figuring out the nuance other than the images you provide it.\nAugment your data with more examples of true positives and negatives. Especially important examples are the ones that are close to the decision boundary (i.e. likely to produce confusion but still correctly labeled).\nSpecify your own TRAIN, TEST, VALIDATION split. The tool randomly assigns images, but near-duplicates may end up in TRAIN and VALIDATION which could lead to overfitting and then poor performance on the TEST set.\nOnce you\u2019ve made changes, train and evaluate a new model until you reach a high enough quality level.\nReference:\u00a0https:\/\/cloud.google.com\/vision\/automl\/docs\/evaluate"},{"label":"test_5","q_format":"single","q_text":"A social media platform stores various details of their platform users such as session login time, URLs visited, activities on platform and other logs. With GDPR (General Data Protection Regulation) compliance to be officially implemented, the platform now allows users to download their activity logs from their profile settings which they can click a button to call an API to generate a full report. Recently, users are complaining timeouts after 60 seconds of requesting to download their activity logs at peak hours when the platform has the most traffic. They have to try for several minutes or even hours for the API to return their report available for download. How can you solve this issue?","answers":[{"ans":"Increase timeout for API at peak times to 120 seconds. If it keeps failing, try increasing the timeout until the issue is resolved.","val":false},{"ans":"Build a Dataflow pipeline to generate daily reports of users\u2019 activity logs. Users can download those daily reports whenever they want to.","val":false},{"ans":"Migrate data source to Cloud Spanner for horizontal scaling to avoid query timeouts.","val":false},{"ans":"Use Pub\/sub to pull the requests for activity logs from users. Send a link to users by their email addresses with a temporary download link for them to access their report.","val":true}],"q_expl":"Cloud Pub\/Sub\u00a0is a service to ingest event streams at any scale. It\u2019s scalable and reliable for stream analytics and event-driven computing systems.\nPub\/sub is a good product to de-couple a system\u2019s components so they communicate with each other asymmetrically. From the scenario shown here, instead of directly calling the API to export required report which puts great loads on the API and hence the timeouts faced by users. Instead, the platform can \u201cpublish\u201d messages to a \u201ctopic\u201d related to exporting activity log reports sending the required parameters such as user ID and custom settings such as date range and what data to export. The API can be switched to be a \u201csubscriber\u201d which receives the messages sent and processes each message asymmetrically to generate the report, then sends the download link to the user\u2019s mailbox when ready.\nHence, answer D is correct.\nAnswer A is incorrect: Increasing timeout isn\u2019t a scalable solution and it may keep occurring eventually when more and more users join the platform.\nAnswer B is incorrect: While this would solve the timeout issues, generating daily reports for users can be costly as more users join, knowing that requesting activity log reports are a non-frequent action and this costs both compute and storage resources. This solution also doesn\u2019t provide flexibility with what parameters the report is generated on such as date range and other custom metrics.\nAnswer C is incorrect: This solution has several issues. First, we\u2019re assuming the data source is a relational database, which can be unlikely since NoSQL databases better perform for massive log input which uses the user ID as a key to reach the data. Second, Cloud Spanner isn\u2019t a cheap solution for a service not frequently used.\nReference:\u00a0https:\/\/cloud.google.com\/pubsub\/docs\/overview"},{"label":"test_5","q_format":"single","q_text":"A company decides to migrate its on-premise data infrastructure to the cloud mainly for high availability of cloud services and to lower the high costs of storing data on-premise. The infrastructure uses HDFS to store data and be processed and transformed using Apache Hive & Spark. The company wants to migrate the infrastructure and DevOps team still wants to administrate the infrastructure in the cloud. As a data architect, which of the following is the approach recommended by Google?","answers":[{"ans":"Use Dataproc to process the data. Store data in Google Storage.","val":true},{"ans":"Build a Dataflow pipeline. Store the data in Google Storage. Use Cloud Compute to launch instances and install the required dependencies for processing the data.","val":false},{"ans":"Use Dataproc to process the data. Store data in Dataproc\u2019s HDFS.","val":false},{"ans":"Build a Dataflow pipeline. Store the data in persistent disks in HDFS. Execute the code in Spark framework provided by Dataflow.","val":false}],"q_expl":"Dataproc\u00a0is a cloud-native Apache Hadoop & Apache Spark service. Dataproc is a fully-managed service from Google to run Apache Hadoop & Spark clusters.\nDataflow is a simplified streaming\/batching data processing service. With Apache Beam, it provides a rich set of windowing and session analysis primitives as well as an ecosystem of source & sink connectors.\nAnswer B is incorrect: Dataflow is serverless which may not suit the DevOps requirement to fully manage the pipeline and it\u2019s unnecessary to use Cloud Compute for installing dependencies.\nAnswer C is incorrect: Dataproc\u2019s HDFS is volatile, which means it will be removed when the cluster is deleted. Dataproc clusters can be kept up indefinitely but this may lead to high costs that defeats the purpose of migration.\nAnswer D: In addition to what discussed in answer B, storing data using persistent disks can be only accessible by Compute engines and it\u2019s more expensive than storing in Google Storage.\nAnswer A fulfills the requirements for migrating the on-premise infra to the cloud with high availability, minimum costs and full control by DevOps.\nReferences:\nhttps:\/\/cloud.google.com\/dataproc\/\nhttps:\/\/cloud.google.com\/dataflow\/"},{"label":"test_5","q_format":"single","q_text":"Data analysts are switching to use Apache Spark to perform experiments on the data before applying the changes to production. Those experiments are not critical, but they will be conducted on big data sets. As a data engineer, the head of data asked you to prepare the tech stack required to be used by data analysts to run their Spark scripts and experiment on with taking into consideration the cost of the stack used. Which of the following tech stack is suggested?","answers":[{"ans":"Launch a Dataproc cluster in high-availability mode with using high-memory worker machine types.","val":false},{"ans":"Launch a Dataproc cluster in standard mode with using high-CPU worker machine types.","val":false},{"ans":"Launch a Dataproc cluster in standard mode with using high-memory worker machine types.","val":true},{"ans":"Advice the data analysts to use Dataprep for their data manipulation.","val":false}],"q_expl":"Answer A is incorrect: Since the scenario states non-critical experiments will be conducted by data analysts, the Dataproc cluster used can be in standard mode.\nAnswer B is incorrect: Since the scenario states non-critical experiments, there is no need for high-CPU worker machine types.\nAnswer D is incorrect: Dataprep does not provide Apache Spark job transformation. Dataprep is best for visual exploration and manual cleaning and preparation of data for analysis and machine learning.\nReference:\u00a0https:\/\/cloud.google.com\/dataprep\/"},{"label":"test_5","q_format":"single","q_text":"You have several Data Studio reports reading from BigQuery. Those reports are used to visualize several metrics for marketing team. Data visualized is updated only once a day. You notice that reports running queries on BigQuery are not free and they cost for each query. You want to control and minimize the costs caused by frequent queries coming from Data Studio dashboards. What should you do?","answers":[{"ans":"Enable caching on reports for reading from BigQuery. No need to change the credentials.","val":true},{"ans":"Grant owner credentials for the reports on BigQuery datasets and enable caching.","val":false},{"ans":"Configure reports data sources to update data every 24 hours only.","val":false},{"ans":"Export data as CSV files to Google Storage every 24 hours and change reports data source to read from those files.","val":false}],"q_expl":"BigQuery\u00a0writes all query results to a table. The table is either explicitly identified by the user (a destination table), or it is a temporary, cached results table. Temporary, cached results tables are maintained per-user, per-project. There are no storage costs for temporary tables, but if you write query results to a permanent table, you are charged for storing the data.\nWhen you run a query, a temporary, cached results table is created in a special dataset referred to as an \u201canonymous dataset\u201d. Unlike regular datasets that inherit permissions from the IAM resource hierarchy model (project and organization permissions), access to anonymous datasets is restricted to the dataset owner. The owner of an anonymous dataset is the user who ran the query that produced the cached result.\nWhen an anonymous dataset is created, the user that runs the query job is explicitly given bigquery.dataOwner access to the anonymous dataset. bigquery.dataOwner access gives only the user who ran the query job full control over the dataset. This includes full control over the cached results tables in the anonymous dataset. If you intend to share query results, do not use the cached results stored in an anonymous dataset. Instead, write the results to a named destination table.\nThough the user that runs the query has full access to the dataset and the cached results table, using them as inputs for dependent jobs is strongly discouraged.\nThe names of anonymous datasets begin with an underscore. This hides them from the datasets list in the GCP Console and the classic BigQuery web UI. You can list anonymous datasets and audit anonymous dataset access controls by using the CLI or the API.\nAnswer B is incorrect: There is no need to enable owner credentials for responsive caching.\nAnswer C is incorrect: The maximum period for caching in Data Studio is 12 hours.\nAnswer D is incorrect: This is a very cumbersome option and there is no need to export to Google Storage when better options are available.\nReference:\u00a0https:\/\/cloud.google.com\/bigquery\/docs\/cached-results"},{"label":"test_5","q_format":"multiple","q_text":"You are using BigQuery as the data warehouse. Data analysts & scientists run queries to get data from BigQuery. When you checked the billing costs for the previous month, you noticed a spike in running queries on BigQuery despite caching is enabled. You tried to find out the reason for the spike by reading some of the queries data analysts and scientists are running on BigQuery. Which of the following can be reasons for queries not cached? (select 2)","answers":[{"ans":"Queries use current_timestamp function.","val":true},{"ans":"SELECT queries with asterisk (*).","val":false},{"ans":"Queries select from authorized views on archive tables.","val":false},{"ans":"Queries use wildcards.","val":true}],"q_expl":"Currently, cached results are not supported for queries against multiple tables using a wildcard even if the \u201cUse Cached Results\u201d option is checked. If you run the same wildcard query multiple times, you are billed for each query.\nIf the query uses non-deterministic functions; for example, date and time functions such as CURRENT_TIMESTAMP() and NOW(), and other functions such as CURRENT_USER() return different values depending on when a query is executed\nReferences:\nhttps:\/\/cloud.google.com\/bigquery\/docs\/querying-wildcard-tables\nhttps:\/\/cloud.google.com\/bigquery\/docs\/cached-results"},{"label":"test_5","q_format":"single","q_text":"You have several Dataflow pipelines streaming data for transformation and further analysis. At one point of the transformation, there is a need for two pipelines to share data among pipeline instances. You need to modify the architecture to allow data sharing between different pipelines. How should this requirement be met in Google Cloud?","answers":[{"ans":"Use Google Storage to share data with other pipeline instances.","val":true},{"ans":"Grant pipeline instances the right IAM roles to access other pipelines instances for data sharing.","val":false},{"ans":"Enable data sharing option when creating Dataflow pipeline.","val":false},{"ans":"Data sharing among Dataflow pipelines is only possible if instances reside in same region.","val":false}],"q_expl":"There is no Cloud Dataflow-specific cross pipeline communication mechanism for sharing data or processing context between pipelines. You can use durable storage like Cloud Storage or an in-memory cache like App Engine to share data between pipeline instances.\nAnswer B is incorrect: This approach is not recommended, if possible. Use Google Storage to share data between pipelines.\nAnswer C\u00a0 is incorrect: Dataflow doesn\u2019t have a cross pipeline communication mechanism for sharing data between pipelines.\nAnswer D is incorrect: Sharing data is not possible unless using reliable data storage such as Google Storage.\nReference:\u00a0https:\/\/cloud.google.com\/dataflow\/docs\/resources\/faq"},{"label":"test_5","q_format":"single","q_text":"A company is migrating its current infrastructure from on-premise to Google cloud. It stores over 280TB of data on its on-premise HDFS servers. You were tasked to move data from HDFS to Google Storage in a secure and efficient manner. Which of the following approaches are best to fulfill this task?","answers":[{"ans":"Install Google Storage gsutil tool on servers and copy the data from HDFS to Google Storage.","val":false},{"ans":"Use Cloud Data Transfer Service to migrate the data to Google Storage.","val":false},{"ans":"Import the data from HDFS to BigQuery. Then, export the data to Google Storage in AVRO format.","val":false},{"ans":"Use Transfer Appliance Service to migrate the data to Google Storage","val":true}],"q_expl":"Storage Transfer Service allows you to quickly import ONLINE data into Cloud Storage. You can also set up a repeating schedule for transferring data, as well as transfer data within Cloud Storage, from one bucket to another.\nTransfer Appliance is an OFFLINE secure, high capacity storage server that you set up in your datacenter. You fill it with data and ship it to an ingest location where the data is uploaded to Google Cloud Storage.\nSo, answer D is the correct one, while B is incorrect.\nAnswer A is incorrect: gsutil tool is good for programmatic usage by developers and may be useful to copy and move megabytes\/gigabytes of data. Not so practical for Terabytes of data. It\u2019s also not a reliable data transfer technique as it is related to the machine\u2019s connectivity with Google Cloud.\nAnswer C is incorrect: In order to migrate to BigQuery, you need to migrate data to Google Storage. This is a useless approach as the main challenge is migrating data from HDFS to Google Storage and BigQuery won\u2019t help to solve it.\nReferences:\nhttps:\/\/cloud.google.com\/storage-transfer\/docs\/\nhttps:\/\/cloud.google.com\/transfer-appliance\/\nhttps:\/\/cloud.google.com\/solutions\/migration\/hadoop\/hadoop-gcp- migration-data"},{"label":"test_5","q_format":"single","q_text":"A company uses BigQuery as its main data warehouse. Data stored in Google Storage is being transformed and enriched using a Dataflow pipeline, to be later loaded into BigQuery. More than 80 different datasets exist in BigQuery with each dataset contains between 20-50 tables, all stored in a single project . Data analysts access BigQuery for their reporting tasks, while data scientists are using BigQuery ML (Machine Learning) by creating forecast models. Since BigQuery is used by wide range of employees, the CTO wants to control the costs of running queries scanning GBs of data from some users frequently trigger such queries. How can you achieve this?","answers":[{"ans":"Set project-level quotas on BigQuery by setting a fixed size limit to be used monthly.","val":false},{"ans":"Set monthly flat-rate pricing for BigQuery.","val":false},{"ans":"Set custom quotas for each user with access on BigQuery based on their business requirements.","val":true},{"ans":"Separate datasets to different projects to benefit from monthly free tier.","val":false}],"q_expl":"If you have multiple BigQuery projects and users, you can manage costs by requesting a custom quote that specifies a limit on the amount of query data processed per day. \nCreating a custom quota on query data allows you to control costs at the project-level or at the user- level. \nProject-level custom quotas limit the aggregate usage of all users in that project. \nUser-level custom quotas are separately applied to each user or service account within a project. \nAnswer A is incorrect: Setting project-level quota is not the best approach for this scenario because this will not set user limit quotas and when a project reaches the limit set it will disallow all users from running queries. Note that, as stated, all datasets reside in a single cloud project. \nAnswer B is incorrect: Flat-rate can be a possible approach. However, BigQuery does not provide flexible flat-rate pricing and the cheapest is $10,000 for 500 slots, which may not be a desirable option for small-medium businesses. \nAnswer D is incorrect: Separating datasets to different projects will lead to more work from data engineers to maintain access among different projects in case users need to join tables from different datasets together. This solution is possible for testing and development projects, as well as small-scale dataset usage, but as for this scenario, setting quotas is more efficient. \nReferences: \nhttps:\/\/cloud.google.com\/bigquery\/docs\/custom- quotas#controlling_query_costs_using_bigquery_custom_quotas \nhttps:\/\/cloud.google.com\/bigquery\/pricing#monthly-flat-rate"},{"label":"test_5","q_format":"single","q_text":"You have been using BigTable instance with HDD as storage type. You want to increase the performance of the instance by changing the storage type to SSD. You want to make sure the data will not be lost. How can you achieve that?","answers":[{"ans":"Use Dataflow to export data from the existing BigTable instance to the new instance.","val":true},{"ans":"From Google Cloud console UI, you can switch the storage type from HDD to SDD. Data will be moved to new storage type. Instance will be inaccessible by this time until migration is complete.","val":false},{"ans":"From Google Cloud console UI, you can switch the storage type from HDD to SDD. Data will be moved to new storage type. Instance will be in read-only mode by this time until migration is complete.","val":false},{"ans":"You need to change the instance from development to production.","val":false}],"q_expl":"You can change cluster IDs only by deleting and recreating the cluster. Also, you cannot change the instance ID or the instance\u2019s storage type, and you cannot downgrade a production instance to a development instance. To change any of these settings, you must create a new instance with your preferred settings; export your data from the old instance; import your data into the new instance; and delete the old instance.\nFrom the description above, the best solution is using Dataflow to migrate data from the old BigTable instance to the new one.\nAll other answers are incorrect based on the description.\nReference:\u00a0https:\/\/cloud.google.com\/bigtable\/docs\/modifying- instance"},{"label":"test_5","q_format":"single","q_text":"You use BigQuery as the main data warehouse. You decided to perform advanced data transformation of the data. You want to use Dataproc with Apache Spark to do the transformation. How can you enable Dataproc\u2019s access to data in BigQuery?","answers":[{"ans":"Install Dataproc\u2019s BigQuery connector on the cluster using initialization actions. Dataproc temporarily loads data from BigQuery to Google Storage. If failed, Dataproc deletes temp files before finishing the job.","val":false},{"ans":"Install Dataproc\u2019s BigQuery connector on the cluster using initialization actions. Dataproc temporarily loads data from BigQuery to Google Storage. If failed, you need to manually delete temp files.","val":true},{"ans":"Dataproc cannot directly connect to BigQuery. You should export data from BigQuery to Google Storage first. Dataproc can then read data from Google Storage. You need to manually delete data files after Dataproc is done.","val":false},{"ans":"Dataproc can connect to BigQuery if you set the cluster as owner to the dataset.","val":false}],"q_expl":"You can use a BigQuery connector to enable programmatic read\/write access to BigQuery. This is an ideal way to process data that is stored in BigQuery. No command-line access is exposed. The BigQuery connector is a Java library that enables Hadoop to process data from BigQuery using abstracted versions of the Apache Hadoop InputFormat and OutputFormat classes.\nYou can access BigQuery from Dataproc by installing BigQuery connector to the Dataproc cluster using initialization actions. When a Dataproc spark job reads from BigQuery, it writes the BigQuery table\u2019s content temporarily to Google Storage using Dataproc cluster\u2019s assigned bucket. If the job completes successfully, temporary files are automatically deleted from the cluster. If the job fails, you need to delete temp files manually.\nAnswer A is incorrect: If a job fails, you need to delete temp files manually.\nAnswer C is incorrect: Dataproc can read from BigQuery by installing the connector. No need to export data from BigQuery to Google Storage manually.\nAnswer D is incorrect: Dataproc cannot directly read from BigQuery without installing the connector first.\nReferences:\nhttps:\/\/cloud.google.com\/dataproc\/docs\/concepts\/connectors\/bigquery\nhttps:\/\/cloud.google.com\/dataproc\/docs\/concepts\/connectors\/bigquery\nhttps:\/\/cloud.google.com\/dataproc\/docs\/concepts\/configuring-clusters\/init-actions"},{"label":"test_5","q_format":"single","q_text":"You have a Dataflow pipeline to run and process set of data files received from a client for transformation and load data into data warehouse. This pipeline should run at morning everyday so metrics can be ready when stakeholders need the latest stats based on data sent the day before. Which tool should you use?","answers":[{"ans":"Cloud Functions","val":false},{"ans":"Compute Engine","val":false},{"ans":"Kubernetes Engine","val":false},{"ans":"Cloud Scheduler","val":true}],"q_expl":"Cloud Scheduler\u00a0is a fully managed enterprise-grade cron job scheduler. It allows you to schedule virtually any job, including batch, big data jobs, cloud infrastructure operations, and more. You can automate everything, including retries in case of failure to reduce manual toil and intervention. Cloud Scheduler even acts as a single pane of glass, allowing you to manage all your automation tasks from one place.\nReference:\u00a0https:\/\/cloud.google.com\/scheduler\/"},{"label":"test_5","q_format":"single","q_text":"A FinTech company has over 20TB of data in ORC format stored in on-premise disks. The CTO decides to migrate the current infrastructure to Google Cloud. The current data pipeline cleanses and transforms raw data for reporting and further analysis and prediction using Apache Hive & Spark. Which of the following Google Cloud products you should use?","answers":[{"ans":"Dataproc for processing, Google Storage for storing data, Dataflow for data pipeline.","val":true},{"ans":"Dataproc for processing, BigQuery for storage, Dataflow for data pipeline.","val":false},{"ans":"App Engine for processing, Google Storage for storing data, Dataflow for data pipeline.","val":false},{"ans":"Dataproc for processing, Dataproc local HDFS for storage, Dataflow for data pipeline.","val":false}],"q_expl":"When you want to move Hadoop & Spark workloads from an on-premises environment to Google Cloud Platform (GCP), It\u2019s recommended to use Dataproc to run Apache Spark & Hadoop clusters.\nCloud Storage is a good option if:\nYour data in ORC, Parquet, Avro, or any other format will be used by different clusters or jobs, and you need data persistence if the cluster terminates.\nYou need high throughput and your data is stored in files larger than 128 MB.\nYou need cross-zone durability for your data.\nYou need data to be highly available\u2014for example, you want to eliminate HDFS NameNode as a single point of failure.\nReference:\nhttps:\/\/cloud.google.com\/solutions\/migration\/hadoop\/ migrating-apache-spark-jobs-to-cloud-dataproc"},{"label":"test_5","q_format":"single","q_text":"A dairy products company is using sensors installed around different areas in its farms to monitor employees activities and detect any intruders. Apache Kafka cluster is used to gather the events coming from sensors. Recently, Kafka cluster is becoming a bottleneck causing lag in receiving sensor events. Turns out sensors are sending more frequent events and due to the company expanding with more farms, more sensors are installed and this will cause extra load on the cluster. What is the most resilient approach to solve this issue?","answers":[{"ans":"Use pub\/sub to ingest and stream sensor events.","val":true},{"ans":"Scale out Kafka cluster to withstand the continuously flowing event stream.","val":false},{"ans":"Spin up a new Kafka cluster and distribute sensors even streams between the two clusters.","val":false},{"ans":"Build a Dataflow pipeline to ingest the events stream.","val":false}],"q_expl":"Cloud Pub\/Sub\u00a0is a service to ingest event streams at any scale. It\u2019s scalable and reliable for stream analytics and event-driven computing systems. So it\u2019s the most reliable Google product for such a scenario.\nAnswers B & C are wrong because these are not scalable solutions.\nAnswer D is wrong because Dataflow cannot ingest event streams.\nReference:\u00a0https:\/\/cloud.google.com\/pubsub\/docs\/overview"},{"label":"test_5","q_format":"single","q_text":"An e-payment company collects its service payment transaction events from its app installed in nearly 200,000 devices. Those events need to be stored for further time-series analysis and fraud detection. Which of the following approaches is recommended to implement?","answers":[{"ans":"Use Google Storage to store data. Use Dataproc with Apache Hive to do required queries on data.","val":false},{"ans":"Use Cloud SQL as a database. Make sure you launch a multi-regional instance for higher performance.","val":false},{"ans":"Use BigTable as a database. Use short & wide tables when designing the schema and row key.","val":false},{"ans":"Use BigTable as a database. Use tall & narrow tables when designing the schema and row key.","val":true}],"q_expl":"Storing time-series data in Cloud Bigtable is a natural fit. Cloud Bigtable stores data as unstructured columns in rows; each row has a row key, and row keys are sorted lexicographically.\nFor time series, you should generally use tall and narrow tables. This is for two reasons:\n\u2013 Storing one event per row makes it easier to run queries against your data.\n\u2013 Storing many events per row makes it more likely that the total row size will exceed the recommended maximum.\nAnswer A is incorrect: For this scenario, using BigTable is preferred over storing data in Google Storage as further data partitioning and file formatting are required to use Dataproc with Apache Hive.\nAnswer B is incorrect: Cloud SQL is a relational database. Event data might not have a fixed structure. Cloud SQL is not scalable to write thousands of rows in a given second.\nAnswer C is incorrect: Wide & short table schema is not optimal for time-series event data.\nReference:\u00a0https:\/\/cloud.google.com\/bigtable\/docs\/schema- design-time-series"},{"label":"test_5","q_format":"single","q_text":"A company wants to use NoSQL database for storing its system logs. The system can generate thousands of logs every minute. Those logs are occasionally read by security team in case of possible anomaly behavior or developers for debugging purposes. Due to system\u2019s architecture, system logs are not structured and can be different between different components. Which database do you suggest be used for this scenario?","answers":[{"ans":"Use BigTable as a database with HDD storage to store system logs.","val":true},{"ans":"Use BigTable as a database with SSD storage to store system logs.","val":false},{"ans":"Use Datastore as a database to store system logs.","val":false},{"ans":"Use Firebase as a database to store system logs.","val":false}],"q_expl":"When you create a Cloud Bigtable instance, you choose whether its clusters store data on solid-state drives (SSD) or hard disk drives (HDD).HDD storage is suitable for use cases that meet the following criteria: \n\u2013 You expect to store at least 10 TB of data. \n\u2013 You will not use the data to back a user-facing or latency-sensitive application. \n\u2013 Your workload falls into one of the following categories: \n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u2013 Batch workloads with scans and writes, and no more than occasional random reads of a small \nnumber of rows. \n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u2013 Data archival, where you write very large amounts of data and rarely read that data. \nFrom the scenario, system logs are to be stored to BigTable. This data will be only used for occasional debugging and security anomaly detection. So, using the HDD storage type for BigTable is the answer. \nAnswer B is incorrect: The scenario does not require SSD storage type. \nAnswer C is incorrect: Datastore is not built for storing and reading huge data volumes as in this scenario. Datastore is designed for web applications of a small scale. \nAnswer D is incorrect: Firebase is for mobile and web applications. Not a solution for storing big data. \nReference:\u00a0https:\/\/cloud.google.com\/bigtable\/docs\/choosing-ssd-hdd"},{"label":"test_5","q_format":"single","q_text":"You design a pipeline for your company. You want to find a solution to store event data generated in CSV format. The goal is to query data using SQL over a time window. Which storage and schema design should you use recommended by Google?","answers":[{"ans":"Use Google Storage to store event data and use BigQuery to create external tables referencing event data and partitioned by time window.","val":true},{"ans":"Use Google Storage to store event data and use DataPrep jobs to partition data by time windows.","val":false},{"ans":"Use BigTable for storage and design tall and narrow tables adding each event as single row.","val":false},{"ans":"Use BigTable for storage and design short and wide tables adding each event as single row.","val":false}],"q_expl":"The scenario states the goal is to query data using SQL. From the available answers, BigQuery is the best service to meet this requirement. Data can be stored in Google Storage, partitioned by time.\nBigQuery can read directly from Google Storage creating external tables partitioned by time.\nAnswer B is incorrect: Dataprep does not support SQL queries.\nAnswer C & D are incorrect: BigTable is a NoSQL database by nature. Nonetheless, it supports SQL to query data. However, Bigtable is used if scaling is a critical issue. For this scenario, data is in CSV format and BigQuery is better structured to handle importing CSV data. While Bigtable requires extra prerequisites.\nReference:\u00a0https:\/\/cloud.google.com\/bigquery\/external-data- sources"},{"label":"test_5","q_format":"single","q_text":"Analytics team receives data from different data sources stored in Google Storage. The team wants to query the data for required ETL operations which they will fully take care of using SQL. They want your advice on what is the best approach recommended by Google to do it. What would you suggest?","answers":[{"ans":"Batch load the data from Google Storage into BigQuery using its batch API, run cleansing and transformation queries on data and insert the transformed rows to another BigQuery table.","val":false},{"ans":"Batch load the data from Google Storage into BigQuery using its batch API, run cleansing and transformation queries on data and export the data to Google Storage. Launch Dataproc cluster and use Hive to query the transformed data.","val":false},{"ans":"Create external tables on data using BigQuery, apply the cleansing and transformation queries on data then load the output to an internal BigQuery table for reporting and visualization.","val":true},{"ans":"Create external tables on data using BigQuery, apply the cleansing and transformation queries on data then load the output to BigTable for reporting and visualization.","val":false}],"q_expl":"An external data source (also known as a federated data source) is a data source that you can query directly even though the data is not stored in BigQuery. Instead of loading or streaming the data, you create a table that references the external data source.\nQuerying an external data source using a temporary table is useful for one-time, ad-hoc queries over external data, or for extract, transform, and load (ETL) processes.\nIn summary, using external tables in BigQuery is useful for such cases:\n1. Perform ETL operations on data.\n2. Frequently changed data.\n3. Data is being ingested periodically.\nAnswer C is the correct answer based on the above explanation and using BigQuery for reporting and visualization is a better approach.\nAnswer D is incorrect because BigTable isn\u2019t a practical (and cheap) approach to report and visualize data.\nAnswers A & B are incorrect: Based on Google\u2019s best practices, using external tables for ETL is better than loading data to BigQuery.\nReferences:\nhttps:\/\/cloud.google.com\/bigquery\/external-data-sources\nhttps:\/\/cloud.google.com\/bigquery\/external-table-definition"},{"label":"test_5","q_format":"single","q_text":"You maintain a web service used by over 12,000 different clients. The service uses MySQL as its main data storage hosted on Google Cloud SQL. Since the service is critical and should be highly available at all times, this includes the MySQL database, considerations should be met so the service does not face any outage due to network connectivity or database having a temporary outage. The service was not developed by your company so code refactoring is not currently possible. How can you ensure high availability for the web service\u2019s database?","answers":[{"ans":"Scale up Cloud SQL instance to high CPU machine type.","val":false},{"ans":"Enable high availability on Cloud SQL instance by creating a read replica.","val":false},{"ans":"Migrate the database to BigQuery and use it as the main data storage instead.","val":false},{"ans":"Enable high availability on Cloud SQL instance by creating a fail-over replica.","val":true}],"q_expl":"The failover replica in Cloud SQL is configured with the same database flags, users (including root) and passwords, authorized applications and networks, and databases as the primary instance. If a High-availability-configured instance becomes unresponsive, Cloud SQL automatically switches to serving data from the failover replica. This is called a failover.\nAnswer A is incorrect: Scaling up Cloud SQL instance does not ensure high availability.\nAnswer B is incorrect: Read replicas offer a copy of the master (primary) instance to read from. In case the master instance is down, read replica will not be updated.\nAnswer C is incorrect: Migrating the database to a different solution is not applicable based on the scenario because the service cannot be refactored.\nReferences:\nhttps:\/\/cloud.google.com\/sql\/docs\/mysql\/high-availability\nhttps:\/\/cloud.google.com\/sql\/docs\/mysql\/replication"},{"label":"test_5","q_format":"multiple","q_text":"Your team decided to use BigTable for storing event data. The engineer responsible of launching and testing the instance has reported a slower performance than expected by Google Cloud documentation. Which of the following could be a factor for the slow performance? (select 3)","answers":[{"ans":"The rows in the tables tested contain high number of cells.","val":true},{"ans":"The rows in the tables have large data size.","val":true},{"ans":"Test data size is over 300GB.","val":false},{"ans":"The instance uses SSD storage type.","val":false},{"ans":"Heavy pre-test was done before the testing started.","val":false},{"ans":"The instance doesn\u2019t have enough nodes.","val":true}],"q_expl":"There are several factors that can cause Cloud Bigtable to perform more slowly than expected:\nThe table\u2019s schema is not designed correctly. To get good performance from Cloud BigTable, it\u2019s essential to design a schema that makes it possible to distribute reads and writes evenly across each table.\nThe workload isn\u2019t appropriate for Cloud BigTable. If you test with a small amount (< 300 GB) of data, or if you test for a very short period of time (seconds rather than minutes or hours), Cloud BigTable won\u2018t be able to balance your data in a way that gives you good performance. The rows in your Cloud Bigtable table contain large amounts of data. You can read and write larger amounts of data per row, but increasing the amount of data per row will also reduce the number of rows per second. The rows in your Cloud Bigtable table contain a very large number of cells. It takes time for Cloud Bigtable to process each cell in a row. Also, each cell adds some overhead to the amount of data that\u2018s stored in your table and sent over the network.\nThe Cloud Bigtable cluster doesn\u2018t have enough nodes. If your Cloud Bigtable cluster is overloaded, adding more nodes can improve performance. The Cloud Bigtable cluster was scaled up or scaled down recently. After you change the number of nodes in a cluster, it can take up to 20 minutes under load before you see an improvement in the cluster\u2018s performance. The Cloud Bigtable cluster uses HDD disks. In most cases, your cluster should use SSD disks, which have significantly better performance than HDD disks. The Cloud Bigtable instance is a development instance. Development instance\u2018s performance is equivalent to an instance with one single-node cluster, it will not perform as well as a production instance.\nThere are issues with the network connection. Network issues can reduce throughput and cause reads and writes to take longer than usual.\nReference:\u00a0\u00a0https:\/\/cloud.google.com\/bigtable\/docs\/performance"},{"label":"test_5","q_format":"single","q_text":"You have built a machine learning model to classify if a customer would buy a certain product when recommended by the company\u2019s website. You trained the model with a sample set. Upon testing the model, you found out only 28% of the testing sets are actually true positives and the model isn\u2019t very accurate. You figured out the model is overfitted. How would you solve this?","answers":[{"ans":"Increase training data, increase feature parameters & increase regularization.","val":false},{"ans":"Decrease training data, decrease feature parameters & increase regularization.","val":false},{"ans":"Increase training data, decrease feature parameters & increase regularization.","val":true},{"ans":"Increase training data, decrease feature parameters & decrease regularization.","val":false}],"q_expl":"Overfitting happens when a model performs well on a training set, generating only a small error while giving the wrong output for the test set. This happens because the model is only picking up specific features input found in the training set instead of picking out the general features of the given training set. \nTo solve overfitting, the following would help improving the model\u2019s quality: \n\u2013 Increase the number of examples, the more data a model is trained with, the more use cases the \nmodel can be training on and better improves its predictions. \n\u2013 Tune hyperparameters which are related to number and size of hidden layers (for neural networks), \nand regularization, which means using techniques to make your model simpler such as using \ndropout method to remove neuron networks or adding \u201cpenalty\u201c parameters to the cost function. \n\u2013 Remove features by removing irrelevant features. Feature engineering is a wide subject and feature \nselection is a critical part of building and training a model. Some algorithms have a built-in feature \nselection, but in some cases, data scientists need to cherry-pick or manually select or remove \nfeatures for debugging and finding the best model output. \nFrom the brief explanation, to solve the overfitting problem in the scenario, you need to: \n\u2013 Increase the training set. \n\u2013 Decrease features parameters. \n\u2013 Increase regularization. \nReference:\u00a0https:\/\/cloud.google.com\/solutions\/building-a- serverless-ml-model"},{"label":"test_5","q_format":"single","q_text":"You have a complex data pipeline that has a combination of shell scripts, python code, and spark jobs. These tasks are scheduled by cron jobs to run. The problem with this approach is, in case of failure, the whole pipeline breaks and failure control with stopping the next tasks from running after a certain task fails and re-running the pipeline again is difficult and messy. You want a solution that can manage the pipeline\u2019s different jobs to be failure-resilient, scalable and easy to monitor. What approach is best for this scenario?","answers":[{"ans":"Use Cloud Composer to orchestrate the pipeline workflow.","val":true},{"ans":"Use Dataproc for Apache Spark jobs and migrate all other tasks to use Apache Spark instead.","val":false},{"ans":"Use Cloud Scheduler to schedule pipeline\u2019s tasks.","val":false},{"ans":"Use Dataflow to re-build the data pipeline.","val":false}],"q_expl":"Cloud Composer is a fully managed workflow orchestration service built on Apache Airflow. Cloud composer is built specifically to schedule and monitor workflows and take required actions. You can use Cloud Composer to orchestrate diverse task types, whether they are shell scripts, python code or spark jobs.\nAnswer B is incorrect: Dataproc does not orchestrate workflows and migrating all tasks can be a difficult job.\nAnswer C is incorrect: Using Cloud Scheduler does not solve the core problem which is how to manage the workflow.\nAnswer D is incorrect: Dataflow pipelines are effective for simple and direct workflows that are similar in environment and purpose. This scenario is dealing with a complicated pipeline with various tech and code snippets used.\nReference:\u00a0https:\/\/cloud.google.com\/composer\/"},{"label":"test_5","q_format":"single","q_text":"You have a PostgreSQL database instance on Cloud SQL. The database is used in production by the different micro-services writing and reading to it based on each micro-service\u2019s needs. Data analysts connect to the instance to run their analysis and reporting SQL queries which adds more load to the database with intensive I\/O operations on it. You want to find a solution that can avoid intensive I\/O operations on the production database and allow data analysts to run their queries without interruptions may lead to late reports to management. What should you do?","answers":[{"ans":"Scale up Cloud SQL instance from standard to high CPU machine type.","val":false},{"ans":"Use Cloud SQL read replicas to replicate production instance. Share endpoint with data analysts to replace with existing endpoint in their SQL clients to use the replica.","val":true},{"ans":"Create a snapshot from production instance. Create a new Cloud SQL instance from the snapshot. Use Cloud Scheduler cron job to create a snapshot and a new instance daily.","val":false},{"ans":"Import data from Cloud SQL instance to BigQuery. Create necessary users on BigQuery dataset created for data analysts to run their SQL queries on.","val":false}],"q_expl":"Cloud SQL provides the ability to replicate a master instance to one or more read replicas. A read replica is a copy of the master that reflects changes to the master instance in almost real-time. You create a replica to offload read requests or analytics traffic from the master. You can create multiple read replicas for a single master instance. By default, the replica is created with the same number of CPUs and the same amount of memory as the master instance. You can increase these values for the replica, but you cannot decrease them.\nAnswer A is incorrect: Scaling up an instance by CPU will not solve the core issue which is analysts using production instance for intensive reads.\nAnswer C & D are incorrect: While both approaches are viable, they require maintenance and implementation to be done by engineers. Google Cloud provides a fully-managed service called read replica that can handle replicating production instances in a near real-time manner.\nReferences:\nhttps:\/\/cloud.google.com\/sql\/docs\/mysql\/instance-settings\nhttps:\/\/cloud.google.com\/sql\/docs\/postgres\/replication\/create-replica"},{"label":"test_5","q_format":"single","q_text":"You are in need to restore a snapshot of a compute engine instance you have previously scheduled for regular daily snapshots. Which of the following are the steps you should do to perform the restoration?","answers":[{"ans":"You can simply create a replacement instance directly by selecting the snapshot from the list of snapshots available.","val":true},{"ans":"You need to create a persistent disk from the snapshot of your choice. Create a new compute engine instance and attach the persistent disk to it.","val":false},{"ans":"Create a new compute instance with the same exact machine type as the one the snapshot was created from. Create a persistent disk using the snapshot to be restored from. Attach the persistent disk to the compute engine instance.","val":false},{"ans":"Export snapshot to Google Storage. Create new compute engine instance, then using gsutil tool, copy the snapshot to the instance\u2019s persistent disk to be restored.","val":false}],"q_expl":"Google Cloud supports easy snapshot restoration to a persistent disk as well as restoring a book disk snapshot to create a new VM instance. You can simply create a replacement instance directly by selecting the snapshot from the list of snapshots available.\nReference:\u00a0https:\/\/cloud.google.com\/compute\/docs\/disks\/restore-and-delete-snapshots"},{"label":"test_5","q_format":"single","q_text":"You\u2018ve migrated a Hadoop job from an on-premises cluster to Dataproc and Good Storage. Your Spark job is a complex analytical workload fiat consists of many shuffling operations, and initial data are parquet toes (on average 200-400 MB size each) You see some degradation in performance after the migration to Dataproc so you\u2018d like to optimize for it. Your organization is very cost-sensitive so you\u2018d Idee to continue using Dataproc on preemptibles (with 2 non-preemptibles workers only) for this workload. What should you do?","answers":[{"ans":"Switch from HODs to SSDs override the preemptible VMs configuration to increase the boot disk size","val":false},{"ans":"Increase the see of your parquet files to ensure them to be 1 GB minimum","val":false},{"ans":"Switch to TFRecords format (appr 200 MB per We) instead of parquet files","val":false},{"ans":"Switch from HDDs to SSDs. copy initial data from Cloud Storage to Hadoop Distributed File System (HDFS) run the Spark job and copy results back to Cloud Storage","val":true}],"q_expl":"To optimize the performance of your Spark job on Dataproc while using preemptible VMs and considering cost sensitivity, the best approach would be:\nD. Switch from HDDs to SSDs, copy initial data from Cloud Storage to Hadoop Distributed File System (HDFS), run the Spark job, and copy results back to Cloud Storage.\nHere\u2018s the reasoning behind this choice:\n1. Switching from HDDs to SSDs: SSDs provide significantly faster read and write speeds compared to HDDs, resulting in improved performance for shuffle operations and overall job execution. This upgrade can help mitigate the performance degradation you observed after the migration to Dataproc.\n2. Copying initial data to HDFS: By copying the initial data from Cloud Storage to HDFS, you can take advantage of the faster local disk access provided by HDFS. This can further enhance the read performance, especially for Parquet files, as they are columnar storage formats.\n3. Running the Spark job on Dataproc: Utilizing the preemptible VMs along with 2 non-preemptible workers helps reduce costs while still having a reasonable level of compute resources. The preemptible VMs may have a higher risk of being interrupted, but the cost savings outweigh the occasional interruptions in this cost-sensitive scenario.\n4. Copying results back to Cloud Storage: After the Spark job completes, copying the results back to Cloud Storage ensures data persistence and availability for further analysis or downstream processing.\nOptions A and B are not directly relevant to addressing the performance degradation or optimizing for cost-efficiency.\nOption C suggests switching to TFRecords format instead of Parquet files. While TFRecords can be efficient for TensorFlow-based workloads, it may not be the most effective solution for optimizing Spark jobs, especially if your existing workload relies on Parquet files.\nTherefore, the best choice among the given options is D. Switch from HDDs to SSDs, copy initial data from Cloud Storage to HDFS, run the Spark job, and copy results back to Cloud Storage. This approach leverages the faster disk access of SSDs and takes advantage of HDFS for local data access while utilizing preemptible VMs to minimize costs."},{"label":"test_5","q_format":"single","q_text":"As a Data Engineer, you must follow your organization\u2018s regulations to prepare Cloud Storage for storing company\u2018s data. These regulations require to archive data older than one year and delete data older than three years. What should you do?","answers":[{"ans":"You should run a script every day. Copy data that is older than one year to an archive bucket, and delete data from three years ago.","val":false},{"ans":"You should set up Object Lifecycle Management policies.","val":true},{"ans":"You should set up default storage class for two buckets with storage classes:\u00a0Standard\u00a0and\u00a0Archive. Use a script to move the data in the appropriate bucket when its needed.","val":false},{"ans":"You should run a script every day. Set storage class to\u00a0Archive\u00a0for data that is older than one year, and delete data from three years ago.","val":false}],"q_expl":"Object Lifecycle Management policy allows you to automate the implementation of your organization\u2019s regulations."},{"label":"test_6","q_format":"multiple","q_text":"Select all options when you can benefit from using sole-tenant nodes. (select 3)","answers":[{"ans":"Windows workloads with licensing requirements.","val":true},{"ans":"Healthcare workloads with security and compliance requirements.","val":true},{"ans":"Analytical workloads with jobs that can be terminated and easily recreated.","val":false},{"ans":"Gaming workloads with performance requirements.","val":true}],"q_expl":"1. Windows workloads with licensing requirements.\n2. Healthcare workloads with security and compliance requirements.\n3. Gaming workloads with performance requirements.\nSo the correct options are:\nWindows workloads with licensing requirements. Healthcare workloads with security and compliance requirements. Gaming workloads with performance requirements.\nLink to resources:\nhttps:\/\/cloud.google.com\/compute\/docs\/nodes\/sole-tenant-best-practices"},{"label":"test_6","q_format":"single","q_text":"Your company wants to connect cloud applications to an Oracle database in on-premises data center. Your connection requires of 8 Gbps of data. Which service should you use?","answers":[{"ans":"Dedicated Interconnect","val":true},{"ans":"Cloud Router with VPN","val":false},{"ans":"Partner Interconnect","val":false},{"ans":"Implement a high-throughput Cloud VPN connection","val":false}],"q_expl":"To connect cloud applications to an Oracle database in an on-premises data center requiring 8 Gbps of data, the recommended service to use is Dedicated Interconnect.\nDedicated Interconnect provides a direct physical connection between an on-premises data center and a Google Cloud VPC, allowing for high-speed, low-latency, and secure data transfer. With Dedicated Interconnect, you can provision a dedicated 10 Gbps or 100 Gbps connection between your on-premises network and your VPC. This can enable high-performance, reliable, and secure connections to mission-critical applications and services running on Google Cloud.\nCloud Router with VPN and Partner Interconnect can also be used for connecting on-premises data centers to Google Cloud, but they may not be able to support the required bandwidth of 8 Gbps. Implementing a high-throughput Cloud VPN connection may also not be suitable for this scenario as it is limited to a maximum throughput of 3 Gbps."},{"label":"test_6","q_format":"multiple","q_text":"You work for a cloud consulting company and your are responsible for managing BigQuery resources for different departments. You need to secure data so that departments can only access their own data and not access other departments\u2018 data. What should you do to achieve this? (select 3)","answers":[{"ans":"You should use the appropriate identity and access management (IAM) roles for each department\u2018s users.","val":true},{"ans":"You should restrict a department\u2018s dataset to approved users.","val":true},{"ans":"You should load data into a different dataset for each department.","val":true},{"ans":"You should load data into different partitions.","val":false}],"q_expl":"The correct options to achieve data security in BigQuery so that departments can only access their own data are: \n1. You should use the appropriate identity and access management (IAM) roles for each department\u2018s users. \n2. You should restrict a department\u2018s dataset to approved users. \n3. You should load data into a different dataset for each department. \nExplanation: \n1. Identity and access management (IAM) roles in BigQuery allow you to control who has access to which resources in your project. By assigning appropriate IAM roles to each department\u2018s users, you can ensure that they can only access their own data and not access other departments\u2018 data. \n2. Restricting a department\u2018s dataset to approved users is another way to ensure that only authorized users can access the data. This can be done by assigning appropriate IAM roles and permissions to the dataset, as well as setting up a list of approved users who can access the dataset. \n3. Loading data into different datasets for each department can help you segregate the data and ensure that each department can only access its own data. This can also help with data organization and management. \nLoading data into different partitions, on the other hand, is not a recommended approach to achieve data security for different departments. Partitioning helps you manage large datasets efficiently, but it does not provide security or access control. \nReference: \nBigQuery Identity and Access Management (IAM):\u00a0https:\/\/cloud.google.com\/bigquery\/docs\/access-control \nBigQuery dataset access controls:\u00a0https:\/\/cloud.google.com\/bigquery\/docs\/dataset"},{"label":"test_6","q_format":"single","q_text":"Which of the following is a best practice for securing data on the Google Cloud Platform?","answers":[{"ans":"Store sensitive data in plain text format","val":false},{"ans":"Use a single Google Cloud Identity and Access Management (IAM) role for all users","val":false},{"ans":"Grant permissions to the least privilege required","val":true},{"ans":"Use publicly available encryption algorithms","val":false}],"q_expl":"A best practice for securing data on the Google Cloud Platform is to grant permissions to the least privilege required. This means granting only the minimum access required for a user or service account to perform its job."},{"label":"test_6","q_format":"single","q_text":"You work for an online marketing company. Your new campaign must be spread across multiple countries with different languages. You\u2018ve already prepared a transcripts for each language. Which Machine Learning API should you use to convert these transcripts (text) to speech?","answers":[{"ans":"Text-to-Speech API","val":true},{"ans":"Speech-to-Text API","val":false},{"ans":"Translation API","val":false},{"ans":"Natural Language API","val":false}],"q_expl":"To convert the transcripts (text) to speech in different languages, you should use the Text-to-Speech API provided by Google Cloud. The Text-to-Speech API enables you to convert written text into natural-sounding audio in a variety of languages and voices.\nThe Speech-to-Text API is used for converting spoken words to text. The Translation API is used for translating text from one language to another. The Natural Language API is used for analyzing the structure and meaning of text.\nTherefore, for the scenario described, the most appropriate API is the Text-to-Speech API."},{"label":"test_6","q_format":"multiple","q_text":"Select all of the features\/tools below that are included with Google Cloud CLI. (select 3)","answers":[{"ans":"kubectl\u00a0(provides commands for greater control over Kubernetes cluster)","val":true},{"ans":"aws\u00a0(provides access to AWS resources)","val":false},{"ans":"gsutil\u00a0(provides access to Cloud Storage)","val":true},{"ans":"bq\u00a0(enables running queries in BigQuery)","val":true},{"ans":"hadoop\u00a0(provides access to Hadoop)","val":false}],"q_expl":"The features\/tools included with Google Cloud CLI are:\nkubectl (provides commands for greater control over Kubernetes cluster)\ngsutil (provides access to Cloud Storage)\nbq (enables running queries in BigQuery)\nTherefore, the correct answer is:\nkubectl\ngsutil\nbq\nhttps:\/\/cloud.google.com\/cli"},{"label":"test_6","q_format":"multiple","q_text":"Select NoSQL solutions available on Google Cloud. (select 2)","answers":[{"ans":"Cloud Storage","val":false},{"ans":"Cloud SQL","val":false},{"ans":"Bigtable","val":true},{"ans":"Cloud Datastore","val":true},{"ans":"Cloud Spanner","val":false}],"q_expl":"The two NoSQL solutions available on Google Cloud are:\n1. Cloud Datastore: Google Cloud Datastore is a highly-scalable NoSQL document database. It provides a fully managed and serverless environment for storing and querying structured data. It is designed to handle large-scale applications and offers high availability, automatic scaling, and built-in data replication.\n2. Bigtable: Google Cloud Bigtable is a distributed NoSQL database designed for handling massive amounts of data with low latency. It is a fully managed service that provides high scalability and performance for large-scale applications. Bigtable is well-suited for use cases that require real-time access to large datasets, such as time series data, analytics, and machine learning.\nCloud Storage is an object storage service ,\u00a0 Cloud SQL and Spanner\u00a0 are a relational database service, both of which are not NoSQL solutions."},{"label":"test_6","q_format":"single","q_text":"A data science team of your company has developed a deep learning model using TensorFlow. They predict that training could take several weeks using a 8 vCPU Compute Engine instance. What can you advise them?","answers":[{"ans":"They should try to use a Tensor Processing Units (TPUs) for training.","val":true},{"ans":"They should try to use a GKE cluster with CPUs.","val":false},{"ans":"They should use App Engine for training.","val":false},{"ans":"They should try to use more vCPUs.","val":false}],"q_expl":"For a deep learning model that takes several weeks to train using an 8 vCPU Compute Engine instance, I would advise the data science team to try using a Tensor Processing Unit (TPU) for training.\nTPUs are custom-designed ASICs (application-specific integrated circuits) that are optimized for performing matrix multiplication operations required for deep learning models. TPUs can significantly reduce the training time for deep learning models compared to traditional CPUs or GPUs.\nUsing a GKE cluster with CPUs or increasing the number of vCPUs may improve training time, but it may not provide the same level of performance improvement as using a TPU.\nApp Engine is not recommended for training deep learning models as it is a fully-managed platform that is optimized for serving web applications and APIs, rather than training machine learning models."},{"label":"test_6","q_format":"single","q_text":"As a Data Engineer, you want to take advantage of reusable machine learning models that are pre-trained and ready for fine-tuning (such as BERT, Faster R-CNN). Which service should you use?","answers":[{"ans":"Cloud Vision","val":false},{"ans":"Cloud Document AI","val":false},{"ans":"Cloud Vertex AI","val":false},{"ans":"TensorFlow Hub","val":true}],"q_expl":"To take advantage of reusable machine learning models that are pre-trained and ready for fine-tuning, such as BERT and Faster R-CNN, you should use TensorFlow Hub. TensorFlow Hub is a repository of reusable machine learning models that have been pre-trained on large datasets and can be easily integrated into new applications with minimal effort. TensorFlow Hub provides a wide range of models for various tasks such as natural language processing, image recognition, and object detection.\nCloud Vision and Cloud Document AI are machine learning services provided by Google Cloud that offer pre-trained models for specific use cases such as image analysis and document analysis. However, they do not provide the same level of flexibility and customizability as TensorFlow Hub.\nCloud Vertex AI is a fully-managed machine learning platform provided by Google Cloud that provides tools for building and deploying custom machine learning models. While Cloud Vertex AI can be used for training custom models, it does not provide pre-trained models for fine-tuning like TensorFlow Hub does."},{"label":"test_6","q_format":"single","q_text":"After using GCP for a while, you noticed that your storage costs exceeded your project budget. You have a bucket in your project that stores application logs. You can afford to delete logs older than 30 days. What should you do to reduce costs?","answers":[{"ans":"You can\u2018t reduce costs in this case.","val":false},{"ans":"You should add a lifecycle management policy that deletes logs older than 30 days.","val":true},{"ans":"You should write a script that deletes logs older than 30 days and schedule the script with Cron Scheduler.","val":false},{"ans":"You should remove it manually every month.","val":false}],"q_expl":"To reduce storage costs for the application logs stored in your bucket, you should add a lifecycle management policy that deletes logs older than 30 days. This policy will automatically delete any objects in your bucket that meet the criteria set by the policy. You can set the policy to run daily or weekly to ensure that logs older than 30 days are regularly deleted, freeing up storage space and reducing costs.\nWriting a script to delete logs older than 30 days and scheduling the script with Cron Scheduler is also a possible solution, but using a lifecycle management policy is a simpler and more efficient way to manage object deletion in your bucket.\nRemoving logs manually every month is not a scalable solution and is prone to human error, so it is not recommended."},{"label":"test_6","q_format":"single","q_text":"Your data science team has prepared several deep learning neural networks using TensorFlow. You are responsible for training these models as efficient as possible. What should you use?","answers":[{"ans":"You should use Preemptible VMs.","val":false},{"ans":"You should use TPUs.","val":true},{"ans":"You should use GPUs.","val":false},{"ans":"You should use Shielded VMs.","val":false}],"q_expl":"To train deep learning neural networks as efficiently as possible, you should use GPUs or TPUs.\nGPUs (Graphical Processing Units) are specialized hardware that is designed to accelerate certain types of calculations, such as those used in deep learning. GPUs can significantly speed up training times for deep learning models compared to using only CPUs.\nTPUs (Tensor Processing Units) are custom-designed ASICs (application-specific integrated circuits) that are optimized for performing matrix multiplication operations required for deep learning models. TPUs can provide even faster training times compared to GPUs for certain types of deep learning workloads.\nWhile Shielded VMs and Preemptible VMs are useful features of Google Cloud, they do not specifically address the needs of training deep learning models. Shielded VMs are a security feature that protects VMs from rootkits and bootkits, while Preemptible VMs are a cost-saving feature that allow you to use spare capacity at a lower cost but can be interrupted at any time.\nhttps:\/\/www.tensorflow.org\/guide\/tpu https:\/\/cloud.google.com\/tpu\/docs\/tensorflow-quickstart-tpu-vm"},{"label":"test_6","q_format":"single","q_text":"As a Data Engineer, you have installed Cloud SDK. Which command would you use to perform operations on BigQuery?","answers":[{"ans":"gsutil","val":false},{"ans":"gcloud","val":false},{"ans":"bigquery","val":false},{"ans":"bq","val":true}],"q_expl":"To perform operations on BigQuery using the Cloud SDK, you would use the command \u201cbq\u201c. This command is a part of the Cloud SDK and provides a command-line interface for managing BigQuery resources, such as datasets, tables, and jobs.\nThe other commands listed have different purposes:\n\u201cgsutil\u201c is used for managing Google Cloud Storage resources\n\u201cgcloud\u201c is a command-line interface for managing various Google Cloud resources and services\n\u201cbigquery\u201c is not a valid command and would result in an error"},{"label":"test_6","q_format":"single","q_text":"A social media company wants to add a new feature for blog posting. They want to automatically generates thematic labels for the user\u2018s blog posts. They don\u2018t want to design this solution from scratch and use Google Cloud services. What should you recommend?","answers":[{"ans":"They should create a text labeling model using TensorFlow and BERT language model.","val":false},{"ans":"They should use the Cloud Natural Language API and Entity Analysis feature.","val":true},{"ans":"They should create a text classification model using TensorFlow and transfer learning.","val":false},{"ans":"They should use the Cloud Natural Language API and Sentiment Analysis feature.","val":false}],"q_expl":"To automatically generate thematic labels for blog posts, I would recommend using the Cloud Natural Language API and its Entity Analysis feature.\nThe Entity Analysis feature of the Cloud Natural Language API can identify and extract various entities from text, such as people, organizations, locations, events, and more. By analyzing the blog post text, the API can generate relevant thematic labels for the post.\nUsing a pre-built service like the Cloud Natural Language API can save time and effort compared to building a text labeling model from scratch using TensorFlow and BERT language model or creating a text classification model using TensorFlow and transfer learning. The Sentiment Analysis feature of the Cloud Natural Language API is designed to analyze the sentiment of a text and may not be suitable for generating thematic labels."},{"label":"test_6","q_format":"single","q_text":"You need to compose visualization for operations teams with the following requirements:Telemetry must include data from all 50,000 installations for the most recent 6 weeks (sampling once every minute)The report must not be more than 3 hours delayed from live data.The actionable report should only show suboptimal links.Most suboptimal links should be sorted to the top.Suboptimal links can be grouped and filtered by regional geography.User response time to load the report must be <5 seconds.You create a data source to store the last 6 weeks of data, and create visualizations that allow viewers to see multiple date ranges, distinct geographic regions, and unique installation types. You always show the latest data without any changes to your visualizations. You want to avoid creating and updating new visualizations each month. What should you do?","answers":[{"ans":"Look through the current data and compose a series of charts and tables, one for each possible combination of criteria.","val":false},{"ans":"Look through the current data and compose a small set of generalized charts and tables bound to criteria filters that allow value selection.","val":true},{"ans":"Export the data to a spreadsheet, compose a series of charts and tables, one for each possible combination of criteria, and spread them across multiple tabs.","val":false},{"ans":"Load the data into relational database tables, write a Google App Engine application that queries all rows, summarizes the data across each criteria, and then renders results using the Google Charts and visualization API.","val":false}],"q_expl":"To meet the requirements of creating visualizations for operations teams with the ability to see multiple date ranges, distinct geographic regions, and unique installation types without the need to create and update new visualizations each month, the best approach would be: \nB. Look through the current data and compose a small set of generalized charts and tables bound to criteria filters that allow value selection. \nHere\u2018s the reasoning behind this choice: \n1. Compose a small set of generalized charts and tables: Instead of creating a large number of individual charts and tables for each possible combination of criteria, a smaller set of generalized visualizations can be designed. These visualizations should provide an overview of the key metrics and insights that operations teams need to monitor and identify suboptimal links. By focusing on the most relevant data, the visualizations can provide actionable information efficiently. \n2. Use criteria filters that allow value selection: By incorporating criteria filters within the visualizations, the operations teams can dynamically select and filter the data based on their specific requirements. This allows them to explore different date ranges, geographic regions, and installation types without the need for creating and updating new visualizations each month. The filters provide flexibility and empower users to customize the view of the data based on their needs. \nOption A suggests creating a series of charts and tables, one for each possible combination of criteria. This approach would result in a large number of visualizations, making it difficult to manage and maintain. It would also require updates whenever new combinations of criteria are added. \nOption C suggests exporting the data to a spreadsheet and creating a series of charts and tables spread across multiple tabs. While spreadsheets can be useful for certain scenarios, they may not be the most efficient solution for creating and managing complex visualizations, especially when dealing with large datasets. \nOption D suggests loading the data into relational database tables and developing a custom application to query and summarize the data. While this approach offers flexibility, it also involves significant development effort and ongoing maintenance. It may not be the most efficient solution for avoiding the need to create and update new visualizations each month. \nTherefore, the best choice among the given options is B. Look through the current data and compose a small set of generalized charts and tables bound to criteria filters that allow value selection. This approach provides flexibility, maintainability, and a user-friendly experience for operations teams to explore and analyze the data."},{"label":"test_6","q_format":"single","q_text":"Suppose you want to create a group of virtual machines for processing big data (Hadoop, Spark). You can afford to interrupt jobs as they can be easily recreated. The solution must be as low cost as possible. What should you do?","answers":[{"ans":"You should create a Unmanaged Instance Group.","val":false},{"ans":"You should create a Managed Instance Group with VMs in a single zone.","val":false},{"ans":"You should create a Managed Instance Group that uses Preemptible VMs.","val":true},{"ans":"You should create a Managed Instance Group with VMs in multiple zones.","val":false}],"q_expl":"You should create a Managed Instance Group that uses Preemptible VMs. Preemptible VMs are low-cost instances that offer short-term capacity for fault-tolerant workloads. They are up to 80% cheaper than regular instances, and can be used for batch processing, Hadoop, Spark and other big data workloads that can tolerate interruptions. The Managed Instance Group provides automatic scaling and load balancing, while Preemptible VMs allow you to keep costs down."},{"label":"test_6","q_format":"single","q_text":"Your company wants to migrate an on-premises Hadoop cluster to Google Cloud. As a Data Engineer, you are responsible for the migration process. What is the recommended way to migrate while minimizing costs?","answers":[{"ans":"You should perform migrations incrementally. The first step is to migrate some data to Cloud Storage. Then, you can deploy an ephemeral Hadoop cluster with Dataproc to run job that uses that data. You should start with low-risk job at first.","val":true},{"ans":"You should migrate all data to Dataproc HDFS as soon as possible and deploy an ephemeral Hadoop cluster with Dataproc to run all jobs.","val":false},{"ans":"You should perform migrations incrementally. The first step is to migrate some data to Dataproc HDFS. Then, you can deploy an ephemeral Hadoop cluster with Dataproc to run job that uses that data. You should start with high-risk job at first.","val":false},{"ans":"You should migrate all data to Cloud Storage as soon as possible and deploy an ephemeral Hadoop cluster with Dataproc to run all jobs.","val":false}],"q_expl":"The recommended way to migrate an on-premises Hadoop cluster to Google Cloud while minimizing costs is to perform migrations incrementally. The first step is to migrate some data to Cloud Storage, and then deploy an ephemeral Hadoop cluster with Dataproc to run jobs that use that data. It is recommended to start with low-risk jobs at first.\nOnce the low-risk jobs are successfully executed, all the data should be migrated to Dataproc HDFS as soon as possible. Then, an ephemeral Hadoop cluster with Dataproc should be deployed to run all jobs.\nMigrating all data to Cloud Storage or Dataproc HDFS should be done after assessing the requirements and considering the cost of storage.\nReferences:\n1. Google Cloud. (n.d.). Migrating Hadoop clusters to Google Cloud. Retrieved from\u00a0https:\/\/cloud.google.com\/solutions\/migration\/hadoop\n2. Google Cloud. (n.d.). Migrate Hadoop workloads to Google Cloud with Dataproc. Retrieved from\u00a0https:\/\/cloud.google.com\/dataproc\/docs\/tutorials\/hadoop-oss-to-dataproc"},{"label":"test_6","q_format":"single","q_text":"An on-premises Hadoop cluster becomes inefficient for your jobs and needs to grow. You have a lot of data and need 60 TB of persistent disk per node. You want to move the cluster to the Google Cloud using Dataproc. What can you do to minimize the storage cost of this migration?","answers":[{"ans":"You should store data in Cloud Storage bucket.","val":false},{"ans":"You should move some of the cold data to Cloud Storage and keep only the hot data in persistent disk.","val":true},{"ans":"You should use Preemptible VMs for the Dataproc cluster.","val":false},{"ans":"You should customize your Dataproc cluster to store 60 TB per node.","val":false}],"q_expl":"To minimize the storage cost of migrating an on-premises Hadoop cluster to Google Cloud using Dataproc, you can move some of the cold data to Cloud Storage and keep only the hot data in persistent disk. This will allow you to use lower-cost storage options for the cold data, while still providing quick access to the frequently accessed hot data.\nUsing Preemptible VMs for the Dataproc cluster can also reduce the cost, as preemptible VMs are offered at a lower price than regular VMs. However, they are not suitable for all types of workloads as they can be preempted at any time, which could result in the loss of data or the need to restart the job.\nCustomizing the Dataproc cluster to store 60 TB per node may not be the most cost-effective option, as it could result in high storage costs that may not be necessary for all data. It\u2018s better to optimize the storage based on the data usage pattern and storage requirements.\nStoring all the data in a Cloud Storage bucket may be a viable option, but it depends on the workload and the access patterns. If the data needs to be accessed frequently, then using persistent disks may be a better option.\nReferences:\n1. Google Cloud. (n.d.). Optimizing Dataproc for cost and performance. Retrieved from\u00a0https:\/\/cloud.google.com\/architecture\/optimizing-dataproc-cost-performance\n2. Google Cloud. (n.d.). Moving data to Google Cloud Storage. Retrieved from\u00a0https:\/\/cloud.google.com\/storage\/docs\/migrating-to-cloud-storage"},{"label":"test_6","q_format":"single","q_text":"As a Data Engineer, you want to organize your BigQuery resources for a better billing understanding and want to group datasets by team: research team, analytics team, development team. What should you use to achieve this?","answers":[{"ans":"You should use labels. For example:\u00a0team:research,\u00a0team:analytics,\u00a0team:development.","val":true},{"ans":"You should create three different custom roles.","val":false},{"ans":"You should use labels. For example:\u00a0research,\u00a0analytics,\u00a0development.","val":false},{"ans":"You should create three different projects.","val":false}],"q_expl":"Labels are key-value pairs.\nLink to resources:\nhttps:\/\/cloud.google.com\/bigquery\/docs\/labels-intro"},{"label":"test_6","q_format":"single","q_text":"Your company needs to store audit logs of a project for at least 5 years. From time to time you would like to do some log analytics. What should you do in this case?","answers":[{"ans":"You should route audit logs to Pub\/Sub.","val":false},{"ans":"You should route audit logs to BigQuery.","val":true},{"ans":"You should route audit logs to Bigtable.","val":false},{"ans":"You should route audit logs to Cloud Storage.","val":false}],"q_expl":"The best option for storing audit logs for at least 5 years and performing log analytics is to route audit logs to BigQuery.\nBigQuery is a serverless, highly scalable, and cost-effective data warehouse that can store and analyze large volumes of data quickly. It is well-suited for storing and analyzing audit logs, and can easily handle data from various sources, including streaming data from Pub\/Sub. Additionally, BigQuery can be used to perform ad-hoc queries and generate reports, making it a great option for log analytics.\nRouting audit logs to Cloud Storage could also be a viable option, but it may not be the best option for log analytics as it does not provide query capabilities or real-time data analysis.\nRouting audit logs to Bigtable could be an option, but it is more suited for storing high-velocity and high-volume data with low latency requirements, and may not be as cost-effective for storing logs for a long duration.\nReferences:\n1. Google Cloud. (n.d.). BigQuery. Retrieved from\u00a0https:\/\/cloud.google.com\/bigquery\n2. Google Cloud. (n.d.). Choosing a storage option for logging. Retrieved from\u00a0https:\/\/cloud.google.com\/logging\/docs\/storage\n3. Google Cloud. (n.d.). Bigtable. Retrieved from\u00a0https:\/\/cloud.google.com\/bigtable"},{"label":"test_6","q_format":"single","q_text":"An agricultural company wants to deploy hundreds of thousands of new IoT devices to collect humidity data in its food warehouses around the world. You need to build solution to process, store and analyze these very large datasets in real time. What should you do?","answers":[{"ans":"You should send the data to Cloud Storage and then spin up a Hadoop cluster in Dataproc whenever analysis is required.","val":false},{"ans":"You should send the data to Pub\/Sub, stream Pub\/Sub to Dataflow, and store the data in BigQuery.","val":true},{"ans":"You should send the data to Datastore and then export to BigQuery.","val":false},{"ans":"You should send the data to Cloud Storage and then spin up a Cloud SQL instance whenever analysis is required.","val":false}],"q_expl":"The best option for processing, storing, and analyzing very large datasets in real-time from hundreds of thousands of new IoT devices collecting humidity data in food warehouses around the world is to send the data to Pub\/Sub, stream Pub\/Sub to Dataflow, and store the data in BigQuery. \nPub\/Sub is a messaging service that can handle large volumes of real-time data from thousands of devices simultaneously. Dataflow is a fully-managed stream processing service that can process data in real-time with Apache Beam to transform, enrich, and analyze the data. Data can be stored in BigQuery, a serverless, highly scalable, and cost-effective data warehouse that can store and analyze large volumes of data quickly. \nUsing this approach, you can process and store data in real-time, perform real-time analysis of the data, and generate insights from the data that can help improve operations and reduce waste. \nSending the data to Cloud Storage and spinning up a Hadoop cluster in Dataproc whenever analysis is required could also be an option, but it may not be the best option for real-time data processing and analysis. \nSending the data to Datastore and then exporting it to BigQuery could be an option, but it may not be the best option for real-time data processing and analysis, and may not be the most cost-effective option for storing large volumes of data. \nSending the data to Cloud Storage and then spinning up a Cloud SQL instance whenever analysis is required is not a suitable option for processing and analyzing very large datasets in real-time, as Cloud SQL is not designed for handling large volumes of data or real-time data processing. \nReferences: \n1. Google Cloud. (n.d.). Pub\/Sub. Retrieved from\u00a0https:\/\/cloud.google.com\/pubsub \n2. Google Cloud. (n.d.). Dataflow. Retrieved from\u00a0https:\/\/cloud.google.com\/dataflow \n3. Google Cloud. (n.d.). BigQuery. Retrieved from\u00a0https:\/\/cloud.google.com\/bigquery \n4. Google Cloud. (n.d.). Best practices for IoT on Google Cloud. Retrieved from\u00a0https:\/\/cloud.google.com\/solutions\/best-practices-for-iot-on-google-cloud"},{"label":"test_6","q_format":"single","q_text":"Everyone in your team has access to the datasets stored in BigQuery. You want to track administrative activities and access to BigQuery resources. Who did what, where, and when. What should you do?","answers":[{"ans":"You should use Cloud Monitoring to see the usage of BigQuery resources.","val":false},{"ans":"You should use the Cloud Billing API to see the usage of BigQuery resources.","val":false},{"ans":"You should set the identity and access management (IAM) policy of each resource (dataset, table).","val":false},{"ans":"You should use Cloud Audit Logs to review data access.","val":true}],"q_expl":"To track administrative activities and access to BigQuery resources, you should use Cloud Audit Logs. Cloud Audit Logs provides a record of administrative activity and data access for your Google Cloud resources, including BigQuery. It allows you to see who did what, where, and when, which is useful for auditing and compliance purposes.\nCloud Monitoring is focused on monitoring the performance and availability of your applications and infrastructure, so it may not provide the level of detail needed to track administrative activities and access to BigQuery resources.\nThe Cloud Billing API is used to retrieve billing data for your Google Cloud projects, so it is not directly relevant for tracking administrative activities and access to BigQuery resources.\nSetting the identity and access management (IAM) policy of each resource (dataset, table) is important to control who can access the data, but it does not provide a complete record of administrative activities and data access.\nReference:\nCloud Audit Logs documentation"},{"label":"test_6","q_format":"single","q_text":"A company needs to move 5 PB of data from on-premises data center do Google Cloud Storage as soon as possible. The network bandwidth is constrained to 40 Mbps. What should you advise them?","answers":[{"ans":"They should use Transfer Appliance to move data to the Cloud Storage.","val":true},{"ans":"This cannot be done, Cloud Storage has a limit of 4 PB.","val":false},{"ans":"They should use Storage Transfer Service to move data to the Cloud Storage.","val":false},{"ans":"They should use\u00a0gsutil\u00a0tool to move data to the Cloud Storage.","val":false}],"q_expl":"Given that the network bandwidth is constrained to 40 Mbps, it would take an impractically long time to move 5 PB of data to Google Cloud Storage using the network. In this case, it is recommended to use Transfer Appliance to move the data to the Cloud Storage.\nTransfer Appliance is a high-capacity storage server that you can use to securely transfer large amounts of data to Google Cloud. You can use Transfer Appliance to transfer data over the internet or by shipping the appliance to a Google data center. Once the data is transferred to Transfer Appliance, you can use the transfer appliance console to create a transfer job that will transfer the data from Transfer Appliance to Cloud Storage.\nThe other options, such as using Storage Transfer Service or gsutil, may not be practical given the network bandwidth limitation and the large volume of data to transfer.\nReference:\nTransfer Appliance documentation"},{"label":"test_6","q_format":"single","q_text":"Your application requires a filesystem that can be mounted using operating system commands. This filesystem should be accessible from multiple virtual machine instances. What GCP service should you recommend?","answers":[{"ans":"Cloud Filestore","val":true},{"ans":"Cloud Storage","val":false},{"ans":"Cloud Spanner","val":false}],"q_expl":"The GCP service that should be recommended in this scenario is Cloud Filestore. Cloud Filestore is a fully-managed file storage service for applications that require a filesystem interface and a shared file system for data. It allows you to create a high-performance file system that multiple virtual machine instances can mount, making it easy to share data between instances. It is accessible from Compute Engine instances and Kubernetes Engine clusters, and provides a traditional file system interface using the NFSv3 and SMB protocols.\nReference:\nGoogle Cloud Filestore Documentation:\u00a0https:\/\/cloud.google.com\/filestore"},{"label":"test_6","q_format":"single","q_text":"As a Data Engineer, you are responsible for designing system for an e-commerce company. The system will send a message to the customer if the customer doesn\u2018t interact with the website for half an hour after completing the basket. You want to use Dataflow to process data and decide if a message should be sent. How should you design this pipeline?","answers":[{"ans":"You should use a session window with a gap time duration of 30 minutes.","val":true},{"ans":"You should use a global window with a time based trigger with a delay of 30 minutes.","val":false},{"ans":"You should use a sliding time window with a duration of 30 minutes.","val":false},{"ans":"You should use a fixed-time window with a duration of 30 minutes.","val":false}],"q_expl":"The correct answer is:\nYou should use a session window with a gap time duration of 30 minutes.\nExplanation:\nIn this scenario, the Dataflow pipeline needs to identify customers who have completed their basket but haven\u2018t interacted with the website for 30 minutes. This requires tracking user activity over time and identifying when a session has ended.\nA session window in Dataflow is appropriate for this use case as it groups events that occur within a certain time frame of user activity. The session window has two parameters, the session gap time and the allowed lateness. The session gap time determines how long to wait for the next event to occur before the current session is considered over. In this case, a gap time duration of 30 minutes would be appropriate.\nReference:\nGoogle Cloud documentation:\u00a0Session windowing in Dataflow"},{"label":"test_6","q_format":"single","q_text":"In your data center you want to deploy an application. This application will use GCP services like AutoML. For this, you created a service account that has appropriate access to AutoML. You need to enable authentication to the APIs from your private environment. What should you do?","answers":[{"ans":"You should go to the IAM & Admin console, grant a user account permissions similar to the service account permissions, and use this user account for authentication from your data center.","val":false},{"ans":"You should use\u00a0gcloud\u00a0to create a key file for the service account that has appropriate permissions.","val":true},{"ans":"You should set up direct interconnect between your data center and GCP to enable authentication for your on-premises applications.","val":false},{"ans":"You should use service account credentials in your on-premises application.","val":false}],"q_expl":"The most suitable option for enabling authentication to GCP APIs from your private environment with a service account is:\nB. You should use gcloud to create a key file for the service account that has appropriate permissions.\nHere\u2019s why this option is the best choice:\n\nSecurity:\u00a0Using a service account key file avoids exposing sensitive service account credentials directly in your application code, enhancing security.\nConvenience:\u00a0The\u00a0gcloud\u00a0command-line tool provides a straightforward way to create and manage service account key files.\nControl:\u00a0You can grant specific permissions to the service account key file, limiting its access to only the required GCP services (like AutoML in this case).\n\nOther options are not as effective:\n\nA. Granting permissions to a user account:\u00a0This approach exposes user credentials and is generally less secure than using a service account.\nC. Setting up direct interconnect:\u00a0While offering improved performance and security, it\u2019s an overkill for authentication purposes and involves additional infrastructure setup.\nD. Using service account credentials directly:\u00a0Exposing service account credentials in the application code is not recommended due to security risks.\n\nTherefore, creating a service account key file with gcloud offers a secure, convenient, and controlled approach for enabling authentication to GCP APIs from your private environment.\nReference:\nhttps:\/\/cloud.google.com\/vision\/automl\/docs\/before-you-begin\nhttps:\/\/cloud.google.com\/iam\/docs\/keys-create-delete#creating"},{"label":"test_6","q_format":"single","q_text":"A social media company stores images in a Cloud Storage bucket for long term. Images older than 30 days are accessed only in exceptional circumstances, and images older than a year are no longer needed. How can you optimize lifecycle management policy to reduce costs?","answers":[{"ans":"You should configure a lifecycle management policy to transition objects older than 30 days to Coldline storage class. Than, configure another lifecycle management policy to delete objects older than 335 days.","val":false},{"ans":"You should configure a lifecycle management policy to transition objects older than 30 days to Archive storage class. Than, configure another lifecycle management policy to delete objects older than 365 days.","val":true},{"ans":"You should configure a lifecycle management policy to transition objects older than 30 days to Coldline storage class. Than, configure another lifecycle management policy to delete objects older than 365 days.","val":false},{"ans":"You should use a Cloud Function to rewrite the storage class to Coldline for objects older than 30 days. And use another Cloud Function to delete objects older than 365 days from Coldline Storage Class.","val":false}],"q_expl":"Option B suggests configuring a lifecycle management policy to transition objects older than 30 days to the Archive storage class and then setting up another policy to delete objects older than 365 days. \nThis approach optimizes cost by moving less frequently accessed data to a lower-cost storage class like Archive after 30 days, reducing immediate storage costs. Subsequently, it ensures that data no longer needed after a year is deleted, preventing unnecessary storage costs.Now, let\u2019s explain why the other options are incorrect:\n\nOption A: Transitioning objects older than 30 days to Coldline storage class and deleting objects older than 335 days may not be the most cost-effective solution. Coldline storage is typically more expensive than Archive storage, which is better suited for long-term retention of infrequently accessed data.\nOption C: Transitioning objects older than 30 days to Coldline storage class and deleting objects older than 365 days could lead to higher costs compared to using Archive storage for long-term retention. Coldline storage is designed for data that is accessed less frequently but still requires quick access when needed.\nOption D: Using Cloud Functions to rewrite the storage class to Coldline for objects older than 30 days and then deleting objects older than 365 days from Coldline Storage Class introduces unnecessary complexity. Lifecycle management policies within the cloud storage service itself are more efficient and cost-effective for managing data lifecycle transitions and deletions.\n\nReferences:\nhttps:\/\/www.trendmicro.com\/cloudoneconformity-staging\/knowledge-base\/gcp\/CloudStorage\/enable-lifecycle-management.html\nhttps:\/\/learn.microsoft.com\/en-us\/azure\/storage\/blobs\/lifecycle-management-overview\nhttps:\/\/learn.microsoft.com\/en-us\/azure\/storage\/blobs\/lifecycle-management-policy-configure?tabs=azure-portal\nhttps:\/\/reintech.io\/blog\/optimizing-costs-azure-blob-storage-lifecycle-management\nhttps:\/\/www.linkedin.com\/pulse\/optimizing-cloud-storage-costs-strategies-efficient-data-lisa-parker-a967c?trk=article-ssr-frontend-pulse_more-articles_related-content-card"},{"label":"test_6","q_format":"single","q_text":"A manufacturing company collects IoT data from hundreds of thousands of devices every 5 seconds. They want to analyze this IoT data for an outlier time series analysis. What should you advise them?","answers":[{"ans":"They should store the data in Bigtable and use the\u00a0cbt\u00a0tool to display outliers based on business requirements.","val":false},{"ans":"They should store the data in Cloud Storage and use the Bigtable\u00a0cbt\u00a0tool to display outliers based on business requirements.","val":false},{"ans":"They should store the data in BigQuery and use SQL queries to display outliers based on business requirements.","val":false},{"ans":"They should store the data in BigQuery and use BigQuery ML to display outliers based on business requirements.","val":true}],"q_expl":"The most effective approach for analyzing IoT data for outlier time series analysis would be:\nThey should store the data in BigQuery and use BigQuery ML to display outliers based on business requirements.\nHere\u2019s why:\n\nScalability: BigQuery is designed to handle massive datasets, making it ideal for storing and analyzing the large volume of IoT data generated every 5 seconds.\nTime Series Analysis: BigQuery ML offers built-in functions and algorithms specifically designed for time series analysis, including outlier detection.\nSQL Queries and Machine Learning: BigQuery\u2019s SQL interface allows for flexible data exploration and querying, while BigQuery ML provides powerful machine learning capabilities for building and deploying models.\nManaged Service: BigQuery is a fully managed service, eliminating the need for infrastructure management and maintenance.\n\nWhile storing data in Bigtable or Cloud Storage could be considered for specific use cases, BigQuery\u2019s combination of scalability, time series analysis capabilities, and machine learning integration makes it the most suitable choice for this scenario."},{"label":"test_6","q_format":"single","q_text":"You store BigQuery data in external CSV files in Cloud Storage. As the amount of data has increased, the query performance has dropped. What should you do to maintain query performance?","answers":[{"ans":"You should move data to Cloud Bigtable for faster access.","val":false},{"ans":"You should divide the data into partitions.","val":false},{"ans":"You should request more slots for greater capacity to improve performance.","val":false},{"ans":"You should import the data into BigQuery for better performance.","val":true}],"q_expl":"Importing the data into BigQuery is the most effective way to maintain query performance in this scenario.\nHere\u2019s why:\n\nOptimized Storage and Query Engine: BigQuery is specifically designed for large-scale data analytics and is highly optimized for querying large datasets. It leverages columnar storage and a distributed query engine to efficiently process complex queries.\nData Compression and Caching: BigQuery automatically compresses data and utilizes caching mechanisms to improve query performance.\nMaterialized Views: You can create materialized views on frequently queried tables to further optimize query performance.\nPartitioning: While partitioning can be beneficial for certain use cases, it\u2019s not always the most effective solution for improving query performance. It can be more complex to manage and may not provide significant performance gains in all scenarios.\n\nBy importing your CSV data into BigQuery, you can leverage its powerful features to significantly improve query performance and reduce query execution time.\nReference:\nhttps:\/\/cloud.google.com\/bigquery\/docs\/partitioned-tables"},{"label":"test_6","q_format":"single","q_text":"As a Data Engineer, you are responsible for monitoring all changes in Cloud Storage bucket. For each change, you must invoke an action that will verify the compliance of the change in near real-time. What should you do?","answers":[{"ans":"You should use Crone Scheduler to schedule your security script.","val":false},{"ans":"You should use the built-in triggering mechanism of Cloud Storage to run your security script.","val":false},{"ans":"You should use Cloud Function events, and call your security script from the Cloud Function triggers.","val":true},{"ans":"You should use a Python script to get appropriate logs, analyze them, and run the security script.","val":false}],"q_expl":"Using Cloud Function is correct because it provides fast response and requires the minimal amount of setup.\nLink to resources:\nhttps:\/\/cloud.google.com\/functions\/docs\/how-to"},{"label":"test_6","q_format":"single","q_text":"A data science team wants to analyze very large data sets and prepares them for a machine learning model. As a Cloud Architect, which service should you recommend to use for interactive queries and online analytics?","answers":[{"ans":"Cloud Storage","val":false},{"ans":"Cloud Spanner","val":false},{"ans":"Cloud SQL","val":false},{"ans":"BigQuery","val":true},{"ans":"Cloud Datastore","val":false},{"ans":"Cloud Bigtable","val":false}],"q_expl":"BigQuery\u00a0is Google Cloud\u2018s fully managed, petabyte-scale, and cost-effective analytics data warehouse that lets you run analytics over vast amounts of data in near real time. With BigQuery, there\u2018s no infrastructure to set up or manage, letting you focus on finding meaningful insights using standard SQL and taking advantage of flexible pricing models across on-demand and flat-rate options.\nhttps:\/\/cloud.google.com\/bigquery\/docs"},{"label":"test_6","q_format":"single","q_text":"An application runs on your on-premises datacenter and uses MySQL as a database engine. You want to migrate your database to the Google Cloud. Which storage service should you use?","answers":[{"ans":"Cloud Spanner","val":false},{"ans":"Cloud SQL","val":true},{"ans":"Cloud Storage","val":false},{"ans":"Cloud Bigtable","val":false}],"q_expl":"Cloud SQL\u00a0is a fully-managed database service that helps you set up, maintain, manage, and administer your relational databases on Google Cloud Platform. You can use Cloud SQL with MySQL, PostgreSQL, or SQL Server.\nhttps:\/\/cloud.google.com\/sql\/docs\/mysql"},{"label":"test_6","q_format":"single","q_text":"An online marketing company wants to migrate a local data warehouse to GCP. They want to use a managed cloud solution. As a Cloud Architect, you advise them which service they should use. What do you recommend?","answers":[{"ans":"BigQuery","val":true},{"ans":"Cloud Bigtable","val":false},{"ans":"Compute Engine","val":false},{"ans":"Cloud Dataproc","val":false},{"ans":"Cloud Storage","val":false}],"q_expl":"BigQuery\u00a0is Google Cloud\u2018s fully managed, petabyte-scale, and cost-effective analytics data warehouse that lets you run analytics over vast amounts of data in near real time. With BigQuery, there\u2018s no infrastructure to set up or manage, letting you focus on finding meaningful insights using standard SQL and taking advantage of flexible pricing models across on-demand and flat-rate options.\nCompute Engine isn\u2018t a managed service. Dataproc is a managed Hadoop and Spark service. Bigtable is a NoSQL database for low-latency writes and limited ranges of queries. Cloud Storage isn\u2018t a good choice for data warehouse (object storage).\nhttps:\/\/cloud.google.com\/bigquery\/docs\/introduction"},{"label":"test_6","q_format":"single","q_text":"A company wants to migrate Hadoop cluster to the Google Cloud. They want to minimize the management of the cluster as much as possible and reuse Hadoop jobs they have already created. They also want to be able to persist data beyond the life of the cluster. What should you advise them?","answers":[{"ans":"They should create a Dataproc cluster that uses the Cloud Storage connector.","val":true},{"ans":"They should create a Hadoop cluster on Compute Engine that uses Local SSD disks.","val":false},{"ans":"They should create a Dataflow job to process the data.","val":false},{"ans":"They should create a Dataproc cluster that uses persistent disks for HDFS.","val":false}],"q_expl":"Ref:\u00a0https:\/\/cloud.google.com\/dataproc\/docs\/concepts\/connectors\/cloud-storage"},{"label":"test_6","q_format":"single","q_text":"What is the purpose of using Cloud SQL in Google Cloud?","answers":[{"ans":"a. To store structured data","val":true},{"ans":"b. To store unstructured data","val":false},{"ans":"c. To store data in the cloud for backup purposes only","val":false},{"ans":"d. To store images and videos","val":false}],"q_expl":"Cloud SQL is a fully-managed relational database service for storing structured data in the cloud."},{"label":"test_6","q_format":"single","q_text":"Which of the following is not a key feature of BigQuery in Google Cloud?","answers":[{"ans":"a. Serverless","val":false},{"ans":"b. Petabyte scale","val":false},{"ans":"c. In-memory processing","val":false},{"ans":"d. FTP support","val":true}],"q_expl":"BigQuery is a fully-managed, serverless, and cloud-native data warehousing solution that enables super-fast SQL queries using the processing power of Google\u2018s infrastructure. It does not support the File Transfer Protocol (FTP)."},{"label":"test_6","q_format":"single","q_text":"What is the main use of Google Cloud Dataflow?","answers":[{"ans":"a. Batch data processing","val":false},{"ans":"b. Real-time data processing","val":false},{"ans":"c. Both batch and real-time data processing","val":true},{"ans":"d. None of the above","val":false}],"q_expl":"Google Cloud Dataflow is a fully managed service for transforming and enriching data in both batch and real-time pipelines.\nJ\u2018esp\u00e8re que cela vous aide ! Si vous avez besoin de plus de questions ou d\u2018autres informations, n\u2018h\u00e9sitez pas \u00e0 me le demander."},{"label":"test_6","q_format":"single","q_text":"What is the main advantage of using Google Cloud Storage?","answers":[{"ans":"a. Low cost","val":false},{"ans":"b. High performance","val":false},{"ans":"c. High durability","val":false},{"ans":"d. All of the above","val":true}],"q_expl":"Google Cloud Storage is a fully managed object storage service that allows you to store and retrieve large amounts of data at low cost, with high performance and high durability."},{"label":"test_6","q_format":"single","q_text":"A web application generates thumbnails based on the uploaded photo by the user. The frontend application uploads photos to Cloud Storage. The backend is quite obsolete and runs a cron job that checks Cloud Storage buckets every 15 minutes for new photos. You want to improve this application and process the photos as soon as possible. Which Google Cloud service should you use?","answers":[{"ans":"A cron job that checks the bucket more often.","val":false},{"ans":"App Engine Flexible","val":false},{"ans":"Kubernetes pod","val":false},{"ans":"Cloud Function","val":true}],"q_expl":"App Engine Flexible application and Kubernetes pod cannot respond to a Cloud Storage events.\nLink to resources:\nhttps:\/\/cloud.google.com\/functions\/docs\/how-to"},{"label":"test_6","q_format":"single","q_text":"You want to have durable storage in a Kubernetes cluster. Pods are ephemeral, so they may be deleted or recreated. You want to decouple pods from persistent storage. What Kubernetes mechanism should you use?","answers":[{"ans":"PersistentVolumes","val":true},{"ans":"DeamonSet","val":false},{"ans":"ReplicaSets","val":false},{"ans":"Deployments","val":false},{"ans":"StatefulSets","val":false}],"q_expl":"To have durable storage in a Kubernetes cluster and decouple pods from persistent storage, the recommended Kubernetes mechanism is PersistentVolumes.\nPersistentVolumes (PVs) are a Kubernetes resource that provides an abstraction layer for the underlying storage infrastructure. PVs decouple storage from the pod, allowing persistent data to exist beyond the lifetime of a pod. PVs also enable users to provision, manage, and consume storage resources in a cluster-independent way. When a pod needs to access storage, it can request a PersistentVolumeClaim (PVC), which is a request for a specific size and access mode of storage. The PVC binds to an available PV, which then provides the persistent storage for the pod.\nDaemonSet, ReplicaSets, Deployments, and StatefulSets are other Kubernetes mechanisms that provide different functionalities for managing pods and workloads in a Kubernetes cluster. DaemonSets ensure that all or some nodes run a copy of a pod, while ReplicaSets, Deployments, and StatefulSets ensure that a specified number of replicas of a pod or set of pods are running at any given time.\nTherefore, the correct answer is A) PersistentVolumes."},{"label":"test_6","q_format":"multiple","q_text":"Which of the following bucket names are invalid? (select 3)","answers":[{"ans":"HadoopDataStorage425","val":true},{"ans":"162.168.10.10","val":true},{"ans":"google-images-backup","val":true},{"ans":"hadoop-data-storage-425","val":false},{"ans":"hadoop_data_storage_425","val":false}],"q_expl":"Bucket names can only contain lowercase letters, numeric characters, dashes (-), underscores (_), and dots (.). Spaces are not allowed. Bucket names cannot be represented as an IP address in dotted-decimal notation. Bucket names cannot begin with the \u201cgoog\u201c prefix. Bucket names cannot contain \u201cgoogle\u201c or close misspellings, such as \u201cg00gle\u201c.\nLink to resources:\nhttps:\/\/cloud.google.com\/storage\/docs\/naming-buckets#requirements"},{"label":"test_6","q_format":"single","q_text":"Select the correct pricing model for Compute Engine.","answers":[{"ans":"Per-day billing, with minimum of 1 week.","val":false},{"ans":"Per-hour billing, with minimum of 1 day.","val":false},{"ans":"Per-minute billing, with minimum of 1 hour.","val":false},{"ans":"Per-second billing, with minimum of 1 minute.","val":true}],"q_expl":"The correct pricing model for Compute Engine is per-second billing, with a minimum of 1 minute.\nCompute Engine is a part of the Google Cloud Platform that offers virtual machines (VMs) that can be customized to meet specific needs. It provides flexible and scalable computing power that can be used for a variety of purposes, including running applications, hosting websites, and processing large data sets.\nCompute Engine charges users based on the amount of resources used, with prices calculated in seconds, and rounded up to the nearest minute. This means that users only pay for the resources they use, and they can stop or start VMs as needed without incurring additional costs. Additionally, Compute Engine offers sustained use discounts, which lower the price per hour for VMs that are used for extended periods.\nTherefore, the correct answer is D) Per-second billing, with a minimum of 1 minute."},{"label":"test_6","q_format":"single","q_text":"You work for a media company. You\u2018ve got a lot of audio files from multiple interviews. You want to convert these audio files to text. Which Machine Learning API should you use to convert speech to text?","answers":[{"ans":"Natural Language API","val":false},{"ans":"Text-to-Speech API","val":false},{"ans":"Speech-to-Text API","val":true},{"ans":"Translation API","val":false}],"q_expl":"The Machine Learning API that you should use to convert speech to text is the Speech-to-Text API.\nThe Speech-to-Text API is a Google Cloud service that uses machine learning technology to convert spoken words into written text. It supports multiple languages and dialects and can recognize various types of audio, including phone calls, videos, and other recordings.\nTo use the Speech-to-Text API, you can upload an audio file to Google Cloud Storage and then use the API to transcribe the speech into text. The API also supports real-time streaming transcription for use cases such as live captions.\nTherefore, the correct answer is C) Speech-to-Text API."},{"label":"test_6","q_format":"single","q_text":"As a Mobile Game Developer you want to launch a new mobile game that will be available to users around the world. Your game requires RDBMS for storing player profiles. Which storage service should you use to be able to scale to a global audience with minimal configuration updates?","answers":[{"ans":"Cloud Spanner","val":true},{"ans":"Cloud Datastore","val":false},{"ans":"Cloud SQL","val":false},{"ans":"Cloud Firestore","val":false}],"q_expl":"Ref:\u00a0https:\/\/cloud.google.com\/spanner\/docs\/quickstart-console"},{"label":"test_6","q_format":"single","q_text":"Select the application that is most suitable for running in a Preemptible VM.","answers":[{"ans":"A mobile application.","val":false},{"ans":"A web application.","val":false},{"ans":"An analytical batch job that cannot be checkpointed and restarted.","val":false},{"ans":"A relational database.","val":false},{"ans":"An analytical batch job that can be checkpointed and restarted (for example, training a neural network).","val":true}],"q_expl":"The application that is most suitable for running in a Preemptible VM is an analytical batch job that can be checkpointed and restarted (for example, training a neural network).\nPreemptible VMs are short-lived instances that are available at a lower cost compared to regular VMs. However, they can be terminated at any time with a 30-second notice, so they are not suitable for applications that cannot be interrupted.\nAnalytical batch jobs that can be checkpointed and restarted are good candidates for Preemptible VMs because they can be easily restarted from the last checkpoint if the VM is terminated. This makes Preemptible VMs a cost-effective option for running long-running jobs, such as training a neural network, where interruptions can be handled gracefully.\nTherefore, the correct answer is E) An analytical batch job that can be checkpointed and restarted (for example, training a neural network)."},{"label":"test_6","q_format":"single","q_text":"The accuracy of your machine learning model when testing with validation data returns 100% accuracy. What can you say about this model?","answers":[{"ans":"This model is ready for production use.","val":false},{"ans":"This model is underfitted.","val":false},{"ans":"This model is a perfect fit.","val":false},{"ans":"This model is overfitted.","val":true}],"q_expl":"If the accuracy of a machine learning model when testing with validation data returns 100% accuracy, it is likely that the model is overfitted.\nOverfitting occurs when a model is too complex and fits the training data too closely, such that it begins to memorize the data instead of generalizing patterns from it. This can result in a high accuracy score on the training data, but poor performance on new, unseen data. When the model\u2018s performance on the validation data is also perfect, it is a sign that the model is likely overfitted and not able to generalize well.\nTherefore, the correct answer is D) This model is overfitted. Further testing on new, unseen data should be performed to confirm the model\u2018s generalizability and make any necessary adjustments to reduce overfitting."},{"label":"test_6","q_format":"single","q_text":"A data scientist team is looking for cost-effective way to run non-critical Apache Spark jobs on Dataproc. What should you advise them?","answers":[{"ans":"They should set up a cluster in standard mode with default machine types and add a few additional local SSDs.","val":false},{"ans":"They should set up a cluster in high availability mode with default machine types and add a few additional Preemptible worker nodes.","val":false},{"ans":"They should set up a cluster in high availability mode with high-memory machine types and add a few additional local SSDs.","val":false},{"ans":"They should set up a cluster in standard mode with high-memory machine types and add a few additional Preemptible worker nodes.","val":true}],"q_expl":"They should set up a cluster in standard mode with default machine types and add a few additional Preemptible worker nodes.\nThis option is the most cost-effective way to run non-critical Apache Spark jobs on Dataproc. Preemptible VMs can be used to save cost since they are up to 80% cheaper than regular VMs. The default machine types are also a cost-effective option for non-critical jobs. The standard mode provides basic cluster functionality, while allowing for the flexibility of resizing the cluster.\nReference:\nhttps:\/\/cloud.google.com\/dataproc\/docs\/concepts\/configuring-clusters\/compute-options#machine_types\nhttps:\/\/cloud.google.com\/dataproc\/docs\/concepts\/compute\/preemptible-vms"},{"label":"test_6","q_format":"single","q_text":"A company wants to migrate on-premises data center to Google Cloud. As a Data Engineer, you need to estimate monthly expenses that this company needs to run all their infrastructure in GCP. How can you calculate your expenses?","answers":[{"ans":"You should capture the pricing from the products pricing page and and calculate monthly expenses.","val":false},{"ans":"You should migrate all applications to GCP and run them for a day. Then based on that, calculate monthly expenses.","val":false},{"ans":"You should migrate all applications to GCP and run them for a week. Then based on that, calculate monthly expenses.","val":false},{"ans":"You should use the Google Cloud Pricing Calculator to estimate the monthly expenses.","val":true}],"q_expl":"You should use the Google Cloud Pricing Calculator to estimate the monthly expenses. The pricing of Google Cloud services is dynamic, and depends on several factors such as usage, location, and type of services used. The Google Cloud Pricing Calculator allows you to estimate the cost of using different Google Cloud services based on your specific requirements and usage patterns. By providing details on the resources that you need, you can get an accurate estimate of the monthly expenses that your company would incur when running all their infrastructure in GCP."},{"label":"test_6","q_format":"single","q_text":"Your team wants to integrate internal company application and BigQuery, so employees can make queries directly from the application interface. You need to securely access BigQuery from your application and don\u2018t want individual employees to authenticate to BigQuery. You also don\u2018t want to give them access to the dataset. What should you recommend?","answers":[{"ans":"You should create groups for your employees and give those groups access to the dataset.","val":false},{"ans":"You should create a dummy employee and grant dataset access to that employee and use those credentials to access the BigQuery dataset.","val":false},{"ans":"You should create a service account and grant dataset access to that account. Use the service account\u2018s private key to access the dataset.","val":true},{"ans":"You should implement your onw sign-on mechanism.","val":false}],"q_expl":"You should create a service account and grant dataset access to that account. Use the service account\u2018s private key to access the dataset. This approach provides a secure and convenient way to access BigQuery from the application interface without giving individual employees direct access to the dataset. The service account is a special type of Google account that is used by applications or virtual machines (VMs) to access Google Cloud resources. By using the service account\u2018s private key, the application can authenticate itself to BigQuery without requiring individual employee authentication. This ensures that only authorized applications can access the dataset.\nReference:\nAuthenticating as a service account"},{"label":"test_6","q_format":"single","q_text":"Suppose you work for a telecommunications company (telecom). Your company collects a lot of data about customers and stores it in BigQuery. You want to use this data and machine learning to predict customer churn rates (whether or not the customer will opt out). Which BigQuery ML model should you use for this?","answers":[{"ans":"Binary logistic regression","val":true},{"ans":"Multiclass logistic regression","val":false},{"ans":"Linear regression","val":false},{"ans":"K-means clustering","val":false}],"q_expl":"For predicting customer churn rates, the suitable BigQuery ML model is binary logistic regression, as it is a binary classification problem where the target variable has only two outcomes (customer churn or not). Binary logistic regression is a statistical method that uses a logistic function to model a binary dependent variable, which is a dichotomy, as the outcome of a linear combination of predictor variables. By training the model on historical customer data, the telecom company can make predictions on whether a customer is likely to churn or not based on various features, such as their usage patterns, demographics, and interactions with the company.\nhttps:\/\/cloud.google.com\/bigquery-ml\/docs\/introduction"},{"label":"test_6","q_format":"single","q_text":"Personally identifiable information (PII) and sensitive information about your company\u2018s customers should be stored securely in Cloud Storage. Several people from your company\u2018s compliance department need access to some of this information. What should you do to follow Google\u2018s best practices?","answers":[{"ans":"You should grant Storage Object Viewer role to your co-workers.","val":false},{"ans":"You should use granular ACLs on the bucket.","val":true},{"ans":"You should grant Storage Object Creator role to your co-workers.","val":false},{"ans":"You should create additional bucket, enable public access, and provide specific file URLs to your co-workers.","val":false}],"q_expl":"To follow Google\u2018s best practices, it\u2018s recommended to use granular access control on the bucket and grant specific permissions to each user based on their need-to-know basis. This can be done by creating a custom role and assigning it to the users who need access to the PII and sensitive information. The custom role should be designed to restrict access to only the necessary resources and operations, minimizing the risk of unauthorized access.\nGranting the Storage Object Viewer role to co-workers is not recommended as it gives them read-only access to all objects in the bucket, including the PII and sensitive information. Granting the Storage Object Creator role is also not recommended as it gives users the ability to create, modify and delete objects in the bucket, which could result in data loss or corruption.\nCreating an additional bucket with public access is not recommended as it could lead to accidental exposure of sensitive information, and it also violates best practices for data security and privacy.\nReferences:\nGoogle Cloud Storage Best Practices:\u00a0https:\/\/cloud.google.com\/storage\/docs\/best-practices"},{"label":"test_6","q_format":"single","q_text":"You work for an e-commerce company in the apparel industry and you are responsible for maintaining a machine learning model for the recommendation engine. Customer\u2018s preference is likely to change over time, and you are constantly collecting new data about customer\u2018s preference. How should you use this new data?","answers":[{"ans":"You should continuously retrain the model only on new data.","val":false},{"ans":"You should train the model on the existing data and use the new data as your test set.","val":false},{"ans":"You should train the model on the new data and use the existing data as your test set.","val":false},{"ans":"You should continuously retrain the model on a combination of existing data and the new data.","val":true}],"q_expl":"To keep the recommendation engine up to date with the latest customer preferences, it\u2018s recommended to continuously retrain the model on a combination of existing data and the new data. This approach is called incremental learning or online learning, where the model is trained on new data as it arrives, allowing the model to adapt to changes over time. By doing so, the model can capture the latest trends and changes in customer preferences.\nAdditionally, it\u2018s important to monitor the performance of the model over time and evaluate it against new data to ensure that the model\u2018s predictions remain accurate and relevant. This can be achieved by regularly testing the model against a holdout dataset, which should be a representative sample of the latest customer data. If the performance of the model starts to decline, it may be time to re-evaluate the model architecture or training approach.\nReference:\nhttps:\/\/cloud.google.com\/solutions\/machine-learning\/recommendation-system-tensorflow-overview\nhttps:\/\/towardsdatascience.com\/machine-learning-101-incremental-learning-or-online-learning-8a3a26f1a3f6"},{"label":"test_6","q_format":"single","q_text":"As a Data Engineer, you have the following three options for deploying MySQL database:1. Use Cloud SQL.2. Use Cloud Marketplace with click-to-deploy interface to install MySQL onto a Compute Engine instance.3. Manually install and customize MySQL on your Compute Engine instance.You want to minimize administrative duties as much as possible. Which option should you use?","answers":[{"ans":"All options have the same administrative duties.","val":false},{"ans":"2","val":false},{"ans":"1","val":true},{"ans":"3","val":false}],"q_expl":"Option 1, using Cloud SQL, would be the best choice to minimize administrative duties. With Cloud SQL, Google Cloud manages the database infrastructure, such as backups, replication, and security patches, and it provides a highly available and scalable database solution. This eliminates most of the administrative tasks that would be required with options 2 and 3."},{"label":"test_6","q_format":"single","q_text":"When you create a Cloud Spanner instance, you specify its compute capacity as a number of processing units or as a number of nodes (1000 processing units is equal to 1 node). You want to deploy a Cloud Spanner database that can have up to 10 TB of data. What is the minimum number of nodes or processing units required?","answers":[{"ans":"5 nodes","val":false},{"ans":"1 node","val":false},{"ans":"2000 processing units","val":true},{"ans":"10000 processing units","val":false}],"q_expl":"To deploy a Cloud Spanner database that can have up to 10 TB of data, the recommended minimum number of nodes or processing units is 2000 processing units. This is because the number of processing units determines the amount of CPU and memory allocated to the instance, and the amount of CPU and memory directly affects the performance and scalability of the database. In general, it is recommended to start with at least 2000 processing units and adjust the capacity as needed based on the size of the database and workload requirements.\nReference:\u00a0Cloud Spanner Pricing"},{"label":"test_6","q_format":"multiple","q_text":"As a Data Engineer, you need to know how to work with different storage and database options. Select all BIgtable best practices. (select 2)","answers":[{"ans":"When a row contains multiple values that are related to one another, it\u2018s a good practice to group columns that contain those values in the same column family.","val":true},{"ans":"You should choose short and descriptive names for your column families.","val":true},{"ans":"You should use row keys that start with a timestamp.","val":false},{"ans":"Whenever possible, you should use sequential numeric identifiers as the row key for the table.","val":false}],"q_expl":"Row keys that start with a timestamp will cause sequential writes to be pushed onto a single node, creating a hotspot. If you put a timestamp in a row key, you need to precede it with a high-cardinality value like a user ID to avoid hotspotting.\nSequential numeric identifiers. Suppose your system assigns a numeric ID to each of your application\u2018s users. You might be tempted to use the user\u2018s numeric identifier as the row key for your table. However, because new users are more likely to be active users, this approach is likely to push most of your traffic to a small number of nodes."},{"label":"test_6","q_format":"single","q_text":"You work for a healthcare company. A lot of physical documents must be transferred to the system. You want to develop an automated solution using Google Cloud. Which Machine Learning API should you use to extract text from document images?","answers":[{"ans":"Cloud Text-to-Speech API","val":false},{"ans":"Cloud Vision API","val":true},{"ans":"Cloud Natural Language API","val":false},{"ans":"Cloud Speech-to-Text API","val":false}],"q_expl":"The Machine Learning API that you should use to extract text from document images is Cloud Vision API. Cloud Vision API provides powerful image analysis capabilities, including Optical Character Recognition (OCR) technology, which can extract text from images of scanned documents, receipts, and other types of images."},{"label":"test_6","q_format":"single","q_text":"You work for an e-commerce company. There are many customer reviews in your data warehouse. You want to analyze customer sentiment for the last year. Which Google Cloud API can you use to detect sentiment?","answers":[{"ans":"Text-to-Speech API","val":false},{"ans":"Recommendations AI API","val":false},{"ans":"Natural Language API","val":true},{"ans":"Translation API","val":false}],"q_expl":"You can use the Natural Language API to detect sentiment of customer reviews."},{"label":"test_7","q_format":"single","q_text":"As a Data Engineer you need to review a Cloud Spanner database design. Database users complain about poor query performance. You review the queries and notice that they all involve joining two or more tables that are related hierarchically. Can you improve this design? What should you recommend?","answers":[{"ans":"They shouldn\u2018t change anything.","val":false},{"ans":"They should use secondary indexes to support queries.","val":false},{"ans":"They should use interleaved tables to optimize query performance.","val":true},{"ans":"They should use read-only replica to execute queries.","val":false}],"q_expl":"If the queries involve joining two or more tables that are related hierarchically, using interleaved tables could optimize query performance in Cloud Spanner. Interleaved tables can improve performance when there is a hierarchical relationship between tables because they group rows from child tables with their parent rows. This way, when the parent row is accessed, all of its related child rows are accessed as well, reducing the number of reads needed to satisfy a query. Using secondary indexes may also help, but interleaved tables are specifically designed for hierarchical relationships and are likely to provide the best performance gains in this scenario. Therefore, I would recommend using interleaved tables to optimize query performance."},{"label":"test_7","q_format":"single","q_text":"You work for a mobile game developer and are responsible for designing a new in-game item trading system database. A user may purchase an item from the catalog or from other user. Users may have different items and can search for other users who have particular item. Future versions of the game may also contain new items. You want to use flexible schema data model. What storage solution would you recommend?","answers":[{"ans":"Wide-column database such as Bigtable","val":false},{"ans":"OLAP solution such as BigQuery.","val":false},{"ans":"OLTP solution such as MySQL or PostgreSQL .","val":false},{"ans":"Document database such as Datastore or MongoDB.","val":true}],"q_expl":"For a flexible schema data model for the in-game item trading system database, a document database such as Datastore or MongoDB would be a good choice. Document databases allow for flexible data models that can adapt to changing business requirements and schema changes. In this case, as future versions of the game may contain new items, a flexible schema is essential. Document databases are also well-suited for handling nested and hierarchical data structures, which may be important in a trading system where users may have different items and can search for other users who have particular items."},{"label":"test_7","q_format":"multiple","q_text":"A mobile gaming company uses BigQuery to upload log files from mobile games. Recently, you discovered that some queries that cover long date ranges run much slower. You plan to use partitioned tables. What can you use to partition a BigQuery table? (select 3)String columnTime-unit column (TIMESTAMP, DATE, DATETIME)Ingestion time (when BigQuery ingest the data)Array columnExplicationsLink to resources:https:\/\/cloud.google.com\/bigquery\/docs\/partitioned-tables","answers":[{"ans":"String column","val":true},{"ans":"Time-unit column (TIMESTAMP, DATE, DATETIME)","val":true},{"ans":"Ingestion time (when BigQuery ingest the data)","val":true},{"ans":"Array column","val":false}],"q_expl":"Yes, you can partition a BigQuery table using a string column, a time-unit column (such as TIMESTAMP, DATE, or DATETIME), and ingestion time (when BigQuery ingests the data). However, you cannot partition a table using an array column.\nLink to resources:\nhttps:\/\/cloud.google.com\/bigquery\/docs\/partitioned-tables"},{"label":"test_7","q_format":"multiple","q_text":"Select all true statements about selecting a row key in Bigtable. (select 2)","answers":[{"ans":"Timestamps are good candidates for a row key.","val":false},{"ans":"Frequently updated identifiers are good candidates for a row key.","val":false},{"ans":"If you are using a multitenant Bigtable database, it is a good practice to use a tenant prefix in the row key. This will ensure that all customer data is kept together and not mixed up with other customers\u2018 data.","val":true},{"ans":"You should avoid monotonically increasing values or lexicographically close strings at the beginning of keys because this can cause hotspots.","val":true}],"q_expl":"The two true statements about selecting a row key in Bigtable are:\nC- If you are using a multitenant Bigtable database, it is a good practice to use a tenant prefix in the row key. This will ensure that all customer data is kept together and not mixed up with other customers\u2018 data.\nD- You should avoid monotonically increasing values or lexicographically close strings at the beginning of keys because this can cause hotspots.\nExplanation:\nA- Timestamps can be a good option for a column qualifier in Bigtable to represent versioning of a data point, but they are not recommended as row keys. Row keys should be chosen carefully to distribute data evenly across tablets, prevent hotspots, and support efficient access patterns. Therefore, statement A is false.\nB- Frequently updated identifiers may also lead to hotspots and should be avoided as row keys. Therefore, statement B is false.\nC- In a multitenant Bigtable database, it is a good practice to use a tenant prefix in the row key to group all data from the same customer together. This makes it easier to manage and maintain the data, and it helps to prevent mixing up customer data. Reference:\u00a0Google Cloud Bigtable documentation\nD- Monotonically increasing values or lexicographically close strings at the beginning of keys can cause hotspots because they will be assigned to the same tablet, which can create a bottleneck. Therefore, it is recommended to use a randomized or hashed value for row keys to distribute the data more evenly across the tablets. Reference:\u00a0Google Cloud Bigtable documentation"},{"label":"test_7","q_format":"single","q_text":"Your company built a TensorFlow neutral-network model with a large number of neurons and layers. The model fits well for the training data. However, when tested against new data, it performs poorly. What method can you employ to address this?","answers":[{"ans":"Threading","val":false},{"ans":"Serialization","val":false},{"ans":"Dropout Methods","val":true},{"ans":"Dimensionality Reduction","val":false}],"q_expl":"The method that can be employed to address the poor performance of a neural network model on new data is \u201cDropout\u201c. Dropout is a regularization technique used in neural networks to prevent overfitting. It involves randomly dropping out (i.e., setting to zero) some of the neurons during training, which helps the network to learn more robust and generalizable features. By doing so, the network becomes less reliant on individual neurons and can better generalize to new data.\nThreading and serialization are not relevant techniques for improving the performance of a neural network model on new data.\nDimensionality reduction can be used to reduce the complexity of the model and improve its performance on new data. However, it is not the primary technique for addressing the issue of overfitting in neural networks.\nReference:\nSrivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., & Salakhutdinov, R. (2014). Dropout: A simple way to prevent neural networks from overfitting. Journal of machine learning research, 15(1), 1929-1958.\nGoodfellow, I., Bengio, Y., & Courville, A. (2016). Deep learning (Vol. 1). MIT Press."},{"label":"test_7","q_format":"single","q_text":"For a critical application your company uses Cloud SQL with MySQL for storing relational data. You have one instance in the zone that is closest to users. Your company\u2018s management is concerned about a single point of failure \u2013 when that instance becomes unavailable, your application will crash. What can you do to reduce this risk?","answers":[{"ans":"You can configure High Availability (HA) for Cloud SQL instance.","val":true},{"ans":"You can enable automated backups.","val":false},{"ans":"You can create cross-region read replica for your instance.","val":false},{"ans":"You can create read replica for your instance.","val":false}],"q_expl":"You can configure High Availability (HA) for Cloud SQL instance to reduce the risk of a single point of failure. By enabling HA, Cloud SQL will automatically replicate data from the primary instance to a standby instance in a different zone within the same region, providing redundancy and failover capability in case of an outage. This ensures that your application can continue running even if the primary instance becomes unavailable.\nEnabling automated backups and creating read replicas are useful features, but they do not address the single point of failure issue. A cross-region read replica could provide additional redundancy and disaster recovery capabilities, but it would not prevent an outage in the primary zone. Therefore, the best option to reduce the risk of a single point of failure in this case is to configure High Availability (HA) for Cloud SQL instance."},{"label":"test_7","q_format":"multiple","q_text":"You need to classify your application into workload types. Select all stateful applications. (select 2)","answers":[{"ans":"Image recognition application that identifies people from images.","val":false},{"ans":"A language recognition application that identifies a language from a text.","val":false},{"ans":"A gaming application that constantly tracks user status.","val":true},{"ans":"A shopping application that saves the user\u2018s basket data between sessions.","val":true}],"q_expl":"The stateful applications are:\nA gaming application that constantly tracks user status.\nA shopping application that saves the user\u2018s basket data between sessions.\nStateful applications are those that require persistent storage to maintain their state or data between sessions. In the case of the gaming application, it constantly tracks the user\u2018s status, which likely requires storing data about the user\u2018s progress and achievements. In the case of the shopping application, it saves the user\u2018s basket data between sessions, which requires persistent storage of the user\u2018s shopping cart items.\nOn the other hand, the image recognition and language recognition applications are likely stateless applications as they do not require persistent storage to maintain their state or data. They simply perform a computation on the input data and return a result without the need to maintain any state between sessions.\nTherefore, the two stateful applications are the gaming application and the shopping application.\nNote: It is worth noting that workload types may vary depending on the specific classification system or framework used. The selection of stateful applications may also depend on additional context and requirements of the application."},{"label":"test_7","q_format":"multiple","q_text":"Select all available data storage options for Hadoop Distributed File System (HDFS) in Dataproc. (select 2)","answers":[{"ans":"HDFS with Cloud Storage","val":true},{"ans":"Local SSD","val":true},{"ans":"Bigtable","val":false},{"ans":"BigQuery","val":false}],"q_expl":"The available data storage options for Hadoop Distributed File System (HDFS) in Dataproc are:\nHDFS with Cloud Storage\nLocal SSD\nHDFS with Cloud Storage allows you to use Cloud Storage as the primary storage layer for Hadoop in Dataproc, providing a scalable and cost-effective option for storing data. This setup also allows for seamless integration with other GCP services, such as BigQuery or Cloud Machine Learning Engine.\nLocal SSD provides high-performance and low-latency storage for Hadoop in Dataproc, suitable for temporary data storage or data caching. Local SSDs are ephemeral and do not persist data across cluster restarts, so they should not be used for long-term storage.\nBigtable and BigQuery are not storage options for HDFS in Dataproc. Bigtable is a NoSQL database service optimized for high-performance and low-latency workloads, while BigQuery is a serverless data warehouse service designed for analyzing large datasets using SQL-like queries.\nTherefore, the two available data storage options for HDFS in Dataproc are HDFS with Cloud Storage and Local SSD."},{"label":"test_7","q_format":"multiple","q_text":"You need to design a schema for your database in Cloud Spanner for your mobile game application. You need to decide what to use as a primary key. Select all true statements. (select 2)","answers":[{"ans":"It\u2018s recommended to use a Universally Unique Identifier (UUID) version 4 as a primary key.","val":true},{"ans":"It\u2018s recommended to use a monotonically increasing integer as a primary key.","val":false},{"ans":"It\u2018s recommended to use a monotonically increasing date values as a primary key.","val":false},{"ans":"You can hash the key and store it in a column and use the hash column (or the hash column and the unique key columns together) as a primary key.","val":true}],"q_expl":"The true statements about choosing a primary key in Cloud Spanner for a mobile game application are: \nIt\u2018s recommended to use a Universally Unique Identifier (UUID) version 4 as a primary key. \nYou can hash the key and store it in a column and use the hash column (or the hash column and the unique key columns together) as a primary key. \nCloud Spanner is a horizontally scalable, globally distributed relational database service designed for high availability and consistency. When designing a schema for a database in Cloud Spanner, it\u2018s important to choose a primary key that evenly distributes the data across the database and avoids hotspots or contention. \nUUID version 4 is a recommended primary key for Cloud Spanner as it is globally unique and random, allowing for even distribution of data across the database. This can help avoid hotspots or contention in the system. \nHashing the key and storing it in a column, and using the hash column (or the hash column and the unique key columns together) as a primary key is another approach to evenly distribute the data across the database. This approach can help avoid hotspots or contention in the system and is suitable for cases where the key is not globally unique, such as when using a user ID. \nUsing a monotonically increasing integer or date values as a primary key is not recommended for Cloud Spanner as it can create hotspots or contention in the system. This is because all new rows will be inserted at the end of the table, which can lead to a bottleneck on the last partition. \nTherefore, the two true statements about choosing a primary key in Cloud Spanner for a mobile game application are to use UUID version 4 or hash the key and store it in a column."},{"label":"test_7","q_format":"multiple","q_text":"Select two ways to invoke the pre-trained machine learning APIs (for example Natural Language API) in your application.","answers":[{"ans":"You can use the REST API.","val":true},{"ans":"You can use Cloud Client Libraries when available for production use.","val":true},{"ans":"You can use Google Cloud Console.","val":false},{"ans":"You can use gcloud command.","val":false}],"q_expl":"The two ways to invoke the pre-trained machine learning APIs (for example Natural Language API) in your application are:\nYou can use the REST API.\nYou can use Cloud Client Libraries when available for production use.\nThe REST API provides a simple and flexible way to interact with Google Cloud services, including pre-trained machine learning APIs, using standard HTTP methods such as POST, GET, PUT, and DELETE. You can use any programming language that supports HTTP requests to interact with the REST API.\nCloud Client Libraries are available for many programming languages and provide a higher-level interface for interacting with Google Cloud services, including pre-trained machine learning APIs. They handle many of the low-level details of interacting with the API, such as authentication and error handling, and provide idiomatic language-specific APIs that can make it easier to integrate the service into your application.\nWhile you can use the Google Cloud Console to interact with pre-trained machine learning APIs, it is not typically used to invoke the APIs in your application. Similarly, while the gcloud command can be used to interact with Google Cloud services, it is not typically used to invoke pre-trained machine learning APIs in your application."},{"label":"test_7","q_format":"single","q_text":"You are using Google BigQuery as your data warehouse. Your users report that the following simple query is running very slowly, no matter when they run the query:\nSELECT country, state, city FROM [myproject:mydataset.mytable] GROUP BY country\nYou check the query plan for the query and see the following output in the Read section of Stage:1:\n\nWhat is the most likely cause of the delay for this query?","answers":[{"ans":"The [myproject:mydataset.mytable] table has too many partitions","val":false},{"ans":"Users are running too many concurrent queries in the system","val":false},{"ans":"Either the state or the city columns in the [myproject:mydataset.mytable] table have too many NULL values","val":false},{"ans":"Most rows in the [myproject:mydataset.mytable] table have the same value in the country column, causing data skew Most Voted","val":true}],"q_expl":"The most likely cause of the delay for the query is\nD. Most rows in the [myproject:mydataset.mytable] table have the same value in the country column, causing data skew.\nHere\u2019s why:\n\nData skew occurs when a large portion of the data in a table has the same value in a particular column. In this case, the query groups the data by country, and since most rows have the same country value, the query has to process a lot of data for that particular country. This can slow down the query significantly.\n\nThe other options are less likely:\nA. Too many partitions:\u00a0While having too many partitions can impact query performance, it\u2019s less likely to be the main cause here. The fact that the query is slow no matter when it\u2019s run suggests the issue lies with the data itself, not how the table is partitioned.\nB. Too many concurrent queries:\u00a0If there were too many concurrent queries overloading the system, you might see slowness for other queries as well. The fact that this specific query is slow suggests the problem is isolated to the data and the query itself.\nC. NULL values in state or city columns:\u00a0NULL values can sometimes impact query performance, but it\u2019s unlikely to be the main culprit here. Even with NULL values, the query would still need to process all the rows to group them by country.\n\n\nTo address data skew, you can try clustering the table by the country column. This will physically store rows with the same country value together, which can improve query performance for aggregations like grouping."},{"label":"test_7","q_format":"single","q_text":"You have some data, which is shown in the graphic below. The two dimensions are X and Y, and the shade of each dot represents what class it is. You want to classify this data accurately using a linear algorithm. To do this you need to add a synthetic feature. What should the value of that feature be?","answers":[{"ans":"X2+Y2","val":true},{"ans":"X2","val":false},{"ans":"Y2","val":false},{"ans":"cos(X)","val":false}],"q_expl":"To classify the data accurately using a linear algorithm, we need to add a synthetic feature that helps separate the classes in the data. One common approach is to add a feature that captures non-linear patterns in the data. In this case, it seems that a circular boundary separates the two classes. Therefore, we can add a synthetic feature that captures the distance of each point from the origin.\nThe value of that feature should be X2 + Y2, which represents the Euclidean distance from the origin to each point. By adding this feature to our linear model, we can better capture the circular boundary and classify the data more accurately.\nReference:\n\u201cFeature engineering for machine learning: principles and techniques for data scientists\u201c by Alice Zheng and Amanda Casari, Chapter 3.\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\nThe best synthetic feature to add to the data for this case is A. X2+Y2.\nHere\u2019s why adding X2+Y2 as a feature helps with linear separability:\n\nThe data consists of two classes represented by different shades of dots.\n\nA linear classification algorithm separates the data using a straight line.\n\nIn the current two-dimensional space (X and Y), the data isn\u2019t linearly separable because no straight line can perfectly divide the two classes.\n\nAdding a new feature, X2+Y2, creates a new dimension. In this new three-dimensional space, the data becomes linearly separable. This is because the X2+Y2 term creates a circular boundary around the data points, allowing a straight line to separate the two classes.\n\nOther options wouldn\u2019t achieve linear separability:\n\nB. X2 and C. Y2 only add a single new feature based on squaring one of the existing dimensions (X or Y). This won\u2019t create the necessary circular boundary for separation.\nD. cos(X) adds a new feature based on cosine of the X value. While it might change the data distribution, it is unlikely to create a clear separation line between the two classes."},{"label":"test_7","q_format":"single","q_text":"An e-commerce company uses Cloud SQL with PostgreSQL for storing relational data. Their data science team wants to perform SQL queries on this database to provide insights and build machine learning models. They don\u2018t want to execute queries directly on the database, because it can lead to poor performance of the application. What should you advise them?","answers":[{"ans":"They should configure High Availability (HA) for the instance.","val":false},{"ans":"The should enable automated backups.","val":false},{"ans":"The should move this data to BigQuery.","val":true},{"ans":"The should create read replica to offload work from a Cloud SQL instance.","val":false}],"q_expl":"The best course of action for the e-commerce company is: C. The should move this data to BigQuery.\nHere\u2019s why:\n\nHigh Availability (HA) (A): While beneficial for ensuring database uptime, it doesn\u2019t address the issue of offloading queries from the primary Cloud SQL instance.\n\nAutomated backups (B): This is essential for data protection but doesn\u2019t solve the problem of query performance on the production database.\n\nRead replicas (D): This can be helpful for offloading read-only traffic, but it still requires maintaining and managing a separate replica instance.\n\nBigQuery (C): This is a serverless data warehouse specifically designed for running large-scale SQL queries and analytics on massive datasets. It offers several advantages:\nScalability:\u00a0BigQuery can handle large datasets efficiently without impacting the performance of the Cloud SQL instance.\nCost-effectiveness:\u00a0You only pay for the resources you use when querying data in BigQuery.\nPerformance:\u00a0BigQuery is optimized for running complex queries on large datasets, providing faster insights for the data science team.\nSecurity:\u00a0BigQuery offers robust access controls to ensure data security.\n\n\nBy moving the data to BigQuery, the e-commerce company can free up resources on the Cloud SQL instance, allowing it to focus on serving core application functionalities. The data science team can then leverage BigQuery\u2019s capabilities to perform complex analyses and build machine learning models without impacting application performance."},{"label":"test_7","q_format":"multiple","q_text":"An e-commerce company wants to develop a web application to sell various types of products. Products can have different types of attributes. They are looking for a solution to store semi-structured data. An important consideration for this application is a powerful search engine. Which service should you advise them? (select 2)","answers":[{"ans":"Cloud Spanner","val":false},{"ans":"MongoDB as a Service on Google Cloud","val":true},{"ans":"Cloud SQL with MySQL or PostgreSQL","val":false},{"ans":"Cloud Datastore","val":true}],"q_expl":"A combination of two services for the e-commerce company\u2019s needs:\n\nB. MongoDB as a Service on Google Cloud\nD. Cloud Search\n\nHere\u2019s why:\n\nCloud Spanner (A): While a great relational database, it\u2019s not ideal for storing and querying semi-structured data like product information with varying attributes.\n\nCloud SQL with MySQL or PostgreSQL (C): These are relational databases and wouldn\u2019t be the best fit for storing schema-less or schema-flexible data like product attributes.\n\nMongoDB as a Service on Google Cloud (B): This is a managed service offering for MongoDB, a NoSQL document database. It\u2019s perfect for storing semi-structured data like product information with varying attributes. MongoDB\u2019s flexible schema allows for easy addition of new product attributes without schema changes.\n\nHowever, MongoDB itself doesn\u2019t provide a built-in powerful search engine.\n\nCloud Search (D):\u00a0This is a managed service that provides a powerful search engine specifically designed for indexing and searching large volumes of data. It can be integrated with MongoDB to enable faceted search and filtering based on product attributes, crucial for e-commerce applications.\n\nTogether, MongoDB and Cloud Search offer a robust solution for the e-commerce company:\n\nMongoDB\u00a0stores product data efficiently with its flexible schema.\nCloud Search\u00a0indexes the data in MongoDB, enabling powerful search functionality for the web application.\n\nThis combination allows the company to manage product information effectively and provide a seamless search experience for their customers."},{"label":"test_7","q_format":"multiple","q_text":"Select all true statements about Bigtable. (select 2)","answers":[{"ans":"Bigtable is a NoSQL, column-oriented database.","val":true},{"ans":"Bigtable is a SQL database engine powered by Google.","val":false},{"ans":"Bigtable is a good choice for time-series and IoT data.","val":true},{"ans":"Bigtable is ideal for applications that need very high throughput and scalability for key\/value data.","val":false}],"q_expl":"Bigtable as a NoSQL, Column-Oriented Database:\nBigtable is classified as a NoSQL database due to its non-relational nature, which allows for flexible and scalable data storage. It is designed as a wide-column store, where data is organized in columns rather than traditional rows and columns found in relational databases. This structure enables efficient storage and retrieval of large amounts of data, making it suitable for applications requiring high scalability and performance\n\nBigtable as a Good Choice for Time-Series and IoT Data:\n\nBigtable\u2019s design and capabilities make it well-suited for handling time-series and IoT data effectively. Time-series data, which involves recording data points at specific time intervals, can be efficiently managed in Bigtable due to its ability to store vast amounts of timestamped data and provide low-latency access to this information. Similarly, IoT data, which involves a large volume of interconnected devices generating continuous streams of data, can benefit from Bigtable\u2019s high throughput and scalability features\nWhy Other Options Are Incorrect:\n\nB. Bigtable is a SQL Database Engine Powered by Google:\u00a0This statement is incorrect as Bigtable is not a SQL database engine but rather a NoSQL database service designed for wide-column and key-value storage.\nD. Bigtable is Ideal for Applications That Need Very High Throughput and Scalability for Key\/Value Data:\u00a0While this statement seems accurate at first glance, it lacks the specificity provided by the correct option C regarding time-series and IoT data. Bigtable\u2019s suitability extends beyond just key\/value data to include specialized use cases like time-series and IoT applications"},{"label":"test_7","q_format":"single","q_text":"Your team has developed a new version of the pipeline for Dataflow with a different windowing algorithm and triggering strategy (Python). As a Data Engineer, you are responsible for updating a running pipeline with the new version. You don\u2018t want to lose any data during the update. What should you do?","answers":[{"ans":"You should stop the Dataflow pipeline with the Cancel option. Than. create a new Dataflow job with the updated version.","val":false},{"ans":"You should update the Dataflow pipeline by passing the\u00a0--update\u00a0option with the\u00a0--job_name\u00a0set to the existing job name.","val":false},{"ans":"You should stop the Dataflow pipeline with the Drain option. Than, create a new Dataflow job with the updated version.","val":true},{"ans":"You should update the Dataflow pipeline by passing the\u00a0--update\u00a0option with the\u00a0--job_name\u00a0set to a new unique job name.","val":false}],"q_expl":"You should stop the Dataflow pipeline with the Drain option. Than, create a new Dataflow job with the updated version.\nHere\u2019s why:\n\nDrain Option: This option ensures that all the currently processing data is processed and written to the sink before the pipeline is stopped. This prevents data loss.\nNew Dataflow Job: Creating a new Dataflow job with the updated version allows you to deploy the new windowing algorithm and triggering strategy without affecting the existing pipeline.\n\nBy following this approach, you can seamlessly transition to the new pipeline version without compromising data integrity."},{"label":"test_7","q_format":"multiple","q_text":"Select all true statements about replication in Cloud Spanner. (select 2)","answers":[{"ans":"Single-region instances use only read-write replicas.","val":true},{"ans":"Cloud Spanner has three types of replicas: read-write replicas, read-only replicas, and witness replicas.","val":true},{"ans":"All types of replicas can participate in voting to commit writes.","val":false},{"ans":"Witness replicas support reads and writes and participate in voting to commit writes.","val":false}],"q_expl":"The correct statements are:\n\nCloud Spanner has three types of replicas: read-write replicas, read-only replicas, and witness replicas.\nSingle-region instances use only read-write replicas.\u00a0 \u00a0\n\nHere\u2019s a brief explanation of each statement:\n\nCloud Spanner has three types of replicas: read-write replicas, read-only replicas, and witness replicas. This is correct. Cloud Spanner uses these three types of replicas to ensure high availability, scalability, and consistency.\nSingle-region instances use only read-write replicas. This is also correct. Single-region instances do not have read-only or witness replicas. All replicas in a single-region instance are read-write replicas, which ensures high availability and performance.\nAll types of replicas can participate in voting to commit writes. This is incorrect. Only read-write replicas participate in voting to commit writes. Read-only and witness replicas do not participate in voting.\nWitness replicas support reads and writes and participate in voting to commit writes. This is incorrect. Witness replicas are primarily used to prevent split-brain scenarios and do not support reads or writes. They do not participate in voting to commit writes.\n\nHere is a table that summarizes the different types of replicas in Cloud Spanner:"},{"label":"test_7","q_format":"single","q_text":"As a Data Engineer, you need to prepare a Cloud Spanner database design for a new mobile game. You want to define a column that can contain a document-like structure. Which data type in Cloud Spanner should you use?","answers":[{"ans":"STRUCT","val":false},{"ans":"ARRAY","val":false},{"ans":"JSON","val":true},{"ans":"STRING","val":false}],"q_expl":"As a Data Engineer, when designing a Cloud Spanner database and wanting to define a column that can contain a document-like structure, you should use the\u00a0JSON\u00a0data type.\nThe JSON data type in Cloud Spanner allows for the storage of semi-structured data in a column. It supports a variety of data types and is similar to storing data in a document-based NoSQL database.\nReference:\nCloud Spanner documentation:\u00a0https:\/\/cloud.google.com\/spanner\/docs\/data-types#json_type"},{"label":"test_7","q_format":"single","q_text":"When you create a table partitioned by ingestion time, BigQuery automatically assigns rows to partitions based on the time when BigQuery ingests the data. Suppose you have a daily partitioned table named\u00a0report\u00a0in your\u00a0sales\u00a0dataset. How do you create a query that only scans the partitions between March 1, 2022 and March 2, 2022?","answers":[{"ans":"SELECT  column(s)FROM  sales.report;","val":false},{"ans":"SELECT  column(s)FROM  sales.report WHERE  _PARTITIONTIME IN [TIMESTAMP(\u20182022-03-01\u2018), TIMESTAMP(\u20182022-03-02\u2018)];","val":false},{"ans":"SELECT  column(s)FROM  sales.report WHERE  _PARTITIONTIME BETWEEN TIMESTAMP(\u20182022-03-01\u2018) AND TIMESTAMP(\u20182022-03-02\u2018);","val":true},{"ans":"SELECT  column(s)FROM  sales.report WHERE  _PARTITIONTIME LIKE TIMESTAMP(\u20182022-03-01\u2018) AND TIMESTAMP(\u20182022-03-02\u2018);","val":false}],"q_expl":"The correct query to scan the partitions between March 1, 2022 and March 2, 2022 in a daily partitioned table named report in the sales dataset in BigQuery is:\nsqlCopy code\nSELECT column(s)\nFROM sales.report\nWHERE _PARTITIONTIME BETWEEN TIMESTAMP(\u20182022-03-01\u2018) AND TIMESTAMP(\u20182022-03-02\u2018);\nThis query uses the\u00a0BETWEEN\u00a0operator to select partitions with a\u00a0_PARTITIONTIME\u00a0within the specified range. The\u00a0_PARTITIONTIME\u00a0pseudo column is used to filter partitions based on ingestion time.\nReference:\u00a0Querying partitioned tables"},{"label":"test_7","q_format":"single","q_text":"You are using TensorFlow for building neural network model with a large numbers of hidden layers and neurons for image classification task. Your model fits very well for training data, but performs poorly on testing data. What can you recommend in this situation?","answers":[{"ans":"Use serialization methods.","val":false},{"ans":"Use dimensionality reduction techniques.","val":false},{"ans":"Use dropout methods.","val":true},{"ans":"Use multithreading.","val":false}],"q_expl":"In this situation, it is likely that the model is overfitting on the training data, meaning that it is too complex and has essentially memorized the training data rather than learning the underlying patterns. One solution to address overfitting is to use regularization techniques such as dropout. Dropout is a method that randomly drops out a fraction of the neurons during training, which helps prevent overfitting by encouraging the network to learn more robust features. Therefore, the recommendation is to use dropout methods.\nSerialization, dimensionality reduction, and multithreading are not directly related to addressing overfitting in neural networks. Serialization is the process of converting an object into a format that can be stored or transmitted, which is not relevant in this situation. Dimensionality reduction techniques are used to reduce the number of features in a dataset, which is not the issue in this situation. Multithreading is a technique to improve performance by utilizing multiple threads in parallel, but it is not directly related to addressing overfitting in neural networks."},{"label":"test_7","q_format":"single","q_text":"Which statement is true about billing for solutions deployed using Cloud Marketplace?","answers":[{"ans":"You only pay for the core GCP resources you use, with no additional fees for commercially licensed software.","val":false},{"ans":"After a trial period, each Cloud Marketplace solution charges you a fixed monthly fee.","val":false},{"ans":"Cloud Marketplace solutions are free.","val":false},{"ans":"You only pay for the core GCP resources you use, with any additional fees for commercially licensed software.","val":true}],"q_expl":"Link to resources:\nhttps:\/\/cloud.google.com\/marketplace\/docs\/understanding-billing"},{"label":"test_7","q_format":"single","q_text":"You need a cost-effective backup of multi-TB databases from another cloud (such as AWS). What should you do?","answers":[{"ans":"You should use Transfer Appliance and transfer data to Cloud Storage Nearline bucket.","val":false},{"ans":"You should use Storage Transfer Service and transfer data to Cloud Storage Standard bucket.","val":false},{"ans":"You should use Storage Transfer Service and transfer data to Cloud Storage Nearline bucket.","val":true},{"ans":"You should use Transfer Appliance and transfer data to Cloud Storage Standard bucket.","val":false}],"q_expl":"All the options provided can be used for backing up multi-TB databases from another cloud like AWS. However, based on the requirement of a cost-effective solution, the best option would be to use\u00a0Storage Transfer Service and transfer data to Cloud Storage Nearline bucket.\nStorage Transfer Service is a cost-effective and reliable solution that simplifies the transfer of data between cloud storage providers. It supports transfers from AWS S3 to Google Cloud Storage and offers flexible scheduling options and configurable transfer settings.\nUsing Cloud Storage Nearline bucket instead of Cloud Storage Standard bucket can provide additional cost savings, as it offers a lower storage cost but with slightly higher retrieval costs and longer retrieval times.\nTransfer Appliance is another option for transferring large amounts of data, but it involves additional hardware costs and shipping fees, which might not be the most cost-effective option for a one-time backup.\nReferences:\nGoogle Cloud Storage Transfer Service:\u00a0https:\/\/cloud.google.com\/storage-transfer-service\nGoogle Cloud Storage Nearline:\u00a0https:\/\/cloud.google.com\/storage\/docs\/nearline"},{"label":"test_7","q_format":"multiple","q_text":"As a Data Engineer, you are responsible for designing an image classification model using TensorFlow. You have 10 millions of images in dataset. You have randomly shuffled the images and split into training and test sets. You\u2018ve prepared your baseline model which is a classic artificial neural network (ANN) and want to improve model accuracy. What can you do to increase accuracy? (select 2)","answers":[{"ans":"In this case you should use Recurrent Neural Network (RNN) architecture.","val":false},{"ans":"You should use regularization techniques. For example, dropout or batch normalization.","val":true},{"ans":"You can use convolutional layers in your architecture and create a Convolutional Neural Network (CNN).","val":true},{"ans":"You can increase the complexity of the model. For example, adding an extra hidden layer.","val":false}],"q_expl":"The correct answers are:\nYou can use convolutional layers in your architecture and create a Convolutional Neural Network (CNN).\nYou should use regularization techniques. For example, dropout or batch normalization.\nExplanation:\nIn image classification tasks, Convolutional Neural Networks (CNNs) are widely used because they can extract spatial features from the image data efficiently. Therefore, using CNNs can improve the accuracy of the image classification model. (Reference: LeCun, Y., Bengio, Y., & Hinton, G. (2015). Deep learning. Nature, 521(7553), 436-444.)\nRegularization techniques such as dropout and batch normalization can prevent overfitting and improve the generalization performance of the model. Dropout randomly drops out some neurons during training, which reduces the model\u2018s reliance on any specific subset of neurons and thus improves generalization. Batch normalization normalizes the input to each activation function in a batch, which helps reduce internal covariate shift and allows higher learning rates, thus improving generalization. (Reference: Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., & Salakhutdinov, R. (2014). Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15(1), 1929-1958. and Ioffe, S., & Szegedy, C. (2015). Batch normalization: Accelerating deep network training by reducing internal covariate shift. International Conference on Machine Learning, 448-456.)\nIncreasing the complexity of the model by adding an extra hidden layer may not always lead to better accuracy, as it can also increase the risk of overfitting. Moreover, Recurrent Neural Networks (RNNs) may not be suitable for image classification tasks, as they are more suitable for sequential data, such as natural language processing."},{"label":"test_7","q_format":"single","q_text":"As a Data Engineer, you are responsible for preparing a migration plan for your company. You want to migrate Apache Kafka (real-time streaming data pipelines) to the Google Cloud. Which Google Cloud service should you use?","answers":[{"ans":"Cloud Bigtable","val":false},{"ans":"Cloud Datastore","val":false},{"ans":"Cloud Firestore","val":false},{"ans":"Cloud Pub\/Sub","val":true}],"q_expl":"The Google Cloud service that should be used to migrate Apache Kafka to the Google Cloud is Cloud Pub\/Sub.\nExplanation:\nCloud Pub\/Sub is a globally distributed message broker service provided by Google Cloud. It is designed for real-time messaging scenarios and can handle high throughput and low latency messaging at scale. Cloud Pub\/Sub can be used to migrate Apache Kafka (real-time streaming data pipelines) to the Google Cloud because it provides similar functionality for message streaming and distribution, but with the added benefits of fully managed service and global scalability. Cloud Pub\/Sub supports pull and push message delivery modes, and it is integrated with other Google Cloud services, such as Dataflow, which can be used for real-time stream processing. (Reference: Google Cloud Documentation \u2013 Cloud Pub\/Sub Overview.)"},{"label":"test_7","q_format":"single","q_text":"Your company has a lot of unstructured data (text, audio, video files) stored in a private data center. They want to back up in case their private datacenter crashes. What GCP service should you use to store this type of data?","answers":[{"ans":"Bigtable","val":false},{"ans":"Cloud Datastore","val":false},{"ans":"BigQuery","val":false},{"ans":"Cloud Storage","val":true}],"q_expl":"The GCP service that should be used to store unstructured data (text, audio, video files) for backup in case of a private data center crash is Cloud Storage.\nExplanation:\nCloud Storage is a highly scalable object storage service provided by Google Cloud. It is designed to store and retrieve large amounts of unstructured data, such as text, audio, video files, and backups. Cloud Storage provides durable and highly available storage for data, with low latency access times and strong consistency guarantees. It also supports multiple storage classes, including Standard, Nearline, Coldline, and Archive, to optimize storage costs based on the frequency of data access. Cloud Storage also provides a number of features to manage data, including lifecycle policies, versioning, and access control. Therefore, Cloud Storage is the most suitable service for backing up unstructured data stored in a private data center. The other services, such as Bigtable, Cloud Datastore, and BigQuery, are designed for structured data storage and analysis, and may not be the best option for unstructured data backup."},{"label":"test_7","q_format":"single","q_text":"As a Data Engineer, you must advise a development team on setting up virtual machines in GCP. The team uses Cloud Console every time they need a virtual machine. You think this is not a productive approach and need to come up with a solution. What Google Cloud service do you recommend?","answers":[{"ans":"Cloud Build","val":false},{"ans":"Deployment Manager","val":false},{"ans":"Managed Instance Group","val":true},{"ans":"Unmanaged Instance Group","val":false}],"q_expl":"The Google Cloud service that can be recommended to the development team for setting up virtual machines in GCP in a more productive way is the Managed Instance Group.\nExplanation:\nA Managed Instance Group is a service provided by Google Cloud to manage and scale virtual machines in an easier and more automated way. Managed Instance Groups provide the ability to create and manage a pool of virtual machine instances from a common instance template. This allows for automated scaling, load balancing, and autohealing of virtual machines. The group can also be managed using the Google Cloud Console, command-line tools, or APIs. Managed Instance Groups can provide a more efficient and streamlined approach to creating and managing virtual machines than creating them individually using the Cloud Console.\nCloud Build is a service for building and deploying applications and artifacts, Deployment Manager is a service for creating and managing cloud resources using templates, and Unmanaged Instance Groups allow developers to manage and scale virtual machines individually, rather than as a group. Therefore, Managed Instance Group is the most appropriate service to manage and scale virtual machines for the development team."},{"label":"test_7","q_format":"single","q_text":"Google Cloud encrypts all customer content stored at rest. Each data chunk written to a storage system in Google Cloud is encrypted with a data encryption key. How does Google Cloud protect the data encryption key so that an attacker who gained access to the storage system storing the key could not use it to decrypt the data chunk?","answers":[{"ans":"Google Cloud writes the data encryption key to a hidden location on disk.","val":false},{"ans":"Google Cloud encrypts the data encryption key with a key encryption key.","val":true},{"ans":"Google Cloud stores the data encryption key in a secure Shielded VM instance.","val":false},{"ans":"Google Cloud stores the data encryption key in a secure Cloud SQL database.","val":false}],"q_expl":"Google Cloud encrypts the data encryption key with a key encryption key. This process is called key wrapping. Key wrapping is a cryptographic technique in which an encryption key is encrypted (wrapped) with another key so that it can be safely stored or transmitted without unauthorized access. In Google Cloud, the key encryption key is managed by the Google Key Management Service (KMS), which is designed to safeguard cryptographic keys and provide a central management system for them. By using KMS, Google Cloud can provide a secure and scalable way to manage encryption keys for its customers. Reference:\u00a0Google Cloud documentation."},{"label":"test_7","q_format":"single","q_text":"As a Data Engineer, you are preparing a migration strategy. A company has sensitive data that must be encrypted with keys controlled by company. The data will be stored in GCP. You want to minimize costs and operational overhead. What would you recommend?","answers":[{"ans":"You should use a custom encryption algorithm for the data.","val":false},{"ans":"You cannot use your own keys with GCP.","val":false},{"ans":"You should use Cloud KMS for sensitive data.","val":true},{"ans":"You should use Google default encryption for the data.","val":false}],"q_expl":"To meet the requirement of having sensitive data encrypted with keys controlled by the company, it is recommended to use Cloud Key Management Service (Cloud KMS) in GCP. Cloud KMS provides a highly available and scalable managed service that allows you to encrypt data using your own encryption keys that you create, own, and manage. This allows you to have full control over the encryption keys and ensure that only authorized personnel can access the sensitive data. Additionally, using Cloud KMS can help minimize operational overhead and costs compared to managing your own encryption infrastructure. Therefore, the recommended option is:\nYou should use Cloud KMS for sensitive data."},{"label":"test_7","q_format":"single","q_text":"The current data warehouse for your company runs on a MySQL instance in a private data center. Your manager wants to migrate to Google Cloud and find cost effective solution with minimal operational overhead. The data warehouse must be scalable to 10 PB. Which GCP service should you recommend?","answers":[{"ans":"Cloud Spanner","val":false},{"ans":"Cloud SQL using MySQL","val":false},{"ans":"BigQuery","val":true},{"ans":"Managed Instance Group (MIG) using MySQL","val":false}],"q_expl":"For a data warehouse that needs to be scalable up to 10 PB, the recommended GCP service is BigQuery. BigQuery is a serverless, highly scalable, and cost-effective data warehouse solution that can easily handle such large amounts of data with minimal operational overhead. With BigQuery, there is no need to manage or provision infrastructure, as it automatically scales to meet the demands of your queries. Additionally, BigQuery supports ANSI SQL and integrates well with other GCP services, making it a great choice for migrating from a private data center. Therefore, the recommended GCP service is BigQuery."},{"label":"test_7","q_format":"single","q_text":"As a Data Engineer, you are responsible for preparing best practices for your company. What kind of data and information should be in an API error message. What would you recommend?","answers":[{"ans":"An API error message should define your own set of application-specific error codes.","val":false},{"ans":"An API error message should return error details in the payload, and don\u2018t return a status code.","val":false},{"ans":"An API error message should return a status code form with the standard 400s and 500s HTTP status codes along with additional error details in the payload.","val":true},{"ans":"An API error message should return HTTP status 200 with additional error details in the payload.","val":false}],"q_expl":"An API error message should return a status code form with the standard 400s and 500s HTTP status codes along with additional error details in the payload. HTTP status codes are standard response codes that indicate whether a request has been successfully completed or not. Standard HTTP status codes are used to provide a consistent and well-defined way of communicating error conditions to API users. The status code should be returned in the response header, and additional error details, such as error message, error code, and suggested actions, should be returned in the response body or payload. This approach provides a clear and consistent way of communicating errors to API users and allows them to identify and troubleshoot issues more effectively."},{"label":"test_7","q_format":"multiple","q_text":"Select all true statements about application data, streaming data and batch data. (select 2)","answers":[{"ans":"Cloud Pub\/Sub is typically used for data streaming and Cloud Storage for batch data.","val":true},{"ans":"Application data is generated by applications. For example, user-generated data, log data or clickstream data.","val":true},{"ans":"Batch data is a set of data that is typically sent in small messages that are transmitted continuously from a data source.","val":false},{"ans":"Streaming data is ingested in bulk, usually in files. For example, archiving data in long-term storage.","val":false}],"q_expl":"Cloud Pub\/Sub is typically used for data streaming and Cloud Storage for batch data. (True)\nApplication data is generated by applications. For example, user-generated data, log data or clickstream data. (True)\nNote: The other two statements are false. Batch data is typically sent in large messages at regular intervals, while streaming data is ingested in real-time as a continuous flow of data."},{"label":"test_7","q_format":"single","q_text":"You need to connect to your Linux virtual machine in GCP using third-party tool. What do you need?","answers":[{"ans":"You need to create a new user account to be able to connect.","val":false},{"ans":"You cannot connect to the virtual machine using a third-party tool.","val":false},{"ans":"GCP creates SSH keys on your behalf, you don\u2018t have to worry about it.","val":false},{"ans":"To be able to connect, you need to create an SSH key and add it to your virtual machine.","val":true}],"q_expl":"To connect to your Linux virtual machine in GCP using a third-party tool, you need to create an SSH key and add it to your virtual machine.\nHere are the steps you can follow:\n1. Generate an SSH key pair on your local machine if you don\u2018t already have one.\n2. Add the public key to the metadata of the virtual machine instance you want to connect to.\n3. Use a third-party tool that supports SSH to connect to the virtual machine using the private key.\nYou can follow the instructions in the GCP documentation to create and add SSH keys to your virtual machine instance:\nCreating an SSH key pair\nAdding or removing SSH keys for instances"},{"label":"test_7","q_format":"single","q_text":"A retail company has deployed a machine learning model to predict customer purchasing behavior based on past transactions. Over time, the model\u2018s predictions become less accurate, leading to a decrease in sales. The company wants to improve the accuracy of the model. What is the best approach for retraining the machine learning model in this scenario?","answers":[{"ans":"Re-evaluate the model\u2018s features and retrain it on a modified version of the original dataset.","val":false},{"ans":"Replace the current model with a more complex model trained on a larger dataset.","val":false},{"ans":"Use the current model and retrain it on the most recent transactions data only.","val":false},{"ans":"Retrain the model on a diverse dataset that includes a mix of recent and historical transactions data.","val":true}],"q_expl":"The best approach for retraining the machine learning model in this scenario would be to retrain the model on a diverse dataset that includes a mix of recent and historical transactions data.\nExplanation:\nThe decrease in accuracy over time suggests that the model has become less effective in capturing the changing patterns and trends in customer behavior. Therefore, it\u2018s important to retrain the model using a diverse dataset that includes a mix of recent and historical transactions data to capture these changes. Retraining the model on only the most recent transactions data may not be sufficient as it may not capture the patterns and trends that have emerged over a longer time frame.\nReplacing the current model with a more complex model trained on a larger dataset may not be the best approach either as it may lead to overfitting and higher computational costs. It\u2018s important to strike a balance between model complexity and dataset size.\nRe-evaluating the model\u2018s features and retraining it on a modified version of the original dataset may be useful in some cases, but it\u2018s not clear from the scenario if the feature selection was the cause of the decrease in accuracy. Therefore, it may not be the most appropriate approach in this scenario."},{"label":"test_7","q_format":"single","q_text":"As a Data Engineer, you must propose a solution for the following case. A development team wants to build an application that stores images in a Cloud Storage bucket and wants to compress those images for future use in machine learning models. They want to use a Google-managed service that can automatically scale up and down to zero with minimal effort. Which Google Cloud service should you recommend to do this?","answers":[{"ans":"App Engine","val":false},{"ans":"Compute Engine","val":false},{"ans":"Kubernetes Engine","val":false},{"ans":"Cloud Functions","val":true}],"q_expl":"I would recommend using Cloud Functions for this use case. Cloud Functions is a serverless compute service that automatically scales up and down to zero. It allows you to write and deploy code that responds to Cloud Storage events, such as the creation of a new image in a bucket. You can then use the Python Imaging Library (Pillow) to compress the image and store it in a separate Cloud Storage bucket. This can be done with minimal effort as Cloud Functions abstracts away the underlying infrastructure and automatically manages scaling for you.\nReference:\nCloud Functions:\u00a0https:\/\/cloud.google.com\/functions\nUsing Cloud Functions with Cloud Storage:\u00a0https:\/\/cloud.google.com\/functions\/docs\/calling\/storage"},{"label":"test_7","q_format":"single","q_text":"As a Data Engineer, you are responsible for creating Bigtable instances. You want to store historical data (over 50 TB) for a large number of remote-sensing devices and use the data to generate weekly reports. You need to choose whether to store data on solid-state drives (SSDs) or hard disk drives (HDDs). You want to choose the most cost-effective solution. Which one should you use?","answers":[{"ans":"You should choose solid-state drives (SSDs) because they cost the same as HDDs and are more efficient than HHDs.","val":false},{"ans":"You should choose a hard disk drives (HDDs).","val":true},{"ans":"You can freely choose between HDDs and SSDs drives. There is no difference in price.","val":false},{"ans":"You should choose a solid-state drives (SSDs).","val":false}],"q_expl":"You should choose a hard disk drives (HDDs) because they are more cost-effective for storing large amounts of data like historical data, and are a better fit for workloads that require high-volume, sequential data access like generating weekly reports. SSDs, on the other hand, are more expensive and are generally better suited for workloads that require high-speed, low-latency random data access, such as online transaction processing (OLTP) and high-performance computing (HPC) applications. Reference:\u00a0Google Cloud documentation."},{"label":"test_7","q_format":"single","q_text":"A shipment tracking application receives data from sensors. Sometimes more data arrives than the virtual machines can process. You don\u2018t want to use additional virtual machines. What can you do to prevent data loss? You also need the most economical solution.","answers":[{"ans":"You should increase the CPU.","val":false},{"ans":"You should write data to local SSDs on the Compute Engine virtual machines.","val":false},{"ans":"You should write data to the Cloud Pub\/Sub queue, and the application should read data from the queue.","val":true},{"ans":"You should write data to Cloud Memorystore, and the application should read data from the cache.","val":false}],"q_expl":"To prevent data loss in a scenario where data arrives faster than the virtual machines can process, and additional virtual machines cannot be used, it is recommended to use a buffer to temporarily store the incoming data until the virtual machines are ready to process it. One option for this is to write the data to a message queue, such as Cloud Pub\/Sub, which can act as a buffer and handle large volumes of data without data loss. This is an economical solution since it doesn\u2018t require additional virtual machines or expensive storage options like local SSDs or Cloud Memorystore. Therefore, the recommended solution in this scenario is to write data to the Cloud Pub\/Sub queue, and the application should read data from the queue."},{"label":"test_7","q_format":"single","q_text":"As a Data Engineer, you are managing a Dataproc cluster. You need to speed up the job while minimizing costs without losing work in progress on your cluster. What should you do?","answers":[{"ans":"You should increase the cluster size with more non-preemptible workers.","val":false},{"ans":"You should increase the cluster size with preemptible worker nodes, and use Cloud Functions to trigger a script to preserve work.","val":false},{"ans":"You should increase the cluster size with preemptible worker nodes, and configure them to use graceful decommissioning.","val":true},{"ans":"You cannot do this without stopping the cluster.","val":false}],"q_expl":"You should increase the cluster size with preemptible worker nodes, and configure them to use graceful decommissioning. This will help you to speed up the job by adding more worker nodes to the cluster, while keeping the cost low by using preemptible VMs. In addition, configuring the preemptible VMs to use graceful decommissioning will help to preserve work in progress on your cluster. When a worker node is preempted, Dataproc will automatically save the job state to HDFS and reschedule the work on other available worker nodes. This will help you to minimize costs without losing work in progress on your cluster.\nReference:\nhttps:\/\/cloud.google.com\/dataproc\/docs\/concepts\/configuring-clusters\/preemptible-workers\nhttps:\/\/cloud.google.com\/dataproc\/docs\/concepts\/configuring-clusters\/graceful-decommission"},{"label":"test_7","q_format":"single","q_text":"Suppose you have a MySQL instance running in Cloud SQL. You want to export this database from Cloud SQL and recreate this database locally so the development team can test new features. What should you do?","answers":[{"ans":"You should create a SQL dump file using\u00a0pg_dump\u00a0to export data from Cloud SQL to Cloud Storage and download the data.","val":false},{"ans":"You should create a SQL dump file using\u00a0mysqldump\u00a0to export data from Cloud SQL to Cloud Storage and download the data.","val":true},{"ans":"You should move data to BigQuery and than export it as\u00a0Avro\u00a0or\u00a0Parquet\u00a0files.","val":false},{"ans":"You should move data to BigQuery and than export it as CSV files.","val":false}],"q_expl":"The second option is the correct answer. To export data from a MySQL instance running in Cloud SQL, you should use the\u00a0mysqldump\u00a0utility. This utility exports data from a MySQL database and creates a text file that contains SQL statements that can be used to recreate the database. Once the data is exported to Cloud Storage, you can download it and recreate the database locally. You can follow the detailed instructions provided in the Google Cloud SQL documentation:\u00a0https:\/\/cloud.google.com\/sql\/docs\/mysql\/import-export\/exporting"},{"label":"test_7","q_format":"single","q_text":"Your team has a Dataflow streaming pipeline running with a Pub\/Sub subscription as the source. You need to stop this Dataflow pipeline but also want to finish processing buffered data. What should you do?","answers":[{"ans":"You should cancel this pipeline.","val":false},{"ans":"You cannot stop running pipeline.","val":false},{"ans":"You should delete this pipeline.","val":false},{"ans":"You should drain this pipeline.","val":true}],"q_expl":"When you drain a streaming pipeline, it allows the pipeline to continue processing any data that is already in the pipeline while stopping any new data from entering the pipeline. This means that the pipeline can finish processing any buffered data before shutting down.\nTo drain a pipeline in Dataflow, you can use the\u00a0\u2013drain\u00a0option with the\u00a0gcloud\u00a0command. For example, the command\u00a0gcloud dataflow jobs drain \u00a0would drain the specified Dataflow job with the given ID.\nCanceling a pipeline with the\u00a0gcloud\u00a0command would stop the pipeline immediately, without allowing it to finish processing buffered data. Deleting a pipeline would remove it from the system entirely, which would not allow any further data processing or retrieval.\nReferences:\nGoogle Cloud documentation on stopping a Dataflow job:\u00a0https:\/\/cloud.google.com\/dataflow\/docs\/guides\/using-command-line-intf#stopping-a-job\nGoogle Cloud documentation on draining a Dataflow job:\u00a0https:\/\/cloud.google.com\/dataflow\/docs\/guides\/using-command-line-intf#draining-a-job"},{"label":"test_7","q_format":"single","q_text":"A social media company uses Cloud Spanner for relational data. Their development team often performs complex queries on the database and looks for a solution to simplify these complex queries. What can you recommend?","answers":[{"ans":"They should create triggers for complex queries to simplify query definitions.","val":false},{"ans":"They should create views for complex queries to simplify query definitions.","val":true},{"ans":"They should create more tables for complex queries to simplify query definitions.","val":false},{"ans":"They should create indexes for complex queries to simplify query definitions.","val":false}],"q_expl":"I would recommend that they create views for complex queries to simplify query definitions. Views are virtual tables created by a query and can be used to simplify complex queries by encapsulating complex joins, filtering, and aggregations into a single view. Once a view is created, it can be queried like any other table, making it easy for developers to use in their applications without needing to write complex SQL queries. This can help improve query performance and reduce the likelihood of errors. Cloud Spanner supports views, and you can learn more about them in the official documentation:\u00a0https:\/\/cloud.google.com\/spanner\/docs\/views."},{"label":"test_7","q_format":"single","q_text":"As a Data Engineer in a machine learning company, you need to prepare infrastructure for developing machine learning models using a framework that requires high-precision arithmetic. Your data science team wants to use Jupyter Notebooks to build models interactively and analyze results. The data for machine learning models fits within 32 GB of memory. What should you use?","answers":[{"ans":"You should use a Managed Instance Group (MIG) of 4 VMs with 32 GB of memory.","val":false},{"ans":"You should use a single TPU.","val":false},{"ans":"You should use a single server with a vCPU.","val":false},{"ans":"You should use a single server with a GPU.","val":true}],"q_expl":"You should use a single server with a GPU. GPUs are particularly useful for machine learning tasks that require high-precision arithmetic, and a server with a GPU can handle the 32 GB of memory required for the data. Additionally, using a single server with a GPU is a cost-effective solution and allows for the use of Jupyter Notebooks to build models interactively and analyze results."},{"label":"test_7","q_format":"single","q_text":"Solarex is a Software as a Service (SaaS) company specializing in renewable energy sources, mainly photovoltaics. It collects streaming time series data from tens of thousands solar panels around the world. The solar panels are owned and operated by 150 different companies which are Solarex\u2018s main customers. The data will be stored in Bigtable using a multitenant database \u2013 all customer data will be stored in the same database. The data sent from the solar panel includes a solar panel ID (globally unique), a timestamp and several metrics about performance. Each client will only query their own data. What row key should Solarex use?","answers":[{"ans":"solar panel ID, timestamp, customer ID","val":false},{"ans":"customer ID, solar panel ID, timestamp","val":true},{"ans":"customer ID, timestamp, solar panel ID","val":false},{"ans":"solar panel ID, customer ID, timestamp","val":false}],"q_expl":"Solarex should use the row key as follows:\u00a0customer ID\u00a0+\u00a0solar panel ID\u00a0+\u00a0reverse timestamp.\nThe reason for including the\u00a0customer ID\u00a0in the row key is to enable each customer to query only their own data. Including the\u00a0solar panel ID\u00a0ensures that all metrics from a particular solar panel are stored together and can be queried efficiently.\nThe reverse timestamp is added to the end of the row key to ensure that recent data is stored at the top of the table, which allows for efficient querying of the most recent data.\nHere is a sample code snippet for creating the row key:\npythonCopy code\ncustomer_id = \u2018CUSTOMER_ID\u2018\nsolar_panel_id = \u2018SOLAR_PANEL_ID\u2018\ntimestamp = \u2018TIMESTAMP\u2018\nrow_key = \u2018{}_{}_{}\u2018.format(customer_id, solar_panel_id, str(sys.maxsize \u2013 int(timestamp)))\nReference:\nhttps:\/\/cloud.google.com\/solutions\/bigtable-schema-design-time-series-data"},{"label":"test_7","q_format":"single","q_text":"You are collecting sensor data across the globe. The data ingestion process is losing some data because your servers cannot ingest the data as fast as it is arriving. What can you do to prevent data loss?","answers":[{"ans":"You can write data to Cloud SQL table.","val":false},{"ans":"You can write data to a Dataflow stream.","val":false},{"ans":"You can write data to a Pub\/Sub topic.","val":true},{"ans":"You can write data to Cloud Dataprep.","val":false}],"q_expl":"To prevent data loss due to high ingestion rates, you should write the data to a Pub\/Sub topic. Pub\/Sub can handle large volumes of data and will buffer messages if subscribers are not immediately available to process them. This ensures that the data is not lost even if your servers cannot keep up with the data arrival rate. Dataflow streams can also handle high volumes of data and provide data processing capabilities, but they may not be necessary in this case if the goal is just to prevent data loss. Cloud SQL and Cloud Dataprep are not designed for real-time data ingestion and would not be suitable solutions for preventing data loss due to high ingestion rates."},{"label":"test_7","q_format":"single","q_text":"Business requirements require the use of customer-managed encryption keys (CMEKs) in cloud services. You don\u2018t need keys to reside on your own infrastructure. Which service should you use for key management in Google Cloud?","answers":[{"ans":"Cloud Armor","val":false},{"ans":"Cloud SDK","val":false},{"ans":"Cloud VPC","val":false},{"ans":"Cloud KMS","val":true}],"q_expl":"For key management in Google Cloud, you should use Cloud KMS. Cloud KMS provides a centralized key management system that allows you to manage keys for encrypting data at rest, encrypting other keys, and verifying signatures. It supports CMEKs for most Google Cloud services, allowing you to maintain control over the keys used to protect your data."},{"label":"test_7","q_format":"single","q_text":"You created Bigtable instance with one cluster for analytical purposes. The instance is not performing as expected. Data is continuously streamed from thousands of IoT devices, and statistical analysis programs run continually in a batch. What can you recommend to improve performance?","answers":[{"ans":"You should use a write-optimized operating system on the nodes.","val":false},{"ans":"You should use a read-optimized operating system on the nodes.","val":false},{"ans":"You should add another cluster. Route writes to the first cluster and run batch processing on the second cluster.","val":true},{"ans":"You cannot improve performance in this case.","val":false}],"q_expl":"If a Bigtable instance is not performing as expected, one possible recommendation to improve performance is to add another cluster. This can be done by routing writes to the first cluster and running batch processing on the second cluster, allowing for a more efficient use of resources and balancing the load between the two clusters.\nHowever, adding another cluster is not the only solution and it\u2018s important to first identify the root cause of the performance issue before making any changes. Other factors that could affect performance include the schema design, workload pattern, indexing strategy, and tuning of other system-level parameters. Therefore, it\u2018s important to perform a thorough analysis and consider all possible factors when trying to optimize the performance of a Bigtable instance.\nReferences:\nhttps:\/\/cloud.google.com\/bigtable\/docs\/performance#performance_best_practices\nhttps:\/\/cloud.google.com\/solutions\/best-practices-for-operating-bigtable"},{"label":"test_8","q_format":"single","q_text":"A startup is providing a streaming service for cricket fans around the world. The service will provide both live streams and videos of previously played matches. The architect of the startup wants to ensure all users have the same experience regardless of where they are located. What GCP service could the startup use to help ensure a consistent experience for previously played matches?","answers":[{"ans":"Cloud Storage using Nearline storage","val":false},{"ans":"Cloud Firestore","val":false},{"ans":"Cloud Storage using multiple regions","val":false},{"ans":"Cloud CDN","val":true}],"q_expl":"Cloud CDN is a content delivery network service designed to store copies of data close to end users. Cloud Storage using multiple regions would require more management than Cloud CDN and does not have the automatic caching features of Cloud CDN. Cloud Storage Nearline is for storing objects that are accessed less than once in 30 days. Cloud Firestore is a NoSQL database and not appropriate for storing and streaming videos. See https:\/\/cloud.google.com\/cdn"},{"label":"test_8","q_format":"single","q_text":"A developer is creating a dashboard to monitor a service that uses Cloud Pub\/Sub. They want to know when applications that read data from a pull subscription in Cloud Pub\/Sub are not keeping up with the messages being ingested. What metric would you recommend they monitor?","answers":[{"ans":"subscription\/num_undelivered_messages","val":true},{"ans":"topic\/excess_ingestion_volume","val":false},{"ans":"subscription\/excess_ingestion_volume","val":false},{"ans":"topic\/num_undelivered_messages","val":false}],"q_expl":"The subscription\/num_undelivered_messages is the count of undelivered messages an one metric to indicate how well subscribers are keeping up with ingestion. The metric is tracked for subscriptions not topics. There is no metric called excess_ingestion_rate. See https:\/\/cloud.google.com\/pubsub\/docs\/monitoring"},{"label":"test_9","q_format":"single","q_text":"A data analyst currently has the bigquery.dataViewer role and can successfully query a materialized view. They also want to be able to refresh the materialized view. You want to use a predefined role but not grant them any more permissions than needed to refresh the materialized view. What predefined role would you grant to the user?","answers":[{"ans":"bigquery.dataOwner","val":false},{"ans":"bigquery.admin","val":false},{"ans":"bigquery.dataEditor","val":true},{"ans":"bigquery.mvUpdater","val":false}],"q_expl":"The correct answer is bigquery.dataEditor, which can refresh a materialized view. Both bigquery.admin and bigquery.dataAdmin grant more permissions than needed. There is no bigquery.mvUpdater role. See https:\/\/cloud.google.com\/bigquery\/docs\/access-control"},{"label":"test_9","q_format":"single","q_text":"A database administrator would like to migrate a PostgreSQL database to a managed service in Google Cloud with minimal changes. The database is used by a team of researchers all located in Spain and France. Which of the following services would you recommend?","answers":[{"ans":"Cloud Spanner","val":false},{"ans":"Cloud SQL","val":true},{"ans":"Cloud Bigtable","val":false},{"ans":"Cloud Firestore","val":false}],"q_expl":"Cloud SQL is a regional SQL database managed service that supports PostgreSQL databases. Cloud Spanner is a global scale relational database which would cost more to operate than Cloud SQL. Cloud Bigtable and Cloud Firestore are both NoSQL databases and would not meet the requirements outlined here. See https:\/\/cloud.google.com\/sql\/docs\/postgres"},{"label":"test_10","q_format":"single","q_text":"How can you speed up a Dataflow pipeline that ingests and transforms compressed gzip text files, uses SideInputs to join data, and writes errors to a dead-letter queue?","answers":[{"ans":"Change to using compressed Avro files instead of gzip","val":false},{"ans":"Decrease the batch size used in the pipeline.","val":false},{"ans":"Implement a retry mechanism for records that encounter errors","val":false},{"ans":"Replace the SideInput with CoGroupByKey to improve performance.","val":true}],"q_expl":"The CoGroupByKey transform is a core Beam transform that merges (flattens) multiple PCollection objects and groups elements that have a common key. Unlike a side input, which makes the entire side input data available to each worker, CoGroupByKey performs a shuffle (grouping) operation to distribute data across workers. CoGroupByKey is therefore ideal when the PCollection objects you want to join are very large and don\u2018t fit into worker memory. Use CoGroupByKey if you need to fetch a large proportion of a PCollection object that significantly exceeds worker memory Ref: https:\/\/cloud.google.com\/architecture\/building-production-ready-data-pipelines-using-dataflow-developing-and-testing#choose_correctly_between_side_inputs_or_cogroupbykey_for_joins"},{"label":"test_12","q_format":"single","q_text":"You set up a streaming data insert into a Redis cluster via a Kafka cluster. Both clusters are running on Compute Engine instances. You need to encrypt data at rest with encryption keys that you can create, rotate, and destroy as needed. What should you do?","answers":[{"ans":"A. Create a dedicated service account, and use encryption at rest to reference your data stored in your Compute Engine cluster instances as part of your API service calls.","val":false},{"ans":"B. Create encryption keys in Cloud Key Management Service. Use those keys to encrypt your data in all of the Compute Engine cluster instances.","val":true},{"ans":"C. Create encryption keys locally. Upload your encryption keys to Cloud Key Management Service. Use those keys to encrypt your data in all of the Compute Engine cluster instances.","val":false},{"ans":"D. Create encryption keys in Cloud Key Management Service. Reference those keys in your API service calls when accessing the data in your Compute Engine cluster instances.","val":false}],"q_expl":"A makes no sense, you need to use your own keys. You don\u0092t create keys locally and upload them, you should import it to make it work..using the kms public key\u0085not just \u0093uploading\u0094 it. C is also out. IT\u0092s between B and D Cloud KMS is a cloud-hosted key management service that lets you manage cryptographic keys for your cloud services the same way you do on-premises, You can generate, use, rotate, and destroy cryptographic keys from there. Since you want to encrypt data at rest, is B, you don\u0092t use them for any API calls. https:\/\/cloud.google.com\/compute\/docs\/disks\/customer-managed-encryption"},{"label":"test_12","q_format":"single","q_text":"Suppose you have a dataset of images that are each labeled as to whether or not they contain a human face. To create a neural network that recognizes human faces in images using this labeled dataset, what approach would likely be the most effective?","answers":[{"ans":"A. Use K-means Clustering to detect faces in the pixels.","val":false},{"ans":"B. Use feature engineering to add features for eyes, noses, and mouths to the input data.","val":false},{"ans":"C. Use deep learning by creating a neural network with multiple hidden layers to automatically detect features of faces.","val":true},{"ans":"D. Build a neural network with an input layer of pixels, a hidden layer, and an output layer with two categories.","val":false}],"q_expl":"Traditional machine learning relies on shallow nets, composed of one input and one output layer, and at most one hidden layer in between. More than three layers (including input and output) qualifies as \u201cdeep\u201c learning. So deep is a strictly defined, technical term that means more than one hidden layer. In deep-learning networks, each layer of nodes trains on a distinct set of features based on the previous layers output. The further you advance into the neural net, the more complex the features your nodes can recognize, since they aggregate and recombine features from the previous layer. A neural network with only one hidden layer would be unable to automatically recognize high-level features of faces, such as eyes, because it wouldn\u2018t be able to \u201cbuild\u201c these features using previous hidden layers that detect low-level features, such as lines. Feature engineering is difficult to perform on raw image data. K-means Clustering is an unsupervised learning method used to categorize unlabeled data. Reference: https:\/\/deeplearning4j.org\/neuralnet-overview"},{"label":"test_12","q_format":"single","q_text":"You work for a shipping company that has distribution centers where packages move on delivery lines to route them properly. The company wants to add cameras to the delivery lines to detect and track any visual damage to the packages in transit. You need to create a way to automate the detection of damaged packages and flag them for human review in real time while the packages are in transit. Which solution should you choose?","answers":[{"ans":"A. Use BigQuery machine learning to be able to train the model at scale, so you can analyze the packages in batches.","val":false},{"ans":"B. Train an AutoML model on your corpus of images, and build an API around that model to integrate with the package tracking applications.","val":true},{"ans":"C. Use the Cloud Vision API to detect for damage, and raise an alert through Cloud Functions. Integrate the package tracking applications with this function.","val":false},{"ans":"D. Use TensorFlow to create a model that is trained on your corpus of images. Create a Python notebook in Cloud Datalab that uses this model so you can analyze for damaged packages.","val":false}],"q_expl":"Answer is B. Cloud Vision API detects lot of things for not damages. The description of Damages can be different for each business . So we need to train the model with test and training data to give our definition of Damages, so we need ML capabilities so answer is B, AutoML."},{"label":"test_12","q_format":"single","q_text":"To give a user read permission for only the first three columns of a table, which access control method would you use?","answers":[{"ans":"A. Primitive role","val":false},{"ans":"B. Predefined role","val":false},{"ans":"C. Authorized view","val":true},{"ans":"D. It\u2018s not possible to give access to only the first three columns of a table.","val":false}],"q_expl":"An authorized view allows you to share query results with particular users and groups without giving them read access to the underlying tables. Authorized views can only be created in a dataset that does not contain the tables queried by the view. When you create an authorized view, you use the view\u2018s SQL query to restrict access to only the rows and columns you want the users to see. Reference: https:\/\/cloud.google.com\/bigquery\/docs\/views#authorized-views"},{"label":"test_12","q_format":"single","q_text":"You need to store and analyze social media postings in Google BigQuery at a rate of 10,000 messages per minute in near real-time. Initially, design the application to use streaming inserts for individual postings. Your application also performs data aggregations right after the streaming inserts. You discover that the queries after streaming inserts do not exhibit strong consistency, and reports from the queries might miss in-flight data. How can you adjust your application design?","answers":[{"ans":"A. Re-write the application to load accumulated data every 2 minutes.","val":false},{"ans":"B. Convert the streaming insert code to batch load for individual messages.","val":false},{"ans":"C. Load the original message to Google Cloud SQL, and export the table every hour to BigQuery via streaming inserts.","val":false},{"ans":"D. Estimate the average latency for data availability after streaming inserts, and always run queries after waiting twice as long.","val":true}],"q_expl":"Answer: D Description: The data is first comes to buffer and then written to Storage. If we are running queries in buffer we will face above mentioned issues. If we wait for the bigquery to write the data to storage then we won\u0092t face the issue. So We need to wait till it\u0092s written tio storage"},{"label":"test_12","q_format":"multiple","q_text":"As your organization expands its usage of GCP, many teams have started to create their own projects. Projects are further multiplied to accommodate different stages of deployments and target audiences. Each project requires unique access control configurations. The central IT team needs to have access to all projects.Furthermore, data from Cloud Storage buckets and BigQuery datasets must be shared for use in other projects in an ad hoc way. You want to simplify access control management by minimizing the number of policies. Which two steps should you take? (Choose two.)","answers":[{"ans":"A. Use Cloud Deployment Manager to automate access provision.","val":false},{"ans":"B. Introduce resource hierarchy to leverage access control policy inheritance.","val":true},{"ans":"C. Create distinct groups for various teams, and specify groups in Cloud IAM policies.","val":true},{"ans":"D. Only use service accounts when sharing data for Cloud Storage buckets and BigQuery datasets.","val":false},{"ans":"E. For each Cloud Storage bucket or BigQuery dataset, decide which projects need access. Find all the active members who have access to these projects, and create a Cloud IAM policy to grant access to all these users.","val":false}],"q_expl":"B & C Google Cloud resources are organized hierarchically, where the organization node is the root node in the hierarchy, the projects are the children of the organization, and the other resources are descendents of projects. You can set Cloud Identity and Access Management (Cloud IAM) policies at different levels of the resource hierarchy. Resources inherit the policies of the parent resource. The effective policy for a resource is the union of the policy set at that resource and the policy inherited from its parent. https:\/\/cloud.google.com\/iam\/docs\/resource-hierarchy-access-control"},{"label":"test_12","q_format":"single","q_text":"Which Java SDK class can you use to run your Dataflow programs locally?","answers":[{"ans":"A. LocalRunner","val":false},{"ans":"B. DirectPipelineRunner","val":true},{"ans":"C. MachineRunner","val":false},{"ans":"D. LocalPipelineRunner","val":false}],"q_expl":"DirectPipelineRunner allows you to execute operations in the pipeline directly, without any optimization. Useful for small local execution and tests Reference: https:\/\/cloud.google.com\/dataflow\/java-sdk\/JavaDoc\/com\/google\/cloud\/dataflow\/sdk\/runners\/DirectPipelineRunner"},{"label":"test_12","q_format":"single","q_text":"You work for a car manufacturer and have set up a data pipeline using Google Cloud Pub\/Sub to capture anomalous sensor events. You are using a push subscription in Cloud Pub\/Sub that calls a custom HTTPS endpoint that you have created to take action of these anomalous events as they occur. Your customHTTPS endpoint keeps getting an inordinate amount of duplicate messages. What is the most likely cause of these duplicate messages?","answers":[{"ans":"A. The message body for the sensor event is too large.","val":false},{"ans":"B. Your custom endpoint has an out-of-date SSL certificate.","val":false},{"ans":"C. The Cloud Pub\/Sub topic has too many messages published to it.","val":false},{"ans":"D. Your custom endpoint is not acknowledging messages within the acknowledgement deadline.","val":true}],"q_expl":"https:\/\/cloud.google.com\/pubsub\/docs\/troubleshooting#dupes"},{"label":"test_12","q_format":"single","q_text":"Your company is performing data preprocessing for a learning algorithm in Google Cloud Dataflow. Numerous data logs are being are being generated during this step, and the team wants to analyze them. Due to the dynamic nature of the campaign, the data is growing exponentially every hour.The data scientists have written the following code to read the data for a new key features in the logs.BigQueryIO.Read -.named(\u201cReadLogData\u201c).from(\u201cclouddataflow-readonly:samples.log_data\u201c)You want to improve the performance of this data read. What should you do?","answers":[{"ans":"A. Specify the TableReference object in the code.","val":false},{"ans":"B. Use .fromQuery operation to read specific fields from the table.","val":true},{"ans":"C. Use of both the Google BigQuery TableSchema and TableFieldSchema classes.","val":false},{"ans":"D. Call a transform that returns TableRow objects, where each element in the PCollection represents a single row in the table.","val":false}],"q_expl":"B BigQueryIO.read.from() directly reads the whole table from BigQuery. This function exports the whole table to temporary files in Google Cloud Storage, where it will later be read from. This requires almost no computation, as it only performs an export job, and later Dataflow reads from GCS (not from BigQuery). BigQueryIO.read.fromQuery() executes a query and then reads the results received after the query execution. Therefore, this function is more time-consuming, given that it requires that a query is first executed (which will incur in the corresponding economic and computational costs)."},{"label":"test_12","q_format":"single","q_text":"Which of these statements about BigQuery caching is true?","answers":[{"ans":"A. By default, a query\u2018s results are not cached.","val":false},{"ans":"B. BigQuery caches query results for 48 hours.","val":false},{"ans":"C. Query results are cached even if you specify a destination table.","val":false},{"ans":"D. There is no charge for a query that retrieves its results from cache.","val":true}],"q_expl":"When query results are retrieved from a cached results table, you are not charged for the query. BigQuery caches query results for 24 hours, not 48 hours. Query results are not cached if you specify a destination table. A query\u2018s results are always cached except under certain conditions, such as if you specify a destination table. Reference: https:\/\/cloud.google.com\/bigquery\/querying-data#query-caching"},{"label":"test_12","q_format":"single","q_text":"You are a head of BI at a large enterprise company with multiple business units that each have different priorities and budgets. You use on-demand pricing forBigQuery with a quota of 2K concurrent on-demand slots per project. Users at your organization sometimes don\u2018t get slots to execute their query and you need to correct this. You\u2018d like to avoid introducing new projects to your account.What should you do?","answers":[{"ans":"A. Convert your batch BQ queries into interactive BQ queries.","val":false},{"ans":"B. Create an additional project to overcome the 2K on-demand per-project quota.","val":false},{"ans":"C. Switch to flat-rate pricing and establish a hierarchical priority model for your projects.","val":true},{"ans":"D. Increase the amount of concurrent slots per project at the Quotas page at the Cloud Console.","val":false}],"q_expl":"C. Switch to flat-rate pricing and establish a hierarchical priority model for your resources \nHere\u2018s why this approach is effective:\nFlat-Rate Pricing: Switching from on-demand pricing to flat-rate pricing for BigQuery eliminates the concerns about users not having enough concurrent on-demand slots to execute queries. With flat-rate pricing, you prepay for a set amount of resources, ensuring predictability and avoiding the issue of slot limitations.\nHierarchical Priority Model: To address the varying priorities of different business units within your organization, you can establish a hierarchical priority model for your resources. This might involve assigning different tiers to projects based on their criticality. Higher tiers could receive a larger allocation of resources within the flat-rate quota, ensuring that crucial business activities are not hindered.\nOther options and why they are less suitable:\nA. Convert batch queries to interactive: While converting batch queries to interactive can improve responsiveness, it wouldn\u2018t solve the underlying issue of limited on-demand slots. Interactive queries also tend to consume more resources compared to batch queries.\nB. Create a new project: Introducing a new project just to overcome the quota limitation creates additional management overhead and wouldn\u2018t address the need to prioritize resource allocation between different business units.\nD. Increase concurrent slots: While increasing the quota for concurrent slots might alleviate the immediate issue, it wouldn\u2018t address the cost concerns associated with on-demand pricing. Additionally, even with a higher quota, resource contention could still occur if queries are particularly demanding."},{"label":"test_12","q_format":"single","q_text":"You are designing an Apache Beam pipeline to enrich data from Cloud Pub\/Sub with static reference data from BigQuery. The reference data is small enough to fit in memory on a single worker. The pipeline should write enriched results to BigQuery for analysis. Which job type and transforms should this pipeline use?","answers":[{"ans":"A. Batch job, PubSubIO, side-inputs","val":false},{"ans":"B. Streaming job, PubSubIO, JdbcIO, side-outputs","val":false},{"ans":"C. Streaming job, PubSubIO, BigQueryIO, side-inputs","val":true},{"ans":"D. Streaming job, PubSubIO, BigQueryIO, side-outputs","val":false}],"q_expl":"Answer is C, You need pubsubIO and BigQueryIO for streaming data and writing enriched data back to BigQuery. side-inputs are a way to enrich the data https:\/\/cloud.google.com\/architecture\/e-commerce\/patterns\/slow-updating-side-inputs"},{"label":"test_12","q_format":"single","q_text":"How would you query specific partitions in a BigQuery table?","answers":[{"ans":"A. Use the DAY column in the WHERE clause","val":false},{"ans":"B. Use the EXTRACT(DAY) clause","val":false},{"ans":"C. Use the __PARTITIONTIME pseudo-column in the WHERE clause","val":true},{"ans":"D. Use DATE BETWEEN in the WHERE clause","val":false}],"q_expl":"Partitioned tables include a pseudo column named _PARTITIONTIME that contains a date-based timestamp for data loaded into the table. To limit a query to particular partitions (such as Jan 1st and 2nd of 2017), use a clause similar to this: WHERE _PARTITIONTIME BETWEEN TIMESTAMP(\u20182017-01-01\u2018) AND TIMESTAMP(\u20182017-01-02\u2018) Reference: https:\/\/cloud.google.com\/bigquery\/docs\/partitioned-tables#the_partitiontime_pseudo_column"},{"label":"test_12","q_format":"single","q_text":"You operate a logistics company, and you want to improve event delivery reliability for vehicle-based sensors. You operate small data centers around the world to capture these events, but leased lines that provide connectivity from your event collection infrastructure to your event processing infrastructure are unreliable, with unpredictable latency. You want to address this issue in the most cost-effective way. What should you do?","answers":[{"ans":"A. Deploy small Kafka clusters in your data centers to buffer events.","val":false},{"ans":"B. Have the data acquisition devices publish data to Cloud Pub\/Sub.","val":false},{"ans":"C. Establish a Cloud Interconnect between all remote data centers and Google.","val":true},{"ans":"D. Write a Cloud Dataflow pipeline that aggregates all data in session windows.","val":false}],"q_expl":"C. This is a tricky one. The issue here is the unreliable connection between data collection and data processing infrastructure, and to resolve it in a cost-effective manner. However, it also mentions that the company is using leased lines. I think replacing the leased lines with Cloud InterConnect would solve the problem, and hopefully not be an added expense. https:\/\/cloud.google.com\/interconnect\/docs\/concepts\/overview"},{"label":"test_12","q_format":"single","q_text":"MJTelco Case Study -Company Overview -MJTelco is a startup that plans to build networks in rapidly growing, underserved markets around the world. The company has patents for innovative optical communications hardware. Based on these patents, they can create many reliable, high-speed backbone links with inexpensive hardware.Company Background -Founded by experienced telecom executives, MJTelco uses technologies originally developed to overcome communications challenges in space. Fundamental to their operation, they need to create a distributed data infrastructure that drives real-time analysis and incorporates machine learning to continuously optimize their topologies. Because their hardware is inexpensive, they plan to overdeploy the network allowing them to account for the impact of dynamic regional politics on location availability and cost.Their management and operations teams are situated all around the globe creating many-to-many relationship between data consumers and provides in their system. After careful consideration, they decided public cloud is the perfect environment to support their needs.Solution Concept -MJTelco is running a successful proof-of-concept (PoC) project in its labs. They have two primary needs:? Scale and harden their PoC to support significantly more data flows generated when they ramp to more than 50,000 installations.? Refine their machine-learning cycles to verify and improve the dynamic models they use to control topology definition.MJTelco will also use three separate operating environments \u201c\u201c development\/test, staging, and production \u201c\u201c to meet the needs of running experiments, deploying new features, and serving production customers.Business Requirements -? Scale up their production environment with minimal cost, instantiating resources when and where needed in an unpredictable, distributed telecom user community.? Ensure security of their proprietary data to protect their leading-edge machine learning and analysis.? Provide reliable and timely access to data for analysis from distributed research workers? Maintain isolated environments that support rapid iteration of their machine-learning models without affecting their customers.Technical Requirements -Ensure secure and efficient transport and storage of telemetry dataRapidly scale instances to support between 10,000 and 100,000 data providers with multiple flows each.Allow analysis and presentation against data tables tracking up to 2 years of data storing approximately 100m records\/daySupport rapid iteration of monitoring infrastructure focused on awareness of data pipeline problems both in telemetry flows and in production learning cycles.CEO Statement -Our business model relies on our patents, analytics and dynamic machine learning. Our inexpensive hardware is organized to be highly reliable, which gives us cost advantages. We need to quickly stabilize our large distributed data pipelines to meet our reliability and capacity commitments.CTO Statement -Our public cloud services must operate as advertised. We need resources that scale and keep our data secure. We also need environments in which our data scientists can carefully study and quickly adapt our models. Because we rely on automation to process our data, we also need our development and test environments to work as we iterate.CFO Statement -The project is too large for us to maintain the hardware and software required for the data and analysis. Also, we cannot afford to staff an operations team to monitor so many data feeds, so we will rely on automation and infrastructure. Google Cloud\u2018s machine learning will allow our quantitative researchers to work on our high-value problems instead of problems with our data pipelines.MJTelco is building a custom interface to share data. They have these requirements:1. They need to do aggregations over their petabyte-scale datasets.2. They need to scan specific time range rows with a very fast response time (milliseconds).Which combination of Google Cloud Platform products should you recommend?","answers":[{"ans":"A. Cloud Datastore and Cloud Bigtable","val":false},{"ans":"B. Cloud Bigtable and Cloud SQL","val":false},{"ans":"C. BigQuery and Cloud Bigtable","val":true},{"ans":"D. BigQuery and Cloud Storage","val":false}],"q_expl":"BigQuery and Cloud Bigtable :\nBigQuery: Ideal for massive data analysis (petabyte scale), aggregations, and machine learning.\nCloud Bigtable: Enables real-time data ingestion, low-latency retrieval (milliseconds), and acts as a staging area for BigQuery.\nOther options are less suitable due to limitations in scalability, query complexity, or real-time data access.\nGCP\u2018s project partitioning and managed services like Cloud Dataflow address MJTelco\u2018s needs for isolated environments and data pipeline automation.\nThis combination offers scalability, security, and agility for MJTelco\u2018s real-time analytics, machine learning, and data management."},{"label":"test_12","q_format":"multiple","q_text":"You need to create a data pipeline that copies time-series transaction data so that it can be queried from within BigQuery by your data science team for analysis.Every hour, thousands of transactions are updated with a new status. The size of the intitial dataset is 1.5 PB, and it will grow by 3 TB per day. The data is heavily structured, and your data science team will build machine learning models based on this data. You want to maximize performance and usability for your data science team. Which two strategies should you adopt? (Choose two.)","answers":[{"ans":"A. Denormalize the data as must as possible.","val":true},{"ans":"B. Preserve the structure of the data as much as possible.","val":false},{"ans":"C. Use BigQuery UPDATE to further reduce the size of the dataset.","val":false},{"ans":"D. Develop a data pipeline where status updates are appended to BigQuery instead of updated.","val":true},{"ans":"E. Copy a daily snapshot of transaction data to Cloud Storage and store it as an Avro file. Use BigQuery\u2018s support for external data sources to query.","val":false}],"q_expl":"A: Denormalization increases query speed for tables with billions of rows because BigQuery\u2018s performance degrades when doing JOINs on large tables, but with a denormalized data structure, you don\u2018t have to use JOINs, since all of the data has been combined into one table. Denormalization also makes queries simpler because you do not have to use JOIN clauses. https:\/\/cloud.google.com\/solutions\/bigquery-data-warehouse#denormalizing_data D: BigQuery append"},{"label":"test_12","q_format":"single","q_text":"You need to create a near real-time inventory dashboard that reads the main inventory tables in your BigQuery data warehouse. Historical inventory data is stored as inventory balances by item and location. You have several thousand updates to inventory every hour. You want to maximize performance of the dashboard and ensure that the data is accurate. What should you do?","answers":[{"ans":"A. Leverage BigQuery UPDATE statements to update the inventory balances as they are changing.","val":true},{"ans":"B. Partition the inventory balance table by item to reduce the amount of data scanned with each inventory update.","val":false},{"ans":"C. Use the BigQuery streaming the stream changes into a daily inventory movement table. Calculate balances in a view that joins it to the historical inventory balance table. Update the inventory balance table nightly.","val":false},{"ans":"D. Use the BigQuery bulk loader to batch load inventory changes into a daily inventory movement table. Calculate balances in a view that joins it to the historical inventory balance table. Update the inventory balance table nightly.","val":false}],"q_expl":"DML without limits: https:\/\/cloud.google.com\/blog\/products\/data-analytics\/dml-without-limits-now-in-bigquery"},{"label":"test_12","q_format":"single","q_text":"By default, which of the following windowing behavior does Dataflow apply to unbounded data sets?","answers":[{"ans":"A. Windows at every 100 MB of data","val":false},{"ans":"B. Single, Global Window","val":true},{"ans":"C. Windows at every 1 minute","val":false},{"ans":"D. Windows at every 10 minutes","val":false}],"q_expl":"Dataflow\u2018s default windowing behavior is to assign all elements of a PCollection to a single, global window, even for unbounded PCollections Reference: https:\/\/cloud.google.com\/dataflow\/model\/pcollection"},{"label":"test_12","q_format":"single","q_text":"A data scientist has created a BigQuery ML model and asks you to create an ML pipeline to serve predictions. You have a REST API application with the requirement to serve predictions for an individual user ID with latency under 100 milliseconds. You use the following query to generate predictions: SELECT predicted_label, user_id FROM ML.PREDICT (MODEL \u201c\u0098dataset.model\u2018, table user_features). How should you create the ML pipeline?","answers":[{"ans":"A. Add a WHERE clause to the query, and grant the BigQuery Data Viewer role to the application service account.","val":false},{"ans":"B. Create an Authorized View with the provided query. Share the dataset that contains the view with the application service account.","val":false},{"ans":"C. Create a Cloud Dataflow pipeline using BigQueryIO to read results from the query. Grant the Dataflow Worker role to the application service account.","val":false},{"ans":"D. Create a Cloud Dataflow pipeline using BigQueryIO to read predictions for all users from the query. Write the results to Cloud Bigtable using BigtableIO. Grant the Bigtable Reader role to the application service account so that the application can read predictions for individual users from Cloud Bigtable.","val":true}],"q_expl":"To create an ML pipeline that serves predictions for an individual user ID with latency under 100 milliseconds, you should D. Create a Cloud Dataflow pipeline using BigQueryIO to read predictions for all users from the query. Write the results to Cloud Bigtable using BigtableIO. Grant the Bigtable Reader role to the application service account so that the application can read predictions for individual users from Cloud Bigtable.\nThis approach ensures that the pipeline can efficiently handle requests for individual user IDs by storing all predictions in Cloud Bigtable, which allows for fast and efficient retrieval of predictions."},{"label":"test_12","q_format":"single","q_text":"A shipping company has live package-tracking data that is sent to an Apache Kafka stream in real time. This is then loaded into BigQuery. Analysts in your company want to query the tracking data in BigQuery to analyze geospatial trends in the lifecycle of a package. The table was originally created with ingest-date partitioning. Over time, the query processing time has increased. You need to implement a change that would improve query performance in BigQuery. What should you do?","answers":[{"ans":"A. Implement clustering in BigQuery on the ingest date column.","val":false},{"ans":"B. Implement clustering in BigQuery on the package-tracking ID column.","val":true},{"ans":"C. Tier older data onto Cloud Storage files, and leverage extended tables.","val":false},{"ans":"D. Re-create the table using data partitioning on the package delivery date.","val":false}],"q_expl":"B. Implement clustering in BigQuery on the package-tracking ID column \nHere\u2018s why this approach is ideal:\nGeospatial Trends: Clustering based on the package-tracking ID groups together data points associated with the same package. Since queries for geospatial trends likely focus on individual package journeys, clustering on this column ensures data relevant to a specific package is physically stored closer together. This can significantly improve query speed when analyzing trends for specific packages.\nOther options and why they are less suitable:\nA. Ingest Date Clustering: While clustering on the ingest date might improve performance for queries focused on temporal trends (e.g., analyzing trends across different days), it wouldn\u2018t be as effective for geospatial analysis based on package IDs.\nC. Tiering & Extended Tables: Tiering older data to Cloud Storage and using BigQuery\u2018s external tables can be a good strategy for managing costs and reducing BigQuery storage usage. However, it wouldn\u2018t directly improve query performance on actively used data for geospatial analysis.\nD. Re-partitioning: Re-creating the table with data partitioning on the delivery date might be helpful if queries primarily focus on delivery trends. However, for geospatial analysis, clustering on the package-tracking ID provides a more targeted performance improvement.\nBy clustering on the package-tracking ID column, BigQuery can optimize data storage and retrieval for queries that analyze geospatial trends associated with individual packages. This significantly reduces query processing time and enhances the responsiveness of your analytics."},{"label":"test_12","q_format":"single","q_text":"Which methods can be used to reduce the number of rows processed by BigQuery?","answers":[{"ans":"A. Splitting tables into multiple tables; putting data in partitions","val":true},{"ans":"B. Splitting tables into multiple tables; putting data in partitions; using the LIMIT clause","val":false},{"ans":"C. Putting data in partitions; using the LIMIT clause","val":false},{"ans":"D. Splitting tables into multiple tables; using the LIMIT clause","val":false}],"q_expl":"If you split a table into multiple tables (such as one table for each day), then you can limit your query to the data in specific tables (such as for particular days). A better method is to use a partitioned table, as long as your data can be separated by the day. If you use the LIMIT clause, BigQuery will still process the entire table. Reference: https:\/\/cloud.google.com\/bigquery\/docs\/partitioned-tables"},{"label":"test_12","q_format":"single","q_text":"MJTelco Case Study -Company Overview -MJTelco is a startup that plans to build networks in rapidly growing, underserved markets around the world. The company has patents for innovative optical communications hardware. Based on these patents, they can create many reliable, high-speed backbone links with inexpensive hardware.Company Background -Founded by experienced telecom executives, MJTelco uses technologies originally developed to overcome communications challenges in space. Fundamental to their operation, they need to create a distributed data infrastructure that drives real-time analysis and incorporates machine learning to continuously optimize their topologies. Because their hardware is inexpensive, they plan to overdeploy the network allowing them to account for the impact of dynamic regional politics on location availability and cost.Their management and operations teams are situated all around the globe creating many-to-many relationship between data consumers and provides in their system. After careful consideration, they decided public cloud is the perfect environment to support their needs.Solution Concept -MJTelco is running a successful proof-of-concept (PoC) project in its labs. They have two primary needs:? Scale and harden their PoC to support significantly more data flows generated when they ramp to more than 50,000 installations.? Refine their machine-learning cycles to verify and improve the dynamic models they use to control topology definition.MJTelco will also use three separate operating environments \u201c\u201c development\/test, staging, and production \u201c\u201c to meet the needs of running experiments, deploying new features, and serving production customers.Business Requirements -? Scale up their production environment with minimal cost, instantiating resources when and where needed in an unpredictable, distributed telecom user community.? Ensure security of their proprietary data to protect their leading-edge machine learning and analysis.? Provide reliable and timely access to data for analysis from distributed research workers? Maintain isolated environments that support rapid iteration of their machine-learning models without affecting their customers.Technical Requirements -? Ensure secure and efficient transport and storage of telemetry data? Rapidly scale instances to support between 10,000 and 100,000 data providers with multiple flows each.? Allow analysis and presentation against data tables tracking up to 2 years of data storing approximately 100m records\/day? Support rapid iteration of monitoring infrastructure focused on awareness of data pipeline problems both in telemetry flows and in production learning cycles.CEO Statement -Our business model relies on our patents, analytics and dynamic machine learning. Our inexpensive hardware is organized to be highly reliable, which gives us cost advantages. We need to quickly stabilize our large distributed data pipelines to meet our reliability and capacity commitments.CTO Statement -Our public cloud services must operate as advertised. We need resources that scale and keep our data secure. We also need environments in which our data scientists can carefully study and quickly adapt our models. Because we rely on automation to process our data, we also need our development and test environments to work as we iterate.CFO Statement -The project is too large for us to maintain the hardware and software required for the data and analysis. Also, we cannot afford to staff an operations team to monitor so many data feeds, so we will rely on automation and infrastructure. Google Cloud\u2018s machine learning will allow our quantitative researchers to work on our high-value problems instead of problems with our data pipelines.MJTelco\u2018s Google Cloud Dataflow pipeline is now ready to start receiving data from the 50,000 installations. You want to allow Cloud Dataflow to scale its compute power up as required. Which Cloud Dataflow pipeline configuration setting should you update?","answers":[{"ans":"A. The zone","val":false},{"ans":"B. The number of workers","val":true},{"ans":"C. The disk size per worker","val":false},{"ans":"D. The maximum number of workers","val":false}],"q_expl":"Answer: B A. There is no indication that the application can do this. Moreover, due to networking problems, it is possible that Pub\/Sub doesn\u2018t receive messages in order. This will analysis difficult. B. This makes sure that you have access to publishing timestamp which provides you with the correct ordering of messages. C. If timestamps are already messed up, BigQuery will get wrong results anyways. D. The timestamp we are interested in is when the data was produced by the publisher, not when it was received by Pub\/Sub."},{"label":"test_12","q_format":"single","q_text":"Your company is using WILDCARD tables to query data across multiple tables with similar names. The SQL statement is currently failing with the following error:\n\n    Which table name will make the SQL statement work correctly?","answers":[{"ans":"'bigquery-public-data.noaa_gsod.gsod'","val":false},{"ans":"bigquery-public-data.noaa_gsod.gsod*","val":false},{"ans":"'bigquery-public-data.noaa_gsod.gsod'*","val":false},{"ans":"'bigquery-public-data.noaa_gsod.gsod*`","val":true}],"q_expl":"Reference:\nhttps:\/\/cloud.google.com\/bigquery\/docs\/wildcard-tables"},{"label":"test_12","q_format":"multiple","q_text":"You are training a spam classifier. You notice that you are overfitting the training data. Which three actions can you take to resolve this problem? (Choose three.)","answers":[{"ans":"A. Get more training examples","val":true},{"ans":"B. Reduce the number of training examples","val":false},{"ans":"C. Use a smaller set of features","val":true},{"ans":"D. Use a larger set of features","val":false},{"ans":"E. Increase the regularization parameters","val":true},{"ans":"F. Decrease the regularization parameters","val":false}],"q_expl":"To address the issue of overfitting in a spam classifier, the three actions you can take are:\n\nA. Get more training examples\nIncreasing the amount of training data can help the model generalize better and reduce overfitting by providing more diverse examples.\nC. Use a smaller set of features\nReducing the number of features can help simplify the model, which may decrease its capacity to overfit the training data.\nE. Increase the regularization parameters\nIncreasing regularization can help penalize overly complex models, thus reducing overfitting by discouraging the model from fitting noise in the training data.\n\nThese actions collectively contribute to improving the model\u2019s performance on unseen data by mitigating overfitting."},{"label":"test_12","q_format":"multiple","q_text":"Which of these operations can you perform from the BigQuery Web UI?","answers":[{"ans":"A. Upload a file in SQL format.","val":false},{"ans":"B. Load data with nested and repeated fields.","val":true},{"ans":"C. Upload a 20 MB file.","val":true},{"ans":"D. Upload multiple files using a wildcard.","val":false}],"q_expl":"The correct answers are:\n\nB. Load data with nested and repeated fields.\nC. Upload a 20 MB file.\n\nHere\u2019s a breakdown of why:\n\nB. Load data with nested and repeated fields: BigQuery Web UI allows you to load data with nested and repeated fields directly from CSV or JSON files.\nC. Upload a 20 MB file: BigQuery Web UI allows you to upload files up to 20 MB in size.\n\nThe other options are not possible:\n\nA. Upload a file in SQL format: BigQuery Web UI does not support uploading files in SQL format. You can create tables and load data using SQL queries directly in the BigQuery console.\nD. Upload multiple files using a wildcard: BigQuery Web UI does not support uploading multiple files using a wildcard. You can use the bq load command-line tool or the BigQuery API to load multiple files.\n\nTherefore, the operations that can be performed from the BigQuery Web UI are loading data with nested and repeated fields and uploading files up to 20 MB in size."},{"label":"test_12","q_format":"single","q_text":"You need to choose a database for a new project that has the following requirements:? Fully managed? Able to automatically scale up? Transactionally consistent? Able to scale up to 6 TB? Able to be queried using SQLWhich database do you choose?","answers":[{"ans":"A. Cloud SQL","val":false},{"ans":"B. Cloud Bigtable","val":false},{"ans":"C. Cloud Spanner","val":true},{"ans":"D. Cloud Datastore","val":false}],"q_expl":"The best database for the given requirements is:\nC. Cloud Spanner\nHere\u2019s a breakdown of why:\n\nFully managed: Cloud Spanner is a fully managed database service, meaning Google handles the underlying infrastructure and maintenance.\nAutomatically scale up: Cloud Spanner can automatically scale up to handle increased workloads, ensuring high availability and performance.\nTransactionally consistent: Cloud Spanner is a strongly consistent database, meaning it guarantees that all transactions are executed in a consistent manner.\nScale up to 6 TB: Cloud Spanner can scale up to handle large datasets, including up to 6 TB.\nQueried using SQL: Cloud Spanner uses a SQL-like query language, making it easy for developers to work with.\n\nThe other options are not as suitable:\n\nCloud SQL: While Cloud SQL is a fully managed relational database service, it might not be able to handle the large dataset and high transaction rates required for the project.\nCloud Bigtable: Cloud Bigtable is a NoSQL database that is optimized for large-scale, high-performance analytics workloads. It might not be suitable for transactional workloads or SQL-based queries.\nCloud Datastore: Cloud Datastore is a NoSQL database that is optimized for storing and retrieving individual entities. It might not be suitable for complex queries or large datasets.\n\nTherefore, Cloud Spanner is the best choice for the given requirements, as it offers a fully managed, scalable, transactionally consistent, and SQL-compatible database solution."},{"label":"test_12","q_format":"single","q_text":"You have a job that you want to cancel. It is a streaming pipeline, and you want to ensure that any data that is in-flight is processed and written to the output.Which of the following commands can you use on the Dataflow monitoring console to stop the pipeline job?","answers":[{"ans":"A. Cancel","val":false},{"ans":"B. Drain","val":true},{"ans":"C. Stop","val":false},{"ans":"D. Finish","val":false}],"q_expl":"Using the Drain option to stop your job tells the Dataflow service to finish your job in its current state. Your job will immediately stop ingesting new data from input sources, but the Dataflow service will preserve any existing resources (such as worker instances) to finish processing and writing any buffered data in your pipeline. Reference: https:\/\/cloud.google.com\/dataflow\/pipelines\/stopping-a-pipeline"},{"label":"test_12","q_format":"single","q_text":"Your company uses a proprietary system to send inventory data every 6 hours to a data ingestion service in the cloud. Transmitted data includes a payload of several fields and the timestamp of the transmission. If there are any concerns about a transmission, the system re-transmits the data. How should you deduplicate the data most efficiency?","answers":[{"ans":"A. Assign global unique identifiers (GUID) to each data entry.","val":true},{"ans":"B. Compute the hash value of each data entry, and compare it with all historical data.","val":false},{"ans":"C. Store each data entry as the primary key in a separate database and apply an index.","val":false},{"ans":"D. Maintain a database table to store the hash value and other metadata for each data entry.","val":false}],"q_expl":"The best answer is \u201cA\u201c. Answer \u201cD\u201c is not as efficient or error-proof due to two reasons 1. You need to calculate hash at sender as well as at receiver end to do the comparison. Waste of computing power. 2. Even if we discount the computing power, we should note that the system is sending inventory information. Two messages sent at different can denote same inventory level (and thus have same hash). Adding sender time stamp to hash will defeat the purpose of using hash as now retried messages will have different timestamp and a different hash. if timestamp is used as message creation timestamp than that can also be used as a UUID."},{"label":"test_12","q_format":"single","q_text":"Flowlogistic Case Study \u2013 Company Overview \u2013 Flowlogistic is a leading logistics and supply chain provider. They help businesses throughout the world manage their resources and transport them to their final destination. The company has grown rapidly, expanding their offerings to include rail, truck, aircraft, and oceanic shipping. Company Background \u2013 The company started as a regional trucking company, and then expanded into other logistics market. Because they have not updated their infrastructure, managing and tracking orders and shipments has become a bottleneck. To improve operations, Flowlogistic developed proprietary technology for tracking shipments in real time at the parcel level. However, they are unable to deploy it because their technology stack, based on Apache Kafka, cannot support the processing volume. In addition, Flowlogistic wants to further analyze their orders and shipments to determine how best to deploy their resources. Solution Concept \u2013 Flowlogistic wants to implement two concepts using the cloud: ? Use their proprietary technology in a real-time inventory-tracking system that indicates the location of their loads ? Perform analytics on all their orders and shipment logs, which contain both structured and unstructured data, to determine how best to deploy resources, which markets to expand info. They also want to use predictive analytics to learn earlier when a shipment will be delayed. Existing Technical Environment \u2013 Flowlogistic architecture resides in a single data center: ? Databases 8 physical servers in 2 clusters \u2013 SQL Server \u201c\u201c user data, inventory, static data 3 physical servers \u2013 Cassandra \u201c\u201c metadata, tracking messages 10 Kafka servers \u201c\u201c tracking message aggregation and batch insert ? Application servers \u201c\u201c customer front end, middleware for order\/customs 60 virtual machines across 20 physical servers \u2013 Tomcat \u201c\u201c Java services \u2013 Nginx \u201c\u201c static content \u2013 Batch servers ? Storage appliances \u2013 iSCSI for virtual machine (VM) hosts \u2013 Fibre Channel storage area network (FC SAN) \u201c\u201c SQL server storage \u2013 Network-attached storage (NAS) image storage, logs, backups ? 10 Apache Hadoop \/Spark servers \u2013 Core Data Lake \u2013 Data analysis workloads ? 20 miscellaneous servers \u2013 Jenkins, monitoring, bastion hosts, Business Requirements \u2013 ? Build a reliable and reproducible environment with scaled panty of production. ? Aggregate data in a centralized Data Lake for analysis ? Use historical data to perform predictive analytics on future shipments ? Accurately track every shipment worldwide using proprietary technology ? Improve business agility and speed of innovation through rapid provisioning of new resources ? Analyze and optimize architecture for performance in the cloud Migrate fully to the cloud if all other requirements are met Technical Requirements \u2013 ? Handle both streaming and batch data ? Migrate existing Hadoop workloads ? Ensure architecture is scalable and elastic to meet the changing demands of the company. ? Use managed services whenever possible ? Encrypt data flight and at rest ? Connect a VPN between the production data center and cloud environment SEO Statement \u2013 We have grown so quickly that our inability to upgrade our infrastructure is really hampering further growth and efficiency. We are efficient at moving shipments around the world, but we are inefficient at moving data around. We need to organize our information so we can more easily understand where our customers are and what they are shipping. CTO Statement \u2013 IT has never been a priority for us, so as our data has grown, we have not invested enough in our technology. I have a good staff to manage IT, but they are so busy managing our infrastructure that I cannot get them to do the things that really matter, such as organizing our data, building the analytics, and figuring out how to implement the CFO\u2018 s tracking technology. CFO Statement \u2013 Part of our competitive advantage is that we penalize ourselves for late shipments and deliveries. Knowing where out shipments are at all times has a direct correlation to our bottom line and profitability. Additionally, I don\u2018t want to commit capital to building out a server environment. Flowlogistic\u2018s management has determined that the current Apache Kafka servers cannot handle the data volume for their real-time inventory tracking system. You need to build a new system on Google Cloud Platform (GCP) that will feed the proprietary tracking software. The system must be able to ingest data from a variety of global sources, process and query in real-time, and store the data reliably. Which combination of GCP products should you choose?","answers":[{"ans":"A. Cloud Pub\/Sub, Cloud Dataflow, and Cloud Storage","val":true},{"ans":"B. Cloud Pub\/Sub, Cloud Dataflow, and Local SSD","val":false},{"ans":"C. Cloud Pub\/Sub, Cloud SQL, and Cloud Storage","val":false},{"ans":"D. Cloud Load Balancing, Cloud Dataflow, and Cloud Storage","val":false}],"q_expl":"Vote for \u2018A\u2018 But, it looks, Flowlogistic Case Study now removed from syllabus of PDE https:\/\/cloud.google.com\/certification\/guides\/data-engineer"},{"label":"test_12","q_format":"single","q_text":"You create an important report for your large team in Google Data Studio 360. The report uses Google BigQuery as its data source. You notice that visualizations are not showing data that is less than 1 hour old. What should you do?","answers":[{"ans":"A. Disable caching by editing the report settings.","val":true},{"ans":"B. Disable caching in BigQuery by editing table details.","val":false},{"ans":"C. Refresh your browser tab showing the visualizations.","val":false},{"ans":"D. Clear your browser history for the past hour then reload the tab showing the virtualizations.","val":false}],"q_expl":"correct answer -> Disable caching by editing the report settings. A cache is a temporary data storage system. Fetching cached data can be much faster than fetching it directly from the underlying data set, and helps reduce the number of queries sent, minimizing costs for paid data access. Reference: https:\/\/support.google.com\/datastudio\/answer\/7020039?hl=en#zippy=%2Cin-this-article"},{"label":"test_12","q_format":"single","q_text":"You are developing an application on Google Cloud that will automatically generate subject labels for users\u2018 blog posts. You are under competitive pressure to add this feature quickly, and you have no additional developer resources. No one on your team has experience with machine learning. What should you do?","answers":[{"ans":"A. Call the Cloud Natural Language API from your application. Process the generated Entity Analysis as labels.","val":true},{"ans":"B. Call the Cloud Natural Language API from your application. Process the generated Sentiment Analysis as labels.","val":false},{"ans":"C. Build and train a text classification model using TensorFlow. Deploy the model using Cloud Machine Learning Engine. Call the model from your application and process the results as labels.","val":false},{"ans":"D. Build and train a text classification model using TensorFlow. Deploy the model using a Kubernetes Engine cluster. Call the model from your application and process the results as labels.","val":false}],"q_expl":"Correct Answer : A Entity analysis -> Identify entities within documents receipts, invoices, and contracts and label them by types such as date, person, contact information, organization, location, events, products, and media. Sentiment analysis -> Understand the overall opinion, feeling, or attitude sentiment expressed in a block of text. \u2014 Avoid Custom models"},{"label":"test_12","q_format":"single","q_text":"What Dataflow concept determines when a Window\u2018s contents should be output based on certain criteria being met?","answers":[{"ans":"A. Sessions","val":false},{"ans":"B. OutputCriteria","val":false},{"ans":"C. Windows","val":false},{"ans":"D. Triggers","val":true}],"q_expl":"Triggers control when the elements for a specific key and window are output. As elements arrive, they are put into one or more windows by a Window transform and its associated WindowFn, and then passed to the associated Trigger to determine if the Windows contents should be output. Reference: https:\/\/cloud.google.com\/dataflow\/java-sdk\/JavaDoc\/com\/google\/cloud\/dataflow\/sdk\/transforms\/windowing\/Trigger"},{"label":"test_12","q_format":"single","q_text":"Your financial services company is moving to cloud technology and wants to store 50 TB of financial time-series data in the cloud. This data is updated frequently and new data will be streaming in all the time. Your company also wants to move their existing Apache Hadoop jobs to the cloud to get insights into this data.Which product should they use to store the data?","answers":[{"ans":"A. Cloud Bigtable","val":true},{"ans":"B. Google BigQuery","val":false},{"ans":"C. Google Cloud Storage","val":false},{"ans":"D. Google Cloud Datastore","val":false}],"q_expl":"https:\/\/cloud.google.com\/blog\/products\/databases\/getting-started-with-time-series-trend-predictions-using-gcp"},{"label":"test_12","q_format":"single","q_text":"You are building new real-time data warehouse for your company and will use Google BigQuery streaming inserts. There is no guarantee that data will only be sent in once but you do have a unique ID for each row of data and an event timestamp. You want to ensure that duplicates are not included while interactively querying data. Which query type should you use?","answers":[{"ans":"A. Include ORDER BY DESK on timestamp column and LIMIT to 1.","val":false},{"ans":"B. Use GROUP BY on the unique ID column and timestamp column and SUM on the values.","val":false},{"ans":"C. Use the LAG window function with PARTITION by unique ID along with WHERE LAG IS NOT NULL.","val":false},{"ans":"D. Use the ROW_NUMBER window function with PARTITION by unique ID along with WHERE row equals 1.","val":true}],"q_expl":"Explanation: https:\/\/www.youtube.com\/watch?v=ysArdMImULo&list=PLQMsfKRZZviSLraRoqXulcMKFvIXQkHdA&index=3"},{"label":"test_12","q_format":"multiple","q_text":"Which of the following statements about the Wide & Deep Learning model are true? (Select 2 answers.)","answers":[{"ans":"A. The wide model is used for memorization, while the deep model is used for generalization.","val":true},{"ans":"B. A good use for the wide and deep model is a recommender system.","val":true},{"ans":"C. The wide model is used for generalization, while the deep model is used for memorization.","val":false},{"ans":"D. A good use for the wide and deep model is a small-scale linear regression problem.","val":false}],"q_expl":"Can we teach computers to learn like humans do, by combining the power of memorization and generalization? It\u2018s not an easy question to answer, but by jointly training a wide linear model (for memorization) alongside a deep neural network (for generalization), one can combine the strengths of both to bring us one step closer. At Google, we call it Wide & Deep Learning. It\u2018s useful for generic large-scale regression and classification problems with sparse inputs (categorical features with a large number of possible feature values), such as recommender systems, search, and ranking problems. Reference: https:\/\/research.googleblog.com\/2016\/06\/wide-deep-learning-better-together-with.html"},{"label":"test_12","q_format":"single","q_text":"You need to set access to BigQuery for different departments within your company. Your solution should comply with the following requirements: ? Each department should have access only to their data. Each department will have one or more leads who need to be able to create and update tables and provide them to their team. ? Each department has data analysts who need to be able to query but not modify data. How should you set access to the data in BigQuery?","answers":[{"ans":"A. Create a dataset for each department. Assign the department leads the role of OWNER, and assign the data analysts the role of WRITER on their dataset.","val":false},{"ans":"B. Create a dataset for each department. Assign the department leads the role of WRITER, and assign the data analysts the role of READER on their dataset.","val":true},{"ans":"C. Create a table for each department. Assign the department leads the role of Owner, and assign the data analysts the role of Editor on the project the table is in.","val":false},{"ans":"D. Create a table for each department. Assign the department leads the role of Editor, and assign the data analysts the role of Viewer on the project the table is in.","val":false}],"q_expl":"B: By default, granting access to a project also grants access to datasets within it. Default access can be overridden on a per-dataset basis. Primitive roles apply at the dataset level: https:\/\/cloud.google.com\/bigquery\/docs\/access-control-primitive-roles"},{"label":"test_12","q_format":"single","q_text":"Your company\u2018s on-premises Apache Hadoop servers are approaching end-of-life, and IT has decided to migrate the cluster to Google Cloud Dataproc. A like-for- like migration of the cluster would require 50 TB of Google Persistent Disk per node. The CIO is concerned about the cost of using that much block storage. You want to minimize the storage cost of the migration. What should you do?","answers":[{"ans":"A. Put the data into Google Cloud Storage.","val":true},{"ans":"B. Use preemptible virtual machines (VMs) for the Cloud Dataproc cluster.","val":false},{"ans":"C. Tune the Cloud Dataproc cluster so that there is just enough disk for all data.","val":false},{"ans":"D. Migrate some of the cold data into Google Cloud Storage, and keep only the hot data in Persistent Disk.","val":false}],"q_expl":"Ans: A B: Wrong eVM wont solve the problem of larger storage prices. C: May be, but nothing mentioned in terms of what to tune in the question, also this is like-for-like migration so tuning may not be part of the migration. D: Again, this is like-for-like so need to define what is hot data and which is cold data, also persistent disk costlier than cloud storage."},{"label":"test_12","q_format":"single","q_text":"An online retailer has built their current application on Google App Engine. A new initiative at the company mandates that they extend their application to allow their customers to transact directly via the application. They need to manage their shopping transactions and analyze combined data from multiple datasets using a business intelligence (BI) tool. They want to use only a single database for this purpose. Which Google Cloud database should they choose?","answers":[{"ans":"A. BigQuery","val":false},{"ans":"B. Cloud SQL","val":true},{"ans":"C. Cloud BigTable","val":false},{"ans":"D. Cloud Datastore","val":false}],"q_expl":"B, The Cloud SQL connector allows you to access data from Cloud SQL databases within Data Studio. https:\/\/support.google.com\/datastudio\/answer\/7020436?hl=en Access Cloud SQL instances from just about any application. Easily connect from App Engine, Compute Engine, Google Kubernetes Engine, and your workstation. https:\/\/cloud.google.com\/sql\/"},{"label":"test_12","q_format":"single","q_text":"You need to copy millions of sensitive patient records from a relational database to BigQuery. The total size of the database is 10 TB. You need to design a solution that is secure and time-efficient. What should you do?","answers":[{"ans":"A. Export the records from the database as an Avro file. Upload the file to GCS using gsutil, and then load the Avro file into BigQuery using the BigQuery web UI in the GCP Console.","val":false},{"ans":"B. Export the records from the database as an Avro file. Copy the file onto a Transfer Appliance and send it to Google, and then load the Avro file into BigQuery using the BigQuery web UI in the GCP Console.","val":true},{"ans":"C. Export the records from the database into a CSV file. Create a public URL for the CSV file, and then use Storage Transfer Service to move the file to Cloud Storage. Load the CSV file into BigQuery using the BigQuery web UI in the GCP Console.","val":false},{"ans":"D. Export the records from the database as an Avro file. Create a public URL for the Avro file, and then use Storage Transfer Service to move the file to Cloud Storage. Load the Avro file into BigQuery using the BigQuery web UI in the GCP Console.","val":false}],"q_expl":"You are transferring sensitive patient information, so C & D are ruled out. Choice comes down to A & B. Here it gets tricky. How to choose Transfer Appliance: (https:\/\/cloud.google.com\/transfer-appliance\/docs\/2.0\/overview) Without knowing the bandwidth, it is not possible to determine whether the upload can be completed within 7 days, as recommended by Google. So the safest and most performant way is to use Transfer Appliance. Therefore my choice is B."},{"label":"test_12","q_format":"multiple","q_text":"Which of the following job types are supported by Cloud Dataproc (select 3 answers)?","answers":[{"ans":"A. Hive","val":true},{"ans":"B. Pig","val":true},{"ans":"C. YARN","val":false},{"ans":"D. Spark","val":true}],"q_expl":"Cloud Dataproc provides out-of-the box and end-to-end support for many of the most popular job types, including Spark, Spark SQL, PySpark, MapReduce, Hive, and Pig jobs. Reference: https:\/\/cloud.google.com\/dataproc\/docs\/resources\/faq#what_type_of_jobs_can_i_run"},{"label":"test_12","q_format":"single","q_text":"You are a retailer that wants to integrate your online sales capabilities with different in-home assistants, such as Google Home. You need to interpret customer voice commands and issue an order to the backend systems. Which solutions should you choose?","answers":[{"ans":"A. Cloud Speech-to-Text API","val":false},{"ans":"B. Cloud Natural Language API","val":false},{"ans":"C. Dialogflow Enterprise Edition","val":true},{"ans":"D. Cloud AutoML Natural Language","val":false}],"q_expl":"C: Dialogflow Enterprise Edition is an end-to-end development suite for building conversational interfaces for websites, mobile applications, popular messaging platforms, and IoT devices. You can use it to build interfaces (e.g., chatbots) that are capable of natural and rich interactions between your users and your business. It is powered by machine learning to recognize the intent and context of what a user says, allowing your conversational interface to provide highly efficient and accurate responses. https:\/\/cloud.google.com\/dialogflow\/ Dialogflow API V2 is the new iteration of our developer API. The new API integrates Google Cloud Speech-to-Text, enabling developers to send audio directly to Dialogflow for combined speech recognition and natural language understanding. https:\/\/dialogflow.com\/v2-faq https:\/\/cloud.google.com\/blog\/products\/gcp\/introducing-dialogflow-enterprise-edition-a-new-way-to-build-voice-and-text-conversational-apps"},{"label":"test_12","q_format":"multiple","q_text":"Which of these numbers are adjusted by a neural network as it learns from a training dataset (select 2 answers)?","answers":[{"ans":"A. Weights","val":true},{"ans":"B. Biases","val":true},{"ans":"C. Continuous features","val":false},{"ans":"D. Input values","val":false}],"q_expl":"A neural network is a simple mechanism thats implemented with basic math. The only difference between the traditional programming model and a neural network is that you let the computer determine the parameters (weights and bias) by learning from training datasets. Reference: https:\/\/cloud.google.com\/blog\/big-data\/2016\/07\/understanding-neural-networks-with-tensorflow-playground"},{"label":"test_12","q_format":"single","q_text":"You have a data stored in BigQuery. The data in the BigQuery dataset must be highly available. You need to define a storage, backup, and recovery strategy of this data that minimizes cost. How should you configure the BigQuery table?","answers":[{"ans":"A. Set the BigQuery dataset to be regional. In the event of an emergency, use a point-in-time snapshot to recover the data.","val":false},{"ans":"B. Set the BigQuery dataset to be regional. Create a scheduled query to make copies of the data to tables suffixed with the time of the backup. In the event of an emergency, use the backup copy of the table.","val":false},{"ans":"C. Set the BigQuery dataset to be multi-regional. In the event of an emergency, use a point-in-time snapshot to recover the data.","val":true},{"ans":"D. Set the BigQuery dataset to be multi-regional. Create a scheduled query to make copies of the data to tables suffixed with the time of the backup. In the event of an emergency, use the backup copy of the table.","val":false}],"q_expl":"Vote for C. Answer D will be less reliable than C since the scheduled query may have a longer gap between failure and recovery than the point-in-time snapshot. Yes, point-in-time snapshot only keeps for 7 days, which means if failure, you have 7 days to recover. This doesn\u2018t mean it only keeps 7 days\u2018 data. It gives 7 days as a buffer for you to return from your vacation and restore the data"},{"label":"test_12","q_format":"single","q_text":"Which of the following is not possible using primitive roles?","answers":[{"ans":"A. Give a user viewer access to BigQuery and owner access to Google Compute Engine instances.","val":false},{"ans":"B. Give UserA owner access and UserB editor access for all datasets in a project.","val":false},{"ans":"C. Give a user access to view all datasets in a project, but not run queries on them.","val":true},{"ans":"D. Give GroupA owner access and GroupB editor access for all datasets in a project.","val":false}],"q_expl":"Primitive roles can be used to give owner, editor, or viewer access to a user or group, but they can\u2018t be used to separate data access permissions from job-running permissions. Reference: https:\/\/cloud.google.com\/bigquery\/docs\/access-control#primitive_iam_roles"},{"label":"test_12","q_format":"single","q_text":"MJTelco Case Study -Company Overview -MJTelco is a startup that plans to build networks in rapidly growing, underserved markets around the world. The company has patents for innovative optical communications hardware. Based on these patents, they can create many reliable, high-speed backbone links with inexpensive hardware.Company Background -Founded by experienced telecom executives, MJTelco uses technologies originally developed to overcome communications challenges in space. Fundamental to their operation, they need to create a distributed data infrastructure that drives real-time analysis and incorporates machine learning to continuously optimize their topologies. Because their hardware is inexpensive, they plan to overdeploy the network allowing them to account for the impact of dynamic regional politics on location availability and cost.Their management and operations teams are situated all around the globe creating many-to-many relationship between data consumers and provides in their system. After careful consideration, they decided public cloud is the perfect environment to support their needs.Solution Concept -MJTelco is running a successful proof-of-concept (PoC) project in its labs. They have two primary needs:? Scale and harden their PoC to support significantly more data flows generated when they ramp to more than 50,000 installations.? Refine their machine-learning cycles to verify and improve the dynamic models they use to control topology definition.MJTelco will also use three separate operating environments \u201c\u201c development\/test, staging, and production \u201c\u201c to meet the needs of running experiments, deploying new features, and serving production customers.Business Requirements -? Scale up their production environment with minimal cost, instantiating resources when and where needed in an unpredictable, distributed telecom user community.? Ensure security of their proprietary data to protect their leading-edge machine learning and analysis.? Provide reliable and timely access to data for analysis from distributed research workers? Maintain isolated environments that support rapid iteration of their machine-learning models without affecting their customers.Technical Requirements -? Ensure secure and efficient transport and storage of telemetry data? Rapidly scale instances to support between 10,000 and 100,000 data providers with multiple flows each.? Allow analysis and presentation against data tables tracking up to 2 years of data storing approximately 100m records\/day? Support rapid iteration of monitoring infrastructure focused on awareness of data pipeline problems both in telemetry flows and in production learning cycles.CEO Statement -Our business model relies on our patents, analytics and dynamic machine learning. Our inexpensive hardware is organized to be highly reliable, which gives us cost advantages. We need to quickly stabilize our large distributed data pipelines to meet our reliability and capacity commitments.CTO Statement -Our public cloud services must operate as advertised. We need resources that scale and keep our data secure. We also need environments in which our data scientists can carefully study and quickly adapt our models. Because we rely on automation to process our data, we also need our development and test environments to work as we iterate.CFO Statement -The project is too large for us to maintain the hardware and software required for the data and analysis. Also, we cannot afford to staff an operations team to monitor so many data feeds, so we will rely on automation and infrastructure. Google Cloud\u2018s machine learning will allow our quantitative researchers to work on our high-value problems instead of problems with our data pipelines.You need to compose visualizations for operations teams with the following requirements:? The report must include telemetry data from all 50,000 installations for the most resent 6 weeks (sampling once every minute).? The report must not be more than 3 hours delayed from live data.? The actionable report should only show suboptimal links.? Most suboptimal links should be sorted to the top.? Suboptimal links can be grouped and filtered by regional geography.? User response time to load the report must be <5 seconds.Which approach meets the requirements?","answers":[{"ans":"A. Load the data into Google Sheets, use formulas to calculate a metric, and use filters\/sorting to show only suboptimal links in a table.","val":false},{"ans":"B. Load the data into Google BigQuery tables, write Google Apps Script that queries the data, calculates the metric, and shows only suboptimal rows in a table in Google Sheets.","val":false},{"ans":"C. Load the data into Google Cloud Datastore tables, write a Google App Engine Application that queries all rows, applies a function to derive the metric, and then renders results in a table using the Google charts and visualization API.","val":false},{"ans":"D. Load the data into Google BigQuery tables, write a Google Data Studio 360 report that connects to your data, calculates a metric, and then uses a filter expression to show only suboptimal rows in a table.","val":true}],"q_expl":"The correct answer is D. Please look for the details in below https:\/\/cloud.google.com\/dataflow\/docs\/guides\/specifying-exec-params We need to specify and set execution parameters for cloud data flow . Also, to enable autoscaling, set the following execution parameters when you start your pipeline: \u2013autoscaling_algorithm=THROUGHPUT_BASED \u2013max_num_workers=N The objective of autoscaling streaming pipelines is to minimize backlog while maximizing worker utilization and throughput, and quickly react to spikes in load. By enabling autoscaling, you don\u2018t have to choose between provisioning for peak load and fresh results. Workers are added as CPU utilization and backlog increase and are removed as these metrics come down. This way, you\u0092re paying only for what you need, and the job is processed as efficiently as possible."},{"label":"test_12","q_format":"single","q_text":"You have enabled the free integration between Firebase Analytics and Google BigQuery. Firebase now automatically creates a new table daily in BigQuery in the format app_events_YYYYMMDD. You want to query all of the tables for the past 30 days in legacy SQL. What should you do?","answers":[{"ans":"A. Use the TABLE_DATE_RANGE function","val":true},{"ans":"B. Use the WHERE_PARTITIONTIME pseudo column","val":false},{"ans":"C. Use WHERE date BETWEEN YYYY-MM-DD AND YYYY-MM-DD","val":false},{"ans":"D. Use SELECT IF.(date >= YYYY-MM-DD AND date <= YYYY-MM-DD","val":false}],"q_expl":"https:\/\/cloud.google.com\/blog\/products\/gcp\/using-bigquery-and-firebase-analytics-to-understand-your-mobile-app?hl=am"},{"label":"test_12","q_format":"single","q_text":"You have a requirement to insert minute-resolution data from 50,000 sensors into a BigQuery table. You expect significant growth in data volume and need the data to be available within 1 minute of ingestion for real-time analysis of aggregated trends. What should you do?","answers":[{"ans":"A. Use bq load to load a batch of sensor data every 60 seconds.","val":false},{"ans":"B. Use a Cloud Dataflow pipeline to stream data into the BigQuery table.","val":true},{"ans":"C. Use the INSERT statement to insert a batch of data every 60 seconds.","val":false},{"ans":"D. Use the MERGE statement to apply updates in batch every 60 seconds.","val":false}],"q_expl":"B. Use a Cloud Dataflow pipeline to stream data into the BigQuery table \nHere\u2018s why this approach is ideal for your requirements:\nMinute-resolution data: Cloud Dataflow is well-suited for handling high-throughput streaming data like sensor readings generated every minute.\nSignificant data volume growth: Cloud Dataflow can scale efficiently to accommodate the anticipated increase in data volume from your sensors over time.\nReal-time analysis: Cloud Dataflow pipelines can be designed to process and ingest data with low latency, making it available for analysis within your desired timeframe of 1 minute.\nOther options and why they are less suitable:\nA. bq load: This is a batch loading tool for BigQuery and wouldn\u2018t be ideal for real-time streaming data ingestion. While you could potentially schedule batch loads every 60 seconds, it wouldn\u2018t provide the near real-time availability you need.\nC. INSERT statement: Using a loop with INSERT statements every 60 seconds might be technically possible, but it\u2018s not a scalable or efficient approach for managing high-volume streaming data. Cloud Dataflow offers a more robust and manageable solution.\nD. MERGE statement: The MERGE statement is typically used for combining insert and update operations, but it\u2018s not specifically designed for real-time streaming data ingestion."},{"label":"test_12","q_format":"single","q_text":"Your analytics team wants to build a simple statistical model to determine which customers are most likely to work with your company again, based on a few different metrics. They want to run the model on Apache Spark, using data housed in Google Cloud Storage, and you have recommended using Google CloudDataproc to execute this job. Testing has shown that this workload can run in approximately 30 minutes on a 15-node cluster, outputting the results into GoogleBigQuery. The plan is to run this workload weekly. How should you optimize the cluster for cost?","answers":[{"ans":"A. Migrate the workload to Google Cloud Dataflow","val":false},{"ans":"B. Use pre-emptible virtual machines (VMs) for the cluster","val":true},{"ans":"C. Use a higher-memory node so that the job runs faster","val":false},{"ans":"D. Use SSDs on the worker nodes so that the job can run faster","val":false}],"q_expl":"https:\/\/cloud.google.com\/dataproc\/docs\/concepts\/compute\/preemptible-vms"},{"label":"test_12","q_format":"single","q_text":"You work for an economic consulting firm that helps companies identify economic trends as they happen. As part of your analysis, you use Google BigQuery to correlate customer data with the average prices of the 100 most common goods sold, including bread, gasoline, milk, and others. The average prices of these goods are updated every 30 minutes. You want to make sure this data stays up to date so you can combine it with other data in BigQuery as cheaply as possible.What should you do?","answers":[{"ans":"A. Load the data every 30 minutes into a new partitioned table in BigQuery.","val":false},{"ans":"B. Store and update the data in a regional Google Cloud Storage bucket and create a federated data source in BigQuery","val":true},{"ans":"C. Store the data in Google Cloud Datastore. Use Google Cloud Dataflow to query BigQuery and combine the data programmatically with the data stored in Cloud Datastore","val":false},{"ans":"D. Store the data in a file in a regional Google Cloud Storage bucket. Use Cloud Dataflow to query BigQuery and combine the data programmatically with the data stored in Google Cloud Storage.","val":false}],"q_expl":"B is correct. This is one of the use cases for external (federated) data sources in BigQuery: \u201cSmall amount of frequently changing data to join with other tables in BigQuery\u201c. https:\/\/cloud.google.com\/blog\/products\/gcp\/accessing-external-federated-data-sources-with-bigquerys-data-access-layer"},{"label":"test_12","q_format":"single","q_text":"The marketing team at your organization provides regular updates of a segment of your customer dataset. The marketing team has given you a CSV with 1 million records that must be updated in BigQuery. When you use the UPDATE statement in BigQuery, you receive a quotaExceeded error. What should you do?","answers":[{"ans":"A. Reduce the number of records updated each day to stay within the BigQuery UPDATE DML statement limit.","val":false},{"ans":"B. Increase the BigQuery UPDATE DML statement limit in the Quota management section of the Google Cloud Platform Console.","val":false},{"ans":"C. Split the source CSV file into smaller CSV files in Cloud Storage to reduce the number of BigQuery UPDATE DML statements per BigQuery job.","val":false},{"ans":"D. Import the new records from the CSV file into a new BigQuery table. Create a BigQuery job that merges the new records with the existing records and writes the results to a new BigQuery table.","val":true}],"q_expl":"https:\/\/cloud.google.com\/blog\/products\/gcp\/performing-large-scale-mutations-in-bigquery"},{"label":"test_13","q_format":"single","q_text":"MJTelco Case Study \u2013 Company Overview \u2013 MJTelco is a startup that plans to build networks in rapidly growing, underserved markets around the world. The company has patents for innovative optical communications hardware. Based on these patents, they can create many reliable, high-speed backbone links with inexpensive hardware. Company Background \u2013 Founded by experienced telecom executives, MJTelco uses technologies originally developed to overcome communications challenges in space. Fundamental to their operation, they need to create a distributed data infrastructure that drives real-time analysis and incorporates machine learning to continuously optimize their topologies. Because their hardware is inexpensive, they plan to overdeploy the network allowing them to account for the impact of dynamic regional politics on location availability and cost. Their management and operations teams are situated all around the globe creating many-to-many relationship between data consumers and provides in their system. After careful consideration, they decided public cloud is the perfect environment to support their needs. Solution Concept \u2013 MJTelco is running a successful proof-of-concept (PoC) project in its labs. They have two primary needs: ? Scale and harden their PoC to support significantly more data flows generated when they ramp to more than 50,000 installations. ? Refine their machine-learning cycles to verify and improve the dynamic models they use to control topology definition. MJTelco will also use three separate operating environments \u201c\u201c development\/test, staging, and production \u201c\u201c to meet the needs of running experiments, deploying new features, and serving production customers. Business Requirements \u2013 ? Scale up their production environment with minimal cost, instantiating resources when and where needed in an unpredictable, distributed telecom user community. ? Ensure security of their proprietary data to protect their leading-edge machine learning and analysis. ? Provide reliable and timely access to data for analysis from distributed research workers ? Maintain isolated environments that support rapid iteration of their machine-learning models without affecting their customers. Technical Requirements \u2013 ? Ensure secure and efficient transport and storage of telemetry data ? Rapidly scale instances to support between 10,000 and 100,000 data providers with multiple flows each. ? Allow analysis and presentation against data tables tracking up to 2 years of data storing approximately 100m records\/day ? Support rapid iteration of monitoring infrastructure focused on awareness of data pipeline problems both in telemetry flows and in production learning cycles. CEO Statement \u2013 Our business model relies on our patents, analytics and dynamic machine learning. Our inexpensive hardware is organized to be highly reliable, which gives us cost advantages. We need to quickly stabilize our large distributed data pipelines to meet our reliability and capacity commitments. CTO Statement \u2013 Our public cloud services must operate as advertised. We need resources that scale and keep our data secure. We also need environments in which our data scientists can carefully study and quickly adapt our models. Because we rely on automation to process our data, we also need our development and test environments to work as we iterate. CFO Statement \u2013 The project is too large for us to maintain the hardware and software required for the data and analysis. Also, we cannot afford to staff an operations team to monitor so many data feeds, so we will rely on automation and infrastructure. Google Cloud\u2018s machine learning will allow our quantitative researchers to work on our high-value problems instead of problems with our data pipelines. You need to compose visualizations for operations teams with the following requirements: ? The report must include telemetry data from all 50,000 installations for the most resent 6 weeks (sampling once every minute). ? The report must not be more than 3 hours delayed from live data. ? The actionable report should only show suboptimal links. ? Most suboptimal links should be sorted to the top. ? Suboptimal links can be grouped and filtered by regional geography. ? User response time to load the report must be <5 seconds. Which approach meets the requirements?","answers":[{"ans":"A. Load the data into Google Sheets, use formulas to calculate a metric, and use filters\/sorting to show only suboptimal links in a table.","val":false},{"ans":"B. Load the data into Google BigQuery tables, write Google Apps Script that queries the data, calculates the metric, and shows only suboptimal rows in a table in Google Sheets.","val":false},{"ans":"C. Load the data into Google Cloud Datastore tables, write a Google App Engine Application that queries all rows, applies a function to derive the metric, and then renders results in a table using the Google charts and visualization API.","val":false},{"ans":"D. Load the data into Google BigQuery tables, write a Google Data Studio 360 report that connects to your data, calculates a metric, and then uses a filter expression to show only suboptimal rows in a table.","val":true}],"q_expl":"D. Load the data into Google BigQuery tables, write a Google Data Studio 360 report that connects to your data, calculates a metric, and then uses a filter expression to show only suboptimal rows in a table. \nHere\u2018s why this approach aligns well with their needs:\nScalability and Performance: BigQuery is a serverless data warehouse designed for handling large datasets. It can efficiently store and query the telemetry data from 50,000 installations over 6 weeks, ensuring fast loading times (<5 seconds) for the report.\nReal-time Data with Low Latency: BigQuery supports near real-time data ingestion, minimizing the delay between live data and the visualizations (within the 3-hour requirement).\nDynamic Filtering and Sorting: Google Data Studio offers a user-friendly interface for building reports with filters and sorting options. Operations teams can filter by region and sort the results to prioritize the most suboptimal links.\nMetric Calculation: Data Studio allows calculations within the reports. You can define a metric to identify suboptimal links within the report itself.\nCost-Effectiveness: Compared to Google App Engine, Data Studio offers a more cost-effective solution for building visualizations on top of BigQuery data.\nOther options and why they are less suitable:\nA. Google Sheets: Google Sheets wouldn\u2018t be scalable for handling such a large dataset (millions of rows) and wouldn\u2018t meet the performance requirements for near real-time updates and fast loading times.\nB. Google Apps Script: While Apps Script could be used to query BigQuery and populate a Google Sheet, Sheets still has limitations for data size and performance. Additionally, maintaining and updating Apps Script might be more complex than using Data Studio\u2018s visual interface.\nC. Google Cloud Datastore: Cloud Datastore is a NoSQL database service not ideal for large-scale data warehousing and complex analytics required for this scenario. BigQuery is a more suitable data warehouse solution for this purpose."},{"label":"test_13","q_format":"single","q_text":"Your company is migrating their 30-node Apache Hadoop cluster to the cloud. They want to re-use Hadoop jobs they have already created and minimize the management of the cluster as much as possible. They also want to be able to persist data beyond the life of the cluster. What should you do?","answers":[{"ans":"A. Create a Google Cloud Dataflow job to process the data.","val":false},{"ans":"B. Create a Google Cloud Dataproc cluster that uses persistent disks for HDFS.","val":false},{"ans":"C. Create a Hadoop cluster on Google Compute Engine that uses persistent disks.","val":false},{"ans":"D. Create a Cloud Dataproc cluster that uses the Google Cloud Storage connector.","val":true},{"ans":"E. Create a Hadoop cluster on Google Compute Engine that uses Local SSD disks.","val":false}],"q_expl":"D is correct because it uses managed services, and also allows for the data to persist on GCS beyond the life of the cluster. A is not correct because the goal is to re-use their Hadoop jobs and MapReduce and\/or Spark jobs cannot simply be moved to Dataflow. B is not correct because the goal is to persist the data beyond the life of the ephemeral clusters, and if HDFS is used as the primary attached storage mechanism, it will also disappear at the end of the cluster\u0092s life. C is not correct because the goal is to use managed services as much as possible, and this is the opposite. E is not correct because the goal is to use managed services as much as possible, and this is the opposite."},{"label":"test_13","q_format":"single","q_text":"You need to choose a database to store time series CPU and memory usage for millions of computers. You need to store this data in one-second interval samples. Analysts will be performing real-time, ad hoc analytics against the database. You want to avoid being charged for every query executed and ensure that the schema design will allow for future growth of the dataset. Which database and data model should you choose?","answers":[{"ans":"A. Create a table in BigQuery, and append the new samples for CPU and memory to the table","val":false},{"ans":"B. Create a wide table in BigQuery, create a column for the sample value at each second, and update the row with the interval for each second","val":false},{"ans":"C. Create a narrow table in Cloud Bigtable with a row key that combines the Computer Engine computer identifier with the sample time at each second","val":true},{"ans":"D. Create a wide table in Cloud Bigtable with a row key that combines the computer identifier with the sample time at each minute, and combine the values for each second as column data.","val":false}],"q_expl":"Answer C A tall and narrow table has a small number of events per row, which could be just one event, whereas a short and wide table has a large number of events per row. As explained in a moment, tall and narrow tables are best suited for time-series data. For time series, you should generally use tall and narrow tables. This is for two reasons: Storing one event per row makes it easier to run queries against your data. Storing many events per row makes it more likely that the total row size will exceed the recommended maximum (see Rows can be big but are not infinite). https:\/\/cloud.google.com\/bigtable\/docs\/schema-design-time-series#patterns_for_row_key_design"},{"label":"test_13","q_format":"single","q_text":"You are building an application to share financial market data with consumers, who will receive data feeds. Data is collected from the markets in real time.Consumers will receive the data in the following ways:? Real-time event stream? ANSI SQL access to real-time stream and historical data? Batch historical exportsWhich solution should you use?","answers":[{"ans":"A. Cloud Dataflow, Cloud SQL, Cloud Spanner","val":false},{"ans":"B. Cloud Pub\/Sub, Cloud Storage, BigQuery","val":true},{"ans":"C. Cloud Dataproc, Cloud Dataflow, BigQuery","val":false},{"ans":"D. Cloud Pub\/Sub, Cloud Dataproc, Cloud SQL","val":false}],"q_expl":"B. Cloud Pub\/Sub, Cloud Storage, BigQuery \nHere\u2018s how each service aligns with your requirements:\nCloud Pub\/Sub: This managed pub\/sub messaging service acts as the real-time data backbone. As market data is collected, it can be published to Cloud Pub\/Sub topics. Consumers can subscribe to these topics to receive real-time updates as an event stream.\nCloud Storage: Cloud Storage can be used for batch historical data exports. Consumers can be provided access to download specific datasets or historical snapshots at their convenience.\nBigQuery: This serverless data warehouse is ideal for storing and analyzing the historical financial market data. Consumers can leverage ANSI SQL to query BigQuery for specific data points or trends. BigQuery can also be integrated with Cloud Pub\/Sub for streaming real-time data into BigQuery for further analysis or historical storage.\nWhy other options are less suitable:\nA. Cloud Dataflow, Cloud SQL, Cloud Spanner: While Cloud Dataflow could be used for data processing, it\u2018s not necessary in this scenario. Cloud SQL might be suitable for a separate user management database but isn\u2018t ideal for storing and querying large amounts of market data. Cloud Spanner offers strong consistency guarantees but might be more expensive than BigQuery for this use case.\nC. Cloud Dataproc, Cloud Dataflow, BigQuery: Cloud Dataproc (managed Hadoop) might be an overkill for this solution. Cloud Dataflow could be used for specific data processing tasks, but BigQuery itself can handle many data analysis needs.\nD. Cloud Pub\/Sub, Cloud Dataproc, Cloud SQL: Similar to option C, Cloud Dataproc might be unnecessary complexity. Cloud SQL wouldn\u2018t be ideal for storing and querying large datasets of financial market data."},{"label":"test_13","q_format":"single","q_text":"The Dataflow SDKs have been recently transitioned into which Apache service?","answers":[{"ans":"A. Apache Spark","val":false},{"ans":"B. Apache Hadoop","val":false},{"ans":"C. Apache Kafka","val":false},{"ans":"D. Apache Beam","val":true}],"q_expl":"Dataflow SDKs are being transitioned to Apache Beam, as per the latest Google directive Reference: https:\/\/cloud.google.com\/dataflow\/docs\/"},{"label":"test_13","q_format":"single","q_text":"Flowlogistic Case Study -Company Overview -Flowlogistic is a leading logistics and supply chain provider. They help businesses throughout the world manage their resources and transport them to their final destination. The company has grown rapidly, expanding their offerings to include rail, truck, aircraft, and oceanic shipping.Company Background -The company started as a regional trucking company, and then expanded into other logistics market. Because they have not updated their infrastructure, managing and tracking orders and shipments has become a bottleneck. To improve operations, Flowlogistic developed proprietary technology for tracking shipments in real time at the parcel level. However, they are unable to deploy it because their technology stack, based on Apache Kafka, cannot support the processing volume. In addition, Flowlogistic wants to further analyze their orders and shipments to determine how best to deploy their resources.Solution Concept -Flowlogistic wants to implement two concepts using the cloud:? Use their proprietary technology in a real-time inventory-tracking system that indicates the location of their loads? Perform analytics on all their orders and shipment logs, which contain both structured and unstructured data, to determine how best to deploy resources, which markets to expand info. They also want to use predictive analytics to learn earlier when a shipment will be delayed.Existing Technical Environment -Flowlogistic architecture resides in a single data center:? Databases8 physical servers in 2 clusters- SQL Server \u201c\u201c user data, inventory, static data3 physical servers- Cassandra \u201c\u201c metadata, tracking messages10 Kafka servers \u201c\u201c tracking message aggregation and batch insert? Application servers \u201c\u201c customer front end, middleware for order\/customs60 virtual machines across 20 physical servers- Tomcat \u201c\u201c Java services- Nginx \u201c\u201c static content- Batch servers? Storage appliances- iSCSI for virtual machine (VM) hosts- Fibre Channel storage area network (FC SAN) \u201c\u201c SQL server storage- Network-attached storage (NAS) image storage, logs, backups? 10 Apache Hadoop \/Spark servers- Core Data Lake- Data analysis workloads? 20 miscellaneous servers- Jenkins, monitoring, bastion hosts,Business Requirements -? Build a reliable and reproducible environment with scaled panty of production.? Aggregate data in a centralized Data Lake for analysis? Use historical data to perform predictive analytics on future shipments? Accurately track every shipment worldwide using proprietary technology? Improve business agility and speed of innovation through rapid provisioning of new resources? Analyze and optimize architecture for performance in the cloud? Migrate fully to the cloud if all other requirements are metTechnical Requirements -? Handle both streaming and batch data? Migrate existing Hadoop workloads? Ensure architecture is scalable and elastic to meet the changing demands of the company.? Use managed services whenever possible? Encrypt data flight and at rest? Connect a VPN between the production data center and cloud environmentSEO Statement -We have grown so quickly that our inability to upgrade our infrastructure is really hampering further growth and efficiency. We are efficient at moving shipments around the world, but we are inefficient at moving data around.We need to organize our information so we can more easily understand where our customers are and what they are shipping.CTO Statement -IT has never been a priority for us, so as our data has grown, we have not invested enough in our technology. I have a good staff to manage IT, but they are so busy managing our infrastructure that I cannot get them to do the things that really matter, such as organizing our data, building the analytics, and figuring out how to implement the CFO\u2018 s tracking technology.CFO Statement -Part of our competitive advantage is that we penalize ourselves for late shipments and deliveries. Knowing where out shipments are at all times has a direct correlation to our bottom line and profitability. Additionally, I don\u2018t want to commit capital to building out a server environment.Flowlogistic wants to use Google BigQuery as their primary analysis system, but they still have Apache Hadoop and Spark workloads that they cannot move toBigQuery. Flowlogistic does not know how to store the data that is common to both workloads. What should they do?","answers":[{"ans":"A. Store the common data in BigQuery as partitioned tables.","val":false},{"ans":"B. Store the common data in BigQuery and expose authorized views.","val":false},{"ans":"C. Store the common data encoded as Avro in Google Cloud Storage.","val":true},{"ans":"D. Store he common data in the HDFS storage for a Google Cloud Dataproc cluster.","val":false}],"q_expl":"answer is C. Though data proc has connectors for cloud storage, Bigtable, and BigQuery, using a big query connector is a little more work compared to cloud storage and big table. The best thing while moving apache Hadoop and spark jobs to data proc is using cloud storage instead of HDFS. Your data in ORC, Parquet, Avro, or any other format will be used by different clusters or jobs, and you need data persistence if the cluster terminates. We can just replace HDFS:\/\/ with gs:\/\/. Even big query can read data in avro format. The best way to store data common to both workloads is cloud storage. Hence I am going with C."},{"label":"test_13","q_format":"single","q_text":"Which of these statements about exporting data from BigQuery is false?","answers":[{"ans":"A. To export more than 1 GB of data, you need to put a wildcard in the destination filename.","val":false},{"ans":"B. The only supported export destination is Google Cloud Storage.","val":false},{"ans":"C. Data can only be exported in JSON or Avro format.","val":true},{"ans":"D. The only compression option available is GZIP.","val":false}],"q_expl":"Correct: C When you export data from BigQuery, note the following: You cannot export table data to a local file, to Google Sheets, or to Google Drive. The only supported export location is Cloud Storage. For information on saving query results, see Downloading and saving query results. You can export up to 1 GB of table data to a single file. If you are exporting more than 1 GB of data, use a wildcard to export the data into multiple files. When you export data to multiple files, the size of the files will vary. You cannot export nested and repeated data in CSV format. Nested and repeated data is supported for Avro and JSON exports. When you export data in JSON format, INT64 (integer) data types are encoded as JSON strings to preserve 64-bit precision when the data is read by other systems. You cannot export data from multiple tables in a single export job. You cannot choose a compression type other than GZIP when you export data using the Cloud Console or the classic BigQuery web UI."},{"label":"test_13","q_format":"single","q_text":"You used Cloud Dataprep to create a recipe on a sample of data in a BigQuery table. You want to reuse this recipe on a daily upload of data with the same schema, after the load job with variable execution time completes. What should you do?","answers":[{"ans":"A. Create a cron schedule in Cloud Dataprep.","val":false},{"ans":"B. Create an App Engine cron job to schedule the execution of the Cloud Dataprep job.","val":false},{"ans":"C. Export the recipe as a Cloud Dataprep template, and create a job in Cloud Scheduler.","val":false},{"ans":"D. Export the Cloud Dataprep job as a Cloud Dataflow template, and incorporate it into a Cloud Composer job.","val":true}],"q_expl":"Should be D https:\/\/cloud.google.com\/blog\/products\/data-analytics\/how-to-orchestrate-cloud-dataprep-jobs-using-cloud-composer We\u0092re happy to announce the latest release of Cloud Dataprep, which exposes orchestration APIs so you can integrate Cloud Dataprep within your schedulers or other orchestration solutions like Cloud Composer. This means you can expand your automation beyond Cloud Dataflow templates through direct integration in other tools to create repeatable data pipelines for your analytics and AI\/ML initiatives\u0097saving time and adding reliability. In addition, this API lets you use dynamic inputs and outputs through Cloud Dataprep variables or parameters\u0097not available using Cloud Dataflow templates. As a result, you can re-use a single Cloud Dataprep flow to execute on a range of input\/output values that are evaluated at runtime."},{"label":"test_13","q_format":"single","q_text":"You are working on a sensitive project involving private user data. You have set up a project on Google Cloud Platform to house your work internally. An external consultant is going to assist with coding a complex transformation in a Google Cloud Dataflow pipeline for your project. How should you maintain users\u2018 privacy?","answers":[{"ans":"A. Grant the consultant the Viewer role on the project.","val":false},{"ans":"B. Grant the consultant the Cloud Dataflow Developer role on the project.","val":false},{"ans":"C. Create a service account and allow the consultant to log on with it.","val":false},{"ans":"D. Create an anonymized sample of the data for the consultant to work with in a different project.","val":true}],"q_expl":"D. I wouldn\u2018t want to be a consultant who only gets the permissions in B: *This would not allow you to run the pipeline locally, *This would not allow you to verify that the data you\u2018re producing is in any way correct. Security-wise, you\u2018re also not helping yourself much by choosing B, as the consultant could use the permissions granted to the workers he\u2018s controlling to direct the data to a place where he can read it. An anonymized data set put in a separate project solves all of these issues. To reinforce the last point: GCP has recently recognized this by requiring you to to be able to impersonate the controller service account (compute engine default service account by default) if you want to be able to submit jobs. Dataflow Developer alone just won\u2018t allow the consultant to run jobs at all."},{"label":"test_13","q_format":"multiple","q_text":"Which of the following are examples of hyperparameters? (Select 2 answers.)","answers":[{"ans":"A. Number of hidden layers","val":true},{"ans":"B. Number of nodes in each hidden layer","val":true},{"ans":"C. Biases","val":false},{"ans":"D. Weights","val":false}],"q_expl":"If model parameters are variables that get adjusted by training with existing data, your hyperparameters are the variables about the training process itself. For example, part of setting up a deep neural network is deciding how many \u201chidden\u201c layers of nodes to use between the input layer and the output layer, as well as how many nodes each layer should use. These variables are not directly related to the training data at all. They are configuration variables. Another difference is that parameters change during a training job, while the hyperparameters are usually constant during a job. Weights and biases are variables that get adjusted during the training process, so they are not hyperparameters. Reference: https:\/\/cloud.google.com\/ml-engine\/docs\/hyperparameter-tuning-overview"},{"label":"test_13","q_format":"single","q_text":"You store historic data in Cloud Storage. You need to perform analytics on the historic data. You want to use a solution to detect invalid data entries and perform data transformations that will not require programming or knowledge of SQL.What should you do?","answers":[{"ans":"A. Use Cloud Dataflow with Beam to detect errors and perform transformations.","val":false},{"ans":"B. Use Cloud Dataprep with recipes to detect errors and perform transformations.","val":true},{"ans":"C. Use Cloud Dataproc with a Hadoop job to detect errors and perform transformations.","val":false},{"ans":"D. Use federated tables in BigQuery with queries to detect errors and perform transformations.","val":false}],"q_expl":"B, \u0093Cloud Dataprep by Trifacta is an intelligent data service for visually exploring, cleaning, and preparing structured and unstructured data for analysis, reporting, and machine learning\u0094 https:\/\/cloud.google.com\/dataprep\/"},{"label":"test_13","q_format":"single","q_text":"You want to automate execution of a multi-step data pipeline running on Google Cloud. The pipeline includes Cloud Dataproc and Cloud Dataflow jobs that have multiple dependencies on each other. You want to use managed services where possible, and the pipeline will run every day. Which tool should you use?","answers":[{"ans":"A. cron","val":false},{"ans":"B. Cloud Composer","val":true},{"ans":"C. Cloud Scheduler","val":false},{"ans":"D. Workflow Templates on Cloud Dataproc","val":false}],"q_expl":"B: Cloud Composer is a fully managed workflow orchestration service that empowers you to author, schedule, and monitor pipelines that span across clouds and on-premises data centres. https:\/\/cloud.google.com\/composer\/"},{"label":"test_13","q_format":"single","q_text":"Your United States-based company has created an application for assessing and responding to user actions. The primary table\u2018s data volume grows by 250,000 records per second. Many third parties use your application\u2018s APIs to build the functionality into their own frontend applications. Your application\u2018s APIs should comply with the following requirements:? Single global endpoint? ANSI SQL support? Consistent access to the most up-to-date dataWhat should you do?","answers":[{"ans":"A. Implement BigQuery with no region selected for storage or processing.","val":false},{"ans":"B. Implement Cloud Spanner with the leader in North America and read-only replicas in Asia and Europe.","val":true},{"ans":"C. Implement Cloud SQL for PostgreSQL with the master in Norht America and read replicas in Asia and Europe.","val":false},{"ans":"D. Implement Cloud Bigtable with the primary cluster in North America and secondary clusters in Asia and Europe.","val":false}],"q_expl":"B: Cloud Spanner is the first scalable, enterprise-grade, globally-distributed, and strongly consistent database service built for the cloud specifically to combine the benefits of relational database structure with non-relational horizontal scale. https:\/\/cloud.google.com\/spanner\/ Cloud Spanner is a fully managed, mission-critical, relational database service that offers transactional consistency at global scale, schemas, SQL (ANSI 2011 with extensions), and automatic, synchronous replication for high availability. https:\/\/cloud.google.com\/spanner\/docs\/ https:\/\/cloud.google.com\/spanner\/docs\/instances#available-configurations-multi-region"},{"label":"test_13","q_format":"single","q_text":"You want to archive data in Cloud Storage. Because some data is very sensitive, you want to use the \u201cTrust No One\u201c (TNO) approach to encrypt your data to prevent the cloud provider staff from decrypting your data. What should you do?","answers":[{"ans":"A. Use gcloud kms keys create to create a symmetric key. Then use gcloud kms encrypt to encrypt each archival file with the key and unique additional authenticated data (AAD). Use gsutil cp to upload each encrypted file to the Cloud Storage bucket, and keep the AAD outside of Google Cloud.","val":false},{"ans":"B. Use gcloud kms keys create to create a symmetric key. Then use gcloud kms encrypt to encrypt each archival file with the key. Use gsutil cp to upload each encrypted file to the Cloud Storage bucket. Manually destroy the key previously used for encryption, and rotate the key once.","val":false},{"ans":"C. Specify customer-supplied encryption key (CSEK) in the .boto configuration file. Use gsutil cp to upload each archival file to the Cloud Storage bucket. Save the CSEK in Cloud Memorystore as permanent storage of the secret.","val":false},{"ans":"D. Specify customer-supplied encryption key (CSEK) in the .boto configuration file. Use gsutil cp to upload each archival file to the Cloud Storage bucket. Save the CSEK in a different project that only the security team can access.","val":true}],"q_expl":"The correct answer must be D A and B can be eliminated immediately since kms generated keys are considered potentially accessible by CSP. C is incorrect because memory store is essentially a cache service. Additional authenticated data (AAD) acts as a \u201csalt\u201c, it is not a cipher."},{"label":"test_13","q_format":"single","q_text":"Does Dataflow process batch data pipelines or streaming data pipelines?","answers":[{"ans":"A. Only Batch Data Pipelines","val":false},{"ans":"B. Both Batch and Streaming Data Pipelines","val":true},{"ans":"C. Only Streaming Data Pipelines","val":false},{"ans":"D. None of the above","val":false}],"q_expl":"Dataflow is a unified processing model, and can execute both streaming and batch data pipelines Reference: https:\/\/cloud.google.com\/dataflow\/"},{"label":"test_13","q_format":"multiple","q_text":"What are two of the characteristics of using online prediction rather than batch prediction?","answers":[{"ans":"A. It is optimized to handle a high volume of data instances in a job and to run more complex models.","val":false},{"ans":"B. Predictions are returned in the response message.","val":true},{"ans":"C. Predictions are written to output files in a Cloud Storage location that you specify.","val":false},{"ans":"D. It is optimized to minimize the latency of serving predictions.","val":true}],"q_expl":"Online prediction \u2013 .Optimized to minimize the latency of serving predictions. .Predictions returned in the response message. Batch prediction \u2013 .Optimized to handle a high volume of instances in a job and to run more complex models. .Predictions written to output files in a Cloud Storage location that you specify. Reference: https:\/\/cloud.google.com\/ml-engine\/docs\/prediction-overview#online_prediction_versus_batch_prediction"},{"label":"test_13","q_format":"single","q_text":"You are implementing security best practices on your data pipeline. Currently, you are manually executing jobs as the Project Owner. You want to automate these jobs by taking nightly batch files containing non-public information from Google Cloud Storage, processing them with a Spark Scala job on a Google CloudDataproc cluster, and depositing the results into Google BigQuery.How should you securely run this workload?","answers":[{"ans":"A. Restrict the Google Cloud Storage bucket so only you can see the files","val":false},{"ans":"B. Grant the Project Owner role to a service account, and run the job with it","val":false},{"ans":"C. Use a service account with the ability to read the batch files and to write to BigQuery","val":true},{"ans":"D. Use a user account with the Project Viewer role on the Cloud Dataproc cluster to read the batch files and write to BigQuery","val":false}],"q_expl":"Data owners cant create jobs or queries. -> B out We need service Account -> D out Access only granting me does not solve the problem -> A out The answer is C. ( Minimum rights to perform the job)"},{"label":"test_13","q_format":"single","q_text":"Government regulations in your industry mandate that you have to maintain an auditable record of access to certain types of data. Assuming that all expiring logs will be archived correctly, where should you store data that is subject to that mandate?","answers":[{"ans":"A. Encrypted on Cloud Storage with user-supplied encryption keys. A separate decryption key will be given to each authorized user.","val":false},{"ans":"B. In a BigQuery dataset that is viewable only by authorized personnel, with the Data Access log used to provide the auditability.","val":true},{"ans":"C. In Cloud SQL, with separate database user names to each user. The Cloud SQL Admin activity logs will be used to provide the auditability.","val":false},{"ans":"D. In a bucket on Cloud Storage that is accessible only by an AppEngine service that collects user information and logs the access before providing a link to the bucket.","val":false}],"q_expl":"The questions says: 1. You need to \u201cmaintain an auditable record of access to certain types of data\u201c => you need to be able to access and audit that data 2. \u201cAssuming that all expiring logs will be archived correctly\u201c => means that all expired logs are \u201calready\u201c being archived correctly (e.g. in Google Cloud Storage). 3. \u201cwhere should you store data that is subject to that mandate?\u201c => where do you store the data **that you can audit to answer the question regarding data access logs (who accessed what, this is the questions you need to answer when regulatory department asks, so you need a suitable storage to answer this question)** Therefore the answer is B. You store the un-archived data in BigQuery and use BigQuery for audit purpose to answer questions regarding data access in the past. The old data that is no longer subject for data access questions is archived already, like described in the question."},{"label":"test_13","q_format":"single","q_text":"You are creating a new pipeline in Google Cloud to stream IoT data from Cloud Pub\/Sub through Cloud Dataflow to BigQuery. While previewing the data, you notice that roughly 2% of the data appears to be corrupt. You need to modify the Cloud Dataflow pipeline to filter out this corrupt data. What should you do?","answers":[{"ans":"A. Add a SideInput that returns a Boolean if the element is corrupt.","val":false},{"ans":"B. Add a ParDo transform in Cloud Dataflow to discard corrupt elements.","val":true},{"ans":"C. Add a Partition transform in Cloud Dataflow to separate valid data from corrupt data.","val":false},{"ans":"D. Add a GroupByKey transform in Cloud Dataflow to group all of the valid data together and discard the rest.","val":false}],"q_expl":"vote B :https:\/\/beam.apache.org\/documentation\/programming-guide\/#pardo Filtering a data set. You can use ParDo to consider each element in a PCollection and either output that element to a new collection or discard it. Formatting or type-converting each element in a data set. If your input PCollection contains elements that are of a different type or format than you want, you can use ParDo to perform a conversion on each element and output the result to a new PCollection. Extracting parts of each element in a data set. If you have a PCollection of records with multiple fields, for example, you can use a ParDo to parse out just the fields you want to consider into a new PCollection. Performing computations on each element in a data set. You can use ParDo to perform simple or complex computations on every element, or certain elements, of a PCollection and output the results as a new PCollection."},{"label":"test_13","q_format":"single","q_text":"You have an Apache Kafka cluster on-prem with topics containing web application logs. You need to replicate the data to Google Cloud for analysis in BigQuery and Cloud Storage. The preferred replication method is mirroring to avoid deployment of Kafka Connect plugins.What should you do?","answers":[{"ans":"A. Deploy a Kafka cluster on GCE VM Instances. Configure your on-prem cluster to mirror your topics to the cluster running in GCE. Use a Dataproc cluster or Dataflow job to read from Kafka and write to GCS.","val":true},{"ans":"B. Deploy a Kafka cluster on GCE VM Instances with the PubSub Kafka connector configured as a Sink connector. Use a Dataproc cluster or Dataflow job to read from Kafka and write to GCS.","val":false},{"ans":"C. Deploy the PubSub Kafka connector to your on-prem Kafka cluster and configure PubSub as a Source connector. Use a Dataflow job to read from PubSub and write to GCS.","val":false},{"ans":"D. Deploy the PubSub Kafka connector to your on-prem Kafka cluster and configure PubSub as a Sink connector. Use a Dataflow job to read from PubSub and write to GCS.","val":false}],"q_expl":"A. https:\/\/cwiki.apache.org\/confluence\/pages\/viewpage.action?pageId=27846330 The solution specifically mentions mirroring and minimizing the use of Kafka Connect plugin. D would be the more Google Cloud-native way of implementing the same, but the requirement is better met by A."},{"label":"test_13","q_format":"single","q_text":"Your team is responsible for developing and maintaining ETLs in your company. One of your Dataflow jobs is failing because of some errors in the input data, and you need to improve reliability of the pipeline (incl. being able to reprocess all failing data).What should you do?","answers":[{"ans":"A. Add a filtering step to skip these types of errors in the future, extract erroneous rows from logs. that transforms the data, extract erroneous rows from logs. . that can be stored to PubSub later.","val":false},{"ans":"B. Add a try\u0085 catch block to your DoFn that transforms the data, extract erroneous rows from logs.","val":false},{"ans":"C. Add a try\u0085 catch block to your DoFn that transforms the data, write erroneous rows to PubSub directly from the DoFn.","val":false},{"ans":"D. Add a try\u0085 catch block to your DoFn that transforms the data, use a sideOutput to create a PCollection that can be stored to PubSub later.","val":true}],"q_expl":"A. Add a filtering step to skip these types of errors in the future, extract erroneous rows from logs. \nB. Add a try\u0085 catch block to your DoFn that transforms the data, extract erroneous rows from logs. \nC. Add a try\u0085 catch block to your DoFn that transforms the data, write erroneous rows to PubSub directly from the DoFn. \n*D. Add a try\u0085 catch block to your DoFn that transforms the data, use a sideOutput to create a PCollection that can be stored to PubSub later. So correct option is D"},{"label":"test_13","q_format":"single","q_text":"Your company receives both batch- and stream-based event data. You want to process the data using Google Cloud Dataflow over a predictable time period.However, you realize that in some instances data can arrive late or out of order. How should you design your Cloud Dataflow pipeline to handle data that is late or out of order?","answers":[{"ans":"A. Set a single global window to capture all the data.","val":false},{"ans":"B. Set sliding windows to capture all the lagged data.","val":false},{"ans":"C. Use watermarks and timestamps to capture the lagged data.","val":true},{"ans":"D. Ensure every datasource type (stream or batch) has a timestamp, and use the timestamps to define the logic for lagged data.","val":false}],"q_expl":"Answer: C Description: A watermark is a threshold that indicates when Dataflow expects all of the data in a window to have arrived. If new data arrives with a timestamp that\u2018s in the window but older than the watermark, the data is considered late data."},{"label":"test_13","q_format":"single","q_text":"You are planning to migrate your current on-premises Apache Hadoop deployment to the cloud. You need to ensure that the deployment is as fault-tolerant and cost-effective as possible for long-running batch jobs. You want to use a managed service. What should you do?","answers":[{"ans":"A. Deploy a Cloud Dataproc cluster. Use a standard persistent disk and 50% preemptible workers. Store data in Cloud Storage, and change references in scripts from hdfs:\/\/ to gs:\/\/","val":true},{"ans":"B. Deploy a Cloud Dataproc cluster. Use an SSD persistent disk and 50% preemptible workers. Store data in Cloud Storage, and change references in scripts from hdfs:\/\/ to gs:\/\/","val":false},{"ans":"C. Install Hadoop and Spark on a 10-node Compute Engine instance group with standard instances. Install the Cloud Storage connector, and store the data in Cloud Storage. Change references in scripts from hdfs:\/\/ to gs:\/\/","val":false},{"ans":"D. Install Hadoop and Spark on a 10-node Compute Engine instance group with preemptible instances. Store data in HDFS. Change references in scripts from hdfs:\/\/ to gs:\/\/","val":false}],"q_expl":"A. Deploy a Cloud Dataproc cluster. Use a standard persistent disk and 50% preemptible workers. Store data in Cloud Storage, and change references in scripts from hdfs:\/\/ to gs:\/\/ \nLet\u2018s break down why this approach is ideal:\nManaged Service: Cloud Dataproc is a Google Cloud managed Hadoop and Spark service. It eliminates the need to manage and maintain the underlying infrastructure, reducing operational overhead.\nFault Tolerance: While a standard persistent disk might not offer the highest level of redundancy compared to SSDs, it provides some level of fault tolerance compared to relying solely on preemptible VMs.\nCost-Effectiveness: Utilizing 50% preemptible workers allows you to leverage significant cost savings. Preemptible VMs can be interrupted by Google Cloud when needed, but for long-running batch jobs, they offer a cost-effective option with a good balance of fault tolerance through the standard persistent disk.\nCloud Storage: Storing data in Cloud Storage provides scalability, durability, and cost-effectiveness compared to managing on-premises HDFS.\nScript modification: Changing script references from hdfs:\/\/ to gs:\/\/ is a necessary step to point your Hadoop jobs to the data location in Cloud Storage.\nHere\u2018s why other options are less suitable:\nB. Cloud Dataproc with SSD and Preemptible Workers: While SSDs offer better performance, they might increase the cost compared to standard persistent disks. With 50% preemptible workers, the standard disk should provide sufficient fault tolerance for long-running jobs.\nC. Self-managed Hadoop & Spark: Managing your own Hadoop and Spark cluster on Compute Engine requires significant operational overhead and wouldn\u2018t leverage the benefits of a managed service like Cloud Dataproc.\nD. Self-managed Hadoop & Spark with Preemptible VMs and HDFS: Using preemptible VMs for all instances can lead to job interruptions, potentially impacting long-running batch jobs. Additionally, managing your own HDFS in the cloud adds complexity compared to leveraging Cloud Storage."},{"label":"test_13","q_format":"single","q_text":"You receive data files in CSV format monthly from a third party. You need to cleanse this data, but every third month the schema of the files changes. Your requirements for implementing these transformations include:? Executing the transformations on a schedule? Enabling non-developer analysts to modify transformations? Providing a graphical tool for designing transformationsWhat should you do?","answers":[{"ans":"A. Use Cloud Dataprep to build and maintain the transformation recipes, and execute them on a scheduled basis","val":true},{"ans":"B. Load each month\u2018s CSV data into BigQuery, and write a SQL query to transform the data to a standard schema. Merge the transformed tables together with a SQL query","val":false},{"ans":"C. Help the analysts write a Cloud Dataflow pipeline in Python to perform the transformation. The Python code should be stored in a revision control system and modified as the incoming data\u2018s schema changes","val":false},{"ans":"D. Use Apache Spark on Cloud Dataproc to infer the schema of the CSV file before creating a Dataframe. Then implement the transformations in Spark SQL before writing the data out to Cloud Storage and loading into BigQuery","val":false}],"q_expl":"A. Use Cloud Dataprep to build and maintain the transformation recipes, and execute them on a scheduled basis\nHere\u2018s why Cloud Dataprep aligns well with your requirements:\nScheduled Transformations: Cloud Dataprep allows you to design data cleaning and transformation workflows visually. These workflows, called recipes, can be scheduled to run on a recurring basis (e.g., monthly) to process your incoming CSV files.\nNon-developer Friendly: Cloud Dataprep offers a user-friendly graphical interface. This empowers analysts, even those without extensive coding experience, to modify the data transformation steps within the recipes as the schema of the incoming files changes every third month.\nVisual Design Tool: The graphical interface provides a clear view of the data transformations applied in each step, making it easier for analysts to understand and modify the data cleaning process.\nOther options and why they are less suitable:\nB. BigQuery + SQL: While BigQuery can handle data transformations through SQL queries, managing schema changes and merging tables every month can become cumbersome. Additionally, BigQuery\u2018s interface might not be as intuitive for non-developers compared to Cloud Dataprep\u2018s visual tool.\nC. Cloud Dataflow (Python): Cloud Dataflow is a powerful service for building data pipelines, but it requires writing and maintaining Python code. This approach can be complex for non-programmers, especially when dealing with schema changes.\nD. Apache Spark on Cloud Dataproc: Using Spark on Dataproc requires programming expertise and isn\u2018t ideal for non-developers to manage schema changes. Additionally, writing the data out to Cloud Storage and then loading it into BigQuery adds unnecessary complexity."},{"label":"test_13","q_format":"single","q_text":"You\u2018re using Bigtable for a real-time application, and you have a heavy load that is a mix of read and writes. You\u2018ve recently identified an additional use case and need to perform hourly an analytical job to calculate certain statistics across the whole database. You need to ensure both the reliability of your production application as well as the analytical workload.What should you do?","answers":[{"ans":"A. Export Bigtable dump to GCS and run your analytical job on top of the exported files. profile for the analytics workload. profile for the analytics workload.","val":false},{"ans":"B. Add a second cluster to an existing instance with a multi-cluster routing, use live-traffic app profile for your regular workload and batch-analytics profile for the analytics workload.","val":true},{"ans":"C. Add a second cluster to an existing instance with a single-cluster routing, use live-traffic app profile for your regular workload and batch-analytics profile for the analytics workload.","val":false},{"ans":"D. Increase the size of your existing cluster twice and execute your analytics workload on your new resized cluster.","val":false}],"q_expl":"The best approach to ensure both the reliability of your production application and the analytical workload is to:\nB. Add a second cluster to an existing instance with a multi-cluster routing, use live-traffic app profile for your regular workload and batch-analytics profile for the analytics workload.\u00a0\nHere\u2019s why:\n\nMulti-cluster routing: This provides high availability and fault tolerance for your production application, as it allows traffic to be distributed across multiple clusters.\nSeparate clusters: By adding a second cluster with a different profile, you can optimize it for your analytical workload. This ensures that your analytical job doesn\u2019t impact the performance of your production application.\nBatch-analytics profile: The batch-analytics profile can be configured with lower consistency requirements and higher throughput, which is ideal for analytical workloads.\nLive-traffic app profile: The live-traffic app profile can be configured with higher consistency requirements and lower latency, which is suitable for your production application.\n\nThis approach provides a scalable and reliable solution for handling both your real-time application and analytical workloads.\nSource: https:\/\/cloud.google.com\/bigtable\/docs\/replication-overview"},{"label":"test_13","q_format":"single","q_text":"You work for a large fast food restaurant chain with over 400,000 employees. You store employee information in Google BigQuery in a Users table consisting of a FirstName field and a LastName field. A member of IT is building an application and asks you to modify the schema and data in BigQuery so the application can query a FullName field consisting of the value of the FirstName field concatenated with a space, followed by the value of the LastName field for each employee. How can you make that data available while minimizing cost?","answers":[{"ans":"A. Create a view in BigQuery that concatenates the FirstName and LastName field values to produce the FullName.","val":true},{"ans":"B. Add a new column called FullName to the Users table. Run an UPDATE statement that updates the FullName column for each user with the concatenation of the FirstName and LastName values.","val":false},{"ans":"D. Use BigQuery to export the data for the table to a CSV file. Create a Google Cloud Dataproc job to process the CSV file and output a new CSV file containing the proper values for FirstName, LastName and FullName. Run a BigQuery load job to load the new CSV file into BigQuery.","val":false},{"ans":"C. Create a Google Cloud Dataflow job that queries BigQuery for the entire Users table, concatenates the FirstName value and LastName value for each user, and loads the proper values for FirstName, LastName, and FullName into a new table in BigQuery.","val":false}],"q_expl":"The most cost-effective and efficient way to make the concatenated FullName data available in BigQuery is to:\nA. Create a view in BigQuery that concatenates the FirstName and LastName field values to produce the FullName.\nHere\u2019s why:\nNo data modification: Creating a view does not require modifying the existing data in the Users table, which can be expensive and time-consuming.\nMinimal resource usage: Views are virtual tables that don\u2019t store any data. They simply reference the underlying data in the Users table. This means they require minimal storage and processing resources.\nReal-time updates: Views are automatically updated when the underlying data changes, ensuring that the FullName data is always up-to-date.\nQuery efficiency: Views can be optimized using query hints and other techniques to improve query performance.\nThe other options are less efficient or more expensive:\nAdding a new column and updating data: This would require modifying the existing data, which can be expensive and time-consuming. It would also introduce additional complexity and maintenance overhead.\nUsing Dataflow or Dataproc: These options involve creating and running data processing jobs, which can be resource-intensive and time-consuming. They may also require additional infrastructure and management.\nExporting data to CSV: Exporting data to CSV and processing it using Dataproc would involve multiple steps and require additional resources. It would also be less efficient than using a view.\nTherefore, creating a view in BigQuery is the most cost-effective and efficient way to make the concatenated FullName data available for your application."},{"label":"test_13","q_format":"single","q_text":"You are migrating your data warehouse to BigQuery. You have migrated all of your data into tables in a dataset. Multiple users from your organization will be using the data. They should only see certain tables based on their team membership. How should you set user permissions?","answers":[{"ans":"A. Assign the users\/groups data viewer access at the table level for each table","val":false},{"ans":"B. Create SQL views for each team in the same dataset in which the data resides, and assign the users\/groups data viewer access to the SQL views","val":false},{"ans":"C. Create authorized views for each team in the same dataset in which the data resides, and assign the users\/groups data viewer access to the authorized views","val":true},{"ans":"D. Create authorized views for each team in datasets created for each team. Assign the authorized views data viewer access to the dataset in which the data resides. Assign the users\/groups data viewer access to the datasets in which the authorized views reside","val":false}],"q_expl":"C. Create authorized views for each team in the same dataset in which the data resides, and assign the users\/groups data viewer access to the authorized views.\nThis approach is the most secure and efficient way to implement team-based data access in BigQuery. Here\u2019s a breakdown of why:\nAuthorized Views:\n\nSecurity: Authorized views can restrict access to specific rows or columns within a table, ensuring that users only see the data they are authorized to view.\nPerformance: Authorized views can optimize query performance by pre-filtering data before it\u2019s returned to the user.\nSimplicity: By creating authorized views within the same dataset, you simplify permissions management and avoid the complexity of managing multiple datasets.\n\nAssigning Data Viewer Access to Authorized Views:\n\nGranular Control: This allows you to precisely control which users or groups can access specific subsets of data.\nReduced Risk: By limiting access to authorized views, you minimize the risk of unauthorized data exposure.\n\nBy combining authorized views with granular access controls, you can effectively implement a robust and flexible data security strategy for your BigQuery data warehouse."},{"label":"test_13","q_format":"single","q_text":"You are operating a streaming Cloud Dataflow pipeline. Your engineers have a new version of the pipeline with a different windowing algorithm and triggering strategy. You want to update the running pipeline with the new version. You want to ensure that no data is lost during the update. What should you do?","answers":[{"ans":"A. Update the Cloud Dataflow pipeline inflight by passing the --update option with the --jobName set to the existing job name","val":false},{"ans":"B. Update the Cloud Dataflow pipeline inflight by passing the --update option with the --jobName set to a new unique job name","val":false},{"ans":"C. Stop the Cloud Dataflow pipeline with the Cancel option. Create a new Cloud Dataflow job with the updated code","val":false},{"ans":"D. Stop the Cloud Dataflow pipeline with the Drain option. Create a new Cloud Dataflow job with the updated code","val":true}],"q_expl":"D. Stop the Cloud Dataflow pipeline with the Drain option. Create a new Cloud Dataflow job with the updated code.\nThis approach ensures that no data is lost during the update process:\n\nStop with Drain: This option allows the pipeline to finish processing all currently buffered data before shutting down. This guarantees that no data is lost during the transition.\nCreate a New Job: A new Cloud Dataflow job is created with the updated code, including the new windowing algorithm and triggering strategy. This allows for a seamless transition to the new pipeline without affecting the ongoing processing of the old pipeline.\n\nBy following this strategy, you can minimize data loss and ensure a smooth transition to the updated pipeline."},{"label":"test_13","q_format":"single","q_text":"You work for a manufacturing plant that batches application log files together into a single log file once a day at 2:00 AM. You have written a Google CloudDataflow job to process that log file. You need to make sure the log file in processed once per day as inexpensively as possible. What should you do?","answers":[{"ans":"A. Change the processing job to use Google Cloud Dataproc instead.","val":false},{"ans":"B. Manually start the Cloud Dataflow job each morning when you get into the office.","val":false},{"ans":"C. Create a cron job with Google App Engine Cron Service to run the Cloud Dataflow job.","val":true},{"ans":"D. Configure the Cloud Dataflow job as a streaming job so that it processes the log data immediately.","val":false}],"q_expl":"Answer is C. https:\/\/cloud.google.com\/appengine\/docs\/flexible\/nodejs\/scheduling-jobs-with-cron-yaml"},{"label":"test_13","q_format":"multiple","q_text":"You decided to use Cloud Datastore to ingest vehicle telemetry data in real time. You want to build a storage system that will account for the long-term data growth, while keeping the costs low. You also want to create snapshots of the data periodically, so that you can make a point-in-time (PIT) recovery, or clone a copy of the data for Cloud Datastore in a different environment. You want to archive these snapshots for a long time. Which two methods can accomplish this?(Choose two.)","answers":[{"ans":"A. Use managed export, and store the data in a Cloud Storage bucket using Nearline or Coldline class.","val":true},{"ans":"B. Use managed export, and then import to Cloud Datastore in a separate project under a unique namespace reserved for that export.","val":true},{"ans":"C. Use managed export, and then import the data into a BigQuery table created just for that export, and delete temporary export files.","val":false},{"ans":"D. Write an application that uses Cloud Datastore client libraries to read all the entities. Treat each entity as a BigQuery table row via BigQuery streaming insert. Assign an export timestamp for each export, and attach it as an extra column for each row. Make sure that the BigQuery table is partitioned using the export timestamp column.","val":false},{"ans":"E. Write an application that uses Cloud Datastore client libraries to read all the entities. Format the exported data into a JSON file. Apply compression before storing the data in Cloud Source Repositories.","val":false}],"q_expl":"https:\/\/cloud.google.com\/datastore\/docs\/export-import-entities"},{"label":"test_13","q_format":"single","q_text":"You work for a global shipping company. You want to train a model on 40 TB of data to predict which ships in each geographic region are likely to cause delivery delays on any given day. The model will be based on multiple attributes collected from multiple sources. Telemetry data, including location in GeoJSON format, will be pulled from each ship and loaded every hour. You want to have a dashboard that shows how many and which ships are likely to cause delays within a region. You want to use a storage solution that has native functionality for prediction and geospatial processing. Which storage solution should you use?","answers":[{"ans":"A. BigQuery","val":true},{"ans":"B. Cloud Bigtable","val":false},{"ans":"C. Cloud Datastore","val":false},{"ans":"D. Cloud SQL for PostgreSQL","val":false}],"q_expl":"BigQuery :\nScalability to handle 40 TB of data: BigQuery is a managed data warehouse designed to handle massive datasets efficiently. It can easily accommodate your 40 TB of data for training the prediction model.\nBuilt-in functionality for machine learning model training: BigQuery ML allows you to train machine learning models using familiar libraries like TensorFlow or scikit-learn directly within the platform. This eliminates the need for separate machine learning environments and simplifies model development.\nGeoJSON support for geospatial processing: BigQuery offers native support for GeoJSON, the format you\u2018re using for ship location data. This enables you to perform geospatial queries and analysis on your data directly within BigQuery.\nIntegration with Dataflow for streaming data ingestion: Cloud Dataflow is a managed service for building data pipelines. You can leverage Dataflow to ingest the telemetry data from ships every hour into BigQuery for real-time model updates.\nBigQuery ML allows creating and deploying models directly within BigQuery: Once your model is trained, you can deploy it as a BigQuery ML model. This allows you to generate predictions directly within BigQuery using your trained model.\nOther options and why they are less suitable:\nCloud Bigtable: While Bigtable excels in handling high-volume streaming data, it\u2018s not designed for large-scale data warehousing or machine learning model training.\nCloud Datastore: Cloud Datastore is a NoSQL database that can handle various data types, but it\u2018s not optimized for large-scale analytics or geospatial processing.\nCloud SQL for PostgreSQL: Cloud SQL for PostgreSQL is a managed relational database service. It\u2018s a great choice for storing relational data and running complex queries, but it wouldn\u2018t be suitable for training machine learning models or handling geospatial data at the scale you require."},{"label":"test_13","q_format":"single","q_text":"MJTelco Case Study -Company Overview -MJTelco is a startup that plans to build networks in rapidly growing, underserved markets around the world. The company has patents for innovative optical communications hardware. Based on these patents, they can create many reliable, high-speed backbone links with inexpensive hardware.Company Background -Founded by experienced telecom executives, MJTelco uses technologies originally developed to overcome communications challenges in space. Fundamental to their operation, they need to create a distributed data infrastructure that drives real-time analysis and incorporates machine learning to continuously optimize their topologies. Because their hardware is inexpensive, they plan to overdeploy the network allowing them to account for the impact of dynamic regional politics on location availability and cost.Their management and operations teams are situated all around the globe creating many-to-many relationship between data consumers and provides in their system. After careful consideration, they decided public cloud is the perfect environment to support their needs.Solution Concept -MJTelco is running a successful proof-of-concept (PoC) project in its labs. They have two primary needs:? Scale and harden their PoC to support significantly more data flows generated when they ramp to more than 50,000 installations.? Refine their machine-learning cycles to verify and improve the dynamic models they use to control topology definition.MJTelco will also use three separate operating environments \u201c\u201c development\/test, staging, and production \u201c\u201c to meet the needs of running experiments, deploying new features, and serving production customers.Business Requirements -? Scale up their production environment with minimal cost, instantiating resources when and where needed in an unpredictable, distributed telecom user community.? Ensure security of their proprietary data to protect their leading-edge machine learning and analysis.? Provide reliable and timely access to data for analysis from distributed research workers? Maintain isolated environments that support rapid iteration of their machine-learning models without affecting their customers.Technical Requirements -? Ensure secure and efficient transport and storage of telemetry data? Rapidly scale instances to support between 10,000 and 100,000 data providers with multiple flows each.? Allow analysis and presentation against data tables tracking up to 2 years of data storing approximately 100m records\/day? Support rapid iteration of monitoring infrastructure focused on awareness of data pipeline problems both in telemetry flows and in production learning cycles.CEO Statement -Our business model relies on our patents, analytics and dynamic machine learning. Our inexpensive hardware is organized to be highly reliable, which gives us cost advantages. We need to quickly stabilize our large distributed data pipelines to meet our reliability and capacity commitments.CTO Statement -Our public cloud services must operate as advertised. We need resources that scale and keep our data secure. We also need environments in which our data scientists can carefully study and quickly adapt our models. Because we rely on automation to process our data, we also need our development and test environments to work as we iterate.CFO Statement -The project is too large for us to maintain the hardware and software required for the data and analysis. Also, we cannot afford to staff an operations team to monitor so many data feeds, so we will rely on automation and infrastructure. Google Cloud\u2018s machine learning will allow our quantitative researchers to work on our high-value problems instead of problems with our data pipelines.MJTelco needs you to create a schema in Google Bigtable that will allow for the historical analysis of the last 2 years of records. Each record that comes in is sent every 15 minutes, and contains a unique identifier of the device and a data record. The most common query is for all the data for a given device for a given day.Which schema should you use?","answers":[{"ans":"A. Rowkey: date#device_id Column data: data_point","val":false},{"ans":"B. Rowkey: date Column data: device_id, data_point","val":false},{"ans":"C. Rowkey: device_id Column data: date, data_point","val":true},{"ans":"D. Rowkey: data_point Column data: device_id, date","val":false},{"ans":"E. Rowkey: date#data_point Column data: device_id","val":false}],"q_expl":"C. Rowkey: device_id Column data: date, data_point \nHere\u2018s why this schema is ideal for their needs:\nRowkey: device_id: This prioritizes efficient retrieval of all data points for a specific device. Given MJTelco\u2018s most common query is for all data from a particular device on a particular day, this design allows for quick retrieval by using the device ID as the row key.\nColumn data: date, data_point:\ndate: Storing the date within each data point allows for efficient filtering based on specific days or date ranges within a device\u2018s data.\ndata_point: This column stores the actual data record received every 15 minutes.\nOther options and why they are less suitable:\nA. Rowkey: date#device_id: While this design allows for filtering by date, retrieving all data for a specific device would require scanning through all rows for that date, which can be inefficient for a large number of devices.\nB. Rowkey: date: This design wouldn\u2018t allow for efficient retrieval of all data for a specific device.\nD. Rowkey: data_point: This wouldn\u2018t be ideal as the most common query is by device ID, not data point.\nE. Rowkey: date#data_point: This wouldn\u2018t allow for efficient retrieval of data for a specific device."},{"label":"test_13","q_format":"single","q_text":"You operate an IoT pipeline built around Apache Kafka that normally receives around 5000 messages per second. You want to use Google Cloud Platform to create an alert as soon as the moving average over 1 hour drops below 4000 messages per second. What should you do?","answers":[{"ans":"A. Consume the stream of data in Cloud Dataflow using Kafka IO. Set a sliding time window of 1 hour every 5 minutes. Compute the average when the window closes, and send an alert if the average is less than 4000 messages.","val":true},{"ans":"B. Consume the stream of data in Cloud Dataflow using Kafka IO. Set a fixed time window of 1 hour. Compute the average when the window closes, and send an alert if the average is less than 4000 messages.","val":false},{"ans":"C. Use Kafka Connect to link your Kafka message queue to Cloud Pub\/Sub. Use a Cloud Dataflow template to write your messages from Cloud Pub\/Sub to Cloud Bigtable. Use Cloud Scheduler to run a script every hour that counts the number of rows created in Cloud Bigtable in the last hour. If that number falls below 4000, send an alert.","val":false},{"ans":"D. Use Kafka Connect to link your Kafka message queue to Cloud Pub\/Sub. Use a Cloud Dataflow template to write your messages from Cloud Pub\/Sub to BigQuery. Use Cloud Scheduler to run a script every five minutes that counts the number of rows created in BigQuery in the last hour. If that number falls below 4000, send an alert.","val":false}],"q_expl":"Option A is the correct answer. Reasons:- a) Kafka IO and Dataflow is a valid option for interconnect (needless where Kafka is located \u2013 On Prem\/Google Cloud\/Other cloud) b) Sliding Window will help to calculate average. Option C and D are overkill and complex, considering the scenario in the question, https:\/\/cloud.google.com\/solutions\/processing-messages-from-kafka-hosted-outside-gcp"},{"label":"test_13","q_format":"multiple","q_text":"You want to migrate an on-premises Hadoop system to Cloud Dataproc. Hive is the primary tool in use, and the data format is Optimized Row Columnar (ORC).All ORC files have been successfully copied to a Cloud Storage bucket. You need to replicate some data to the cluster\u2018s local Hadoop Distributed File System(HDFS) to maximize performance. What are two ways to start using Hive in Cloud Dataproc? (Choose two.)","answers":[{"ans":"A. Run the gsutil utility to transfer all ORC files from the Cloud Storage bucket to HDFS. Mount the Hive tables locally.","val":false},{"ans":"B. Run the gsutil utility to transfer all ORC files from the Cloud Storage bucket to any node of the Dataproc cluster. Mount the Hive tables locally.","val":false},{"ans":"C. Run the gsutil utility to transfer all ORC files from the Cloud Storage bucket to the master node of the Dataproc cluster. Then run the Hadoop utility to copy them do HDFS. Mount the Hive tables from HDFS.","val":true},{"ans":"D. Leverage Cloud Storage connector for Hadoop to mount the ORC files as external Hive tables. Replicate external Hive tables to the native ones.","val":true},{"ans":"E. Load the ORC files into BigQuery. Leverage BigQuery connector for Hadoop to mount the BigQuery tables as external Hive tables. Replicate external Hive tables to the native ones.","val":false}],"q_expl":"C. Run the gsutil utility to transfer some ORC files from the Cloud Storage bucket to the master node of the Dataproc cluster. Then run the Hadoop utility to copy them do HDFS. Mount the Hive tables from HDFS. \nThis method leverages a combination of tools:\n* **gsutil**: The Google Cloud Storage command-line tool to transfer a subset of ORC files from Cloud Storage to the master node of your Dataproc cluster. Transferring only a subset initially can help assess performance before potentially moving the entire dataset.\n* **Hadoop utilities**: Once on the master node, you can utilize Hadoop utilities like \u2032distcp\u2032 to copy the ORC files from the local node storage to the cluster\u2018s HDFS.\n* **Hive table mounting**: After the data resides in HDFS, you can leverage Hive capabilities to mount these tables, allowing you to interact with the data using HiveQL queries.\nD. Leverage Cloud Storage connector for Hadoop to mount the ORC files as external Hive tables. Replicate external Hive tables to the native ones. \nThis approach utilizes the Cloud Storage connector for Hadoop:\n* **Cloud Storage connector**: This connector allows Hive to access data directly from Cloud Storage without physically moving the data to HDFS. \n* **External Hive tables**: By mounting the ORC files in Cloud Storage as external Hive tables, you can still query and interact with the data using HiveQL.\n* **Replication (optional)**: While using external Hive tables provides access to the data, for performance-critical workloads, you might consider replicating a subset of the data to HDFS for faster query execution. This can be done later as needed.\nOther options and why they are less suitable:\nA. Transferring all ORC files and mounting locally: While technically possible, transferring the entire dataset to all nodes might not be necessary, especially for initial exploration. Additionally, mounting tables locally might not be the most efficient approach for a distributed cluster.\nB. Transferring ORC files to any node: While it might work, transferring data to any node is less controlled compared to using the master node as a designated entry point.\nE. Loading into BigQuery: While BigQuery is a powerful tool, it might be an unnecessary step if the primary goal is to use Hive for data management and querying within the Cloud Dataproc environment."},{"label":"test_13","q_format":"single","q_text":"Which TensorFlow function can you use to configure a categorical column if you don\u2018t know all of the possible values for that column?","answers":[{"ans":"A. categorical_column_with_vocabulary_list","val":false},{"ans":"B. categorical_column_with_hash_bucket","val":true},{"ans":"C. categorical_column_with_unknown_values","val":false},{"ans":"D. sparse_column_with_keys","val":false}],"q_expl":"If you know the set of all possible feature values of a column and there are only a few of them, you can use categorical_column_with_vocabulary_list. Each key in the list will get assigned an auto-incremental ID starting from 0. What if we don\u2018t know the set of possible values in advance? Not a problem. We can use categorical_column_with_hash_bucket instead. What will happen is that each possible value in the feature column occupation will be hashed to an integer ID as we encounter them in training. Reference: https:\/\/www.tensorflow.org\/tutorials\/wide"},{"label":"test_13","q_format":"single","q_text":"You are developing a software application using Google\u2018s Dataflow SDK, and want to use conditional, for loops and other complex programming structures to create a branching pipeline. Which component will be used for the data processing operation?","answers":[{"ans":"A. PCollection","val":false},{"ans":"B. Transform","val":true},{"ans":"C. Pipeline","val":false},{"ans":"D. Sink API","val":false}],"q_expl":"In Google Cloud, the Dataflow SDK provides a transform component. It is responsible for the data processing operation. You can use conditional, for loops, and other complex programming structure to create a branching pipeline. Reference: https:\/\/cloud.google.com\/dataflow\/model\/programming-model"},{"label":"test_13","q_format":"single","q_text":"You want to build a managed Hadoop system as your data lake. The data transformation process is composed of a series of Hadoop jobs executed in sequence.To accomplish the design of separating storage from compute, you decided to use the Cloud Storage connector to store all input data, output data, and intermediary data. However, you noticed that one Hadoop job runs very slowly with Cloud Dataproc, when compared with the on-premises bare-metal Hadoop environment (8-core nodes with 100-GB RAM). Analysis shows that this particular Hadoop job is disk I\/O intensive. You want to resolve the issue. What should you do?","answers":[{"ans":"A. Allocate sufficient memory to the Hadoop cluster, so that the intermediary data of that particular Hadoop job can be held in memory","val":false},{"ans":"B. Allocate sufficient persistent disk space to the Hadoop cluster, and store the intermediate data of that particular Hadoop job on native HDFS","val":true},{"ans":"C. Allocate more CPU cores of the virtual machine instances of the Hadoop cluster so that the networking bandwidth for each instance can scale up","val":false},{"ans":"D. Allocate additional network interface card (NIC), and configure link aggregation in the operating system to use the combined throughput when working with Cloud Storage","val":false}],"q_expl":"Correct: B Local HDFS storage is a good option if: Your jobs require a lot of metadata operations\u0097for example, you have thousands of partitions and directories, and each file size is relatively small. You modify the HDFS data frequently or you rename directories. (Cloud Storage objects are immutable, so renaming a directory is an expensive operation because it consists of copying all objects to a new key and deleting them afterwards.) You heavily use the append operation on HDFS files. You have workloads that involve heavy I\/O. For example, you have a lot of partitioned writes, such as the following: spark.read().write.partitionBy(\u2026).parquet(\u201cgs:\/\/\u201c) You have I\/O workloads that are especially sensitive to latency. For example, you require single-digit millisecond latency per storage operation."},{"label":"test_13","q_format":"single","q_text":"You currently have a single on-premises Kafka cluster in a data center in the us-east region that is responsible for ingesting messages from IoT devices globally.Because large parts of globe have poor internet connectivity, messages sometimes batch at the edge, come in all at once, and cause a spike in load on yourKafka cluster. This is becoming difficult to manage and prohibitively expensive. What is the Google-recommended cloud native architecture for this scenario?","answers":[{"ans":"A. Edge TPUs as sensor devices for storing and transmitting the messages.","val":false},{"ans":"B. Cloud Dataflow connected to the Kafka cluster to scale the processing of incoming messages.","val":false},{"ans":"C. An IoT gateway connected to Cloud Pub\/Sub, with Cloud Dataflow to read and process the messages from Cloud Pub\/Sub.","val":true},{"ans":"D. A Kafka cluster virtualized on Compute Engine in us-east with Cloud Load Balancing to connect to the devices around the world.","val":false}],"q_expl":"C. An IoT gateway connected to Cloud Pub\/Sub, with Cloud Dataflow to read and process the messages from Cloud Pub\/Sub \nThis architecture leverages several Google Cloud Platform services for a scalable and cost-effective solution:\nIoT gateway: Deploy an IoT gateway at the edge near your sensor devices. This gateway can buffer and batch messages locally, reducing the impact of unreliable internet connectivity and minimizing the number of messages sent at once.\nCloud Pub\/Sub: Instead of a single on-premises Kafka cluster, utilize Cloud Pub\/Sub, a fully managed pub\/sub messaging service. Cloud Pub\/Sub is highly scalable and can handle bursty workloads from your global devices efficiently. It acts as a decoupling layer between the gateways and your data processing pipeline.\nCloud Dataflow: Implement Cloud Dataflow, a managed service for building data pipelines. Cloud Dataflow can automatically scale up and down to handle the varying volume of messages received from Cloud Pub\/Sub, ensuring cost-efficiency. It allows you to process the messages and route them to your desired destinations for further analysis or storage.\nOther options and why they are less suitable:\nA. Edge TPUs: Edge TPUs are machine learning accelerators, not suitable for storing and transmitting messages.\nB. Cloud Dataflow connected to the existing Kafka cluster: While Cloud Dataflow can be used for processing, scaling the existing Kafka cluster might not be the most cost-effective solution, especially with bursty workloads.\nD. Virtualized Kafka cluster: This approach replicates your on-premises setup in the cloud, potentially with higher costs and no inherent solution for the spiky workload issue. Additionally, Cloud Load Balancing might not be the most suitable solution for connecting globally distributed devices to a single Kafka cluster in the us-east region."},{"label":"test_13","q_format":"single","q_text":"Your neural network model is taking days to train. You want to increase the training speed. What can you do?","answers":[{"ans":"A. Subsample your test dataset.","val":false},{"ans":"B. Subsample your training dataset.","val":true},{"ans":"C. Increase the number of input features to your model.","val":false},{"ans":"D. Increase the number of layers in your neural network.","val":false}],"q_expl":"increase the number of layers will make the training slower https:\/\/www.quora.com\/Is-there-a-specific-reason-why-a-neural-network-with-more-layers-might-perform-worse-than-a-network-with-fewer-layers https:\/\/towardsdatascience.com\/how-to-train-neural-network-faster-with-optimizers-d297730b3713"},{"label":"test_13","q_format":"single","q_text":"You need to migrate a 2TB relational database to Google Cloud Platform. You do not have the resources to significantly refactor the application that uses this database and cost to operate is of primary concern.Which service do you select for storing and serving your data?","answers":[{"ans":"A. Cloud Spanner","val":false},{"ans":"B. Cloud Bigtable","val":false},{"ans":"C. Cloud Firestore","val":false},{"ans":"D. Cloud SQL","val":true}],"q_expl":"D. Cloud SQL \nHere\u2018s why Cloud SQL is the best fit for your scenario:\nRelational Database Support: Cloud SQL is a managed relational database service that supports popular relational database engines like MySQL, PostgreSQL, and SQL Server. Since you have an existing 2TB relational database, Cloud SQL offers compatibility and a smooth migration path.\nScalability: Cloud SQL offers various configurations to accommodate your 2TB database size. You can choose between shared core or dedicated core options, allowing you to scale storage and processing power based on your needs.\nCost-Effectiveness: Compared to Cloud Spanner, Cloud SQL generally provides a more cost-effective solution for established relational databases, especially when cost is a primary concern. Cloud Spanner offers strong consistency guarantees and horizontal scalability, which might be beneficial for specific use cases, but often come at a higher cost.\nOther options and why they are less suitable:\nA. Cloud Spanner: Cloud Spanner is a fully managed relational database service with strong consistency guarantees and horizontal scalability. However, it can be more expensive than Cloud SQL for established relational databases, especially for a 2TB size.\nB. Cloud Bigtable: Cloud Bigtable is a NoSQL database service ideal for storing large, sparse datasets with high write throughput. It\u2018s not designed for relational data structures and wouldn\u2018t be a suitable choice for migrating your relational database.\nC. Cloud Firestore: Cloud Firestore is a NoSQL document database designed for mobile and web applications. It wouldn\u2018t be appropriate for migrating a large relational database like yours."},{"label":"test_13","q_format":"multiple","q_text":"You want to use a database of information about tissue samples to classify future tissue samples as either normal or mutated. You are evaluating an unsupervised anomaly detection method for classifying the tissue samples. Which two characteristic support this method? (Choose two.)","answers":[{"ans":"A. There are very few occurrences of mutations relative to normal samples.","val":true},{"ans":"B. There are roughly equal occurrences of both normal and mutated samples in the database.","val":false},{"ans":"C. You expect future mutations to have different features from the mutated samples in the database.","val":false},{"ans":"D. You expect future mutations to have similar features to the mutated samples in the database.","val":true},{"ans":"E. You already have labels for which samples are mutated and which are normal in the database.","val":false}],"q_expl":"Anomaly detection has two basic assumptions: *Anomalies only occur very rarely in the data. *Their features differ from the normal instances significantly. Anomaly detection involves identifying rare data instances (anomalies) that come from a different class or distribution than the majority (which are simply called \u0093normal\u0094 instances). Given a training set of only normal data, the semi-supervised anomaly detection task is to identify anomalies in the future. Good solutions to this task have applications in fraud and intrusion detection. The unsupervised anomaly detection task is different: Given unlabeled, mostly-normal data, identify the anomalies among them. https:\/\/www.science.gov\/topicpages\/u\/unsupervised+anomaly+detection A because \u0093Unsupervised anomaly detection techniques detect anomalies in an unlabeled test data set under the assumption that the majority of the instances in the data set are normal\u0094, B is for Supervised anomaly detection https:\/\/en.wikipedia.org\/wiki\/Anomaly_detection"},{"label":"test_13","q_format":"single","q_text":"Your team is working on a binary classification problem. You have trained a support vector machine (SVM) classifier with default parameters, and received an area under the Curve (AUC) of 0.87 on the validation set. You want to increase the AUC of the model. What should you do?","answers":[{"ans":"A. Perform hyperparameter tuning","val":true},{"ans":"B. Train a classifier with deep neural networks, because neural networks would always beat SVMs","val":false},{"ans":"C. Deploy the model and measure the real-world AUC; it\u2018s always higher because of generalization","val":false},{"ans":"D. Scale predictions you get out of the model (tune a scaling factor as a hyperparameter) in order to get the highest AUC","val":false}],"q_expl":"https:\/\/towardsdatascience.com\/understanding-hyperparameters-and-its-optimisation-techniques-f0debba07568"},{"label":"test_13","q_format":"single","q_text":"Your company\u2018s customer and order databases are often under heavy load. This makes performing analytics against them difficult without harming operations.The databases are in a MySQL cluster, with nightly backups taken using mysqldump. You want to perform analytics with minimal impact on operations. What should you do?","answers":[{"ans":"A. Add a node to the MySQL cluster and build an OLAP cube there.","val":false},{"ans":"B. Use an ETL tool to load the data from MySQL into Google BigQuery.","val":false},{"ans":"C. Connect an on-premises Apache Hadoop cluster to MySQL and perform ETL.","val":false},{"ans":"D. Mount the backups to Google Cloud SQL, and then process the data using Google Cloud Dataproc.","val":true}],"q_expl":"D is best answer. Goal is to minimize analytics query load on live OLTP operations. A. Add a node to the MySQL cluster and build an OLAP cube there. This will not help. Cluster nodes will have identical data. B. Use an ETL tool to load the data from MySQL into Google BigQuery. This approach still puts a query load on SQL server. C. Connect an on-premises Apache Hadoop cluster to MySQL and perform ETL. This approach also puts a query load on SQL server. https:\/\/www.examtopics.com\/exams\/google\/professional-data-engineer\/view\/6\/# D. Mount the backups to Google Cloud SQL, and then process the data using Google Cloud Dataproc. In this approach there is read query impact on production SQL instances"},{"label":"test_13","q_format":"single","q_text":"You work on a regression problem in a natural language processing domain, and you have 100M labeled exmaples in your dataset. You have randomly shuffled your data and split your dataset into train and test samples (in a 90\/10 ratio). After you trained the neural network and evaluated your model on a test set, you discover that the root-mean-squared error (RMSE) of your model is twice as high on the train set as on the test set. How should you improve the performance of your model?","answers":[{"ans":"A. Increase the share of the test sample in the train-test split.","val":false},{"ans":"B. Try to collect more data and increase the size of your dataset.","val":false},{"ans":"C. Try out regularization techniques (e.g., dropout of batch normalization) to avoid overfitting.","val":false},{"ans":"D. Increase the complexity of your model by, e.g., introducing an additional layer or increase sizing the size of vocabularies or n-grams used.","val":true}],"q_expl":"D: A is incorrect since test sample is large enough. B is incorrect since dataset is pretty large already, and having more data typically helps with overfitting and not with underfitting. C is incorrect since regularization helps to avoid overfitting and we have a clear underfitting case. D is correct since increasing model complexity generally helps when you have an underfitting problem."},{"label":"test_13","q_format":"single","q_text":"Which of the following statements about Legacy SQL and Standard SQL is not true?","answers":[{"ans":"A. Standard SQL is the preferred query language for BigQuery.","val":false},{"ans":"B. If you write a query in Legacy SQL, it might generate an error if you try to run it with Standard SQL.","val":false},{"ans":"C. One difference between the two query languages is how you specify fully-qualified table names (i.e. table names that include their associated project name).","val":false},{"ans":"D. You need to set a query language for each dataset and the default is Standard SQL.","val":true}],"q_expl":"You do not set a query language for each dataset. It is set each time you run a query and the default query language is Legacy SQL. Standard SQL has been the preferred query language since BigQuery 2.0 was released. In legacy SQL, to query a table with a project-qualified name, you use a colon, :, as a separator. In standard SQL, you use a period, ., instead. Due to the differences in syntax between the two query languages (such as with project-qualified table names), if you write a query in Legacy SQL, it might generate an error if you try to run it with Standard SQL. Reference: https:\/\/cloud.google.com\/bigquery\/docs\/reference\/standard-sql\/migrating-from-legacy-sql"},{"label":"test_13","q_format":"single","q_text":"You want to use a BigQuery table as a data sink. In which writing mode(s) can you use BigQuery as a sink?","answers":[{"ans":"A. Both batch and streaming","val":true},{"ans":"B. BigQuery cannot be used as a sink","val":false},{"ans":"C. Only batch","val":false},{"ans":"D. Only streaming","val":false}],"q_expl":"When you apply a BigQueryIO.Write transform in batch mode to write to a single table, Dataflow invokes a BigQuery load job. When you apply a BigQueryIO.Write transform in streaming mode or in batch mode using a function to specify the destination table, Dataflow uses BigQuery\u2018s streaming inserts Reference: https:\/\/cloud.google.com\/dataflow\/model\/bigquery-io"},{"label":"test_13","q_format":"single","q_text":"Flowlogistic Case Study \u2013 Company Overview \u2013 Flowlogistic is a leading logistics and supply chain provider. They help businesses throughout the world manage their resources and transport them to their final destination. The company has grown rapidly, expanding their offerings to include rail, truck, aircraft, and oceanic shipping. Company Background \u2013 The company started as a regional trucking company, and then expanded into other logistics market. Because they have not updated their infrastructure, managing and tracking orders and shipments has become a bottleneck. To improve operations, Flowlogistic developed proprietary technology for tracking shipments in real time at the parcel level. However, they are unable to deploy it because their technology stack, based on Apache Kafka, cannot support the processing volume. In addition, Flowlogistic wants to further analyze their orders and shipments to determine how best to deploy their resources. Solution Concept \u2013 Flowlogistic wants to implement two concepts using the cloud: ? Use their proprietary technology in a real-time inventory-tracking system that indicates the location of their loads ? Perform analytics on all their orders and shipment logs, which contain both structured and unstructured data, to determine how best to deploy resources, which markets to expand info. They also want to use predictive analytics to learn earlier when a shipment will be delayed. Existing Technical Environment \u2013 Flowlogistic architecture resides in a single data center: ? Databases \u2013 8 physical servers in 2 clusters \u2013 SQL Server \u201c\u201c user data, inventory, static data \u2013 3 physical servers \u2013 Cassandra \u201c\u201c metadata, tracking messages 10 Kafka servers \u201c\u201c tracking message aggregation and batch insert ? Application servers \u201c\u201c customer front end, middleware for order\/customs \u2013 60 virtual machines across 20 physical servers \u2013 Tomcat \u201c\u201c Java services \u2013 Nginx \u201c\u201c static content \u2013 Batch servers ? Storage appliances \u2013 iSCSI for virtual machine (VM) hosts \u2013 Fibre Channel storage area network (FC SAN) \u201c\u201c SQL server storage Network-attached storage (NAS) image storage, logs, backups ? 10 Apache Hadoop \/Spark servers \u2013 Core Data Lake \u2013 Data analysis workloads ? 20 miscellaneous servers \u2013 Jenkins, monitoring, bastion hosts, Business Requirements \u2013 Build a reliable and reproducible environment with scaled panty of production. ? Aggregate data in a centralized Data Lake for analysis ? Use historical data to perform predictive analytics on future shipments ? Accurately track every shipment worldwide using proprietary technology ? Improve business agility and speed of innovation through rapid provisioning of new resources ? Analyze and optimize architecture for performance in the cloud ? Migrate fully to the cloud if all other requirements are met Technical Requirements \u2013 ? Handle both streaming and batch data ? Migrate existing Hadoop workloads ? Ensure architecture is scalable and elastic to meet the changing demands of the company. ? Use managed services whenever possible ? Encrypt data flight and at rest Connect a VPN between the production data center and cloud environment SEO Statement \u2013 We have grown so quickly that our inability to upgrade our infrastructure is really hampering further growth and efficiency. We are efficient at moving shipments around the world, but we are inefficient at moving data around. We need to organize our information so we can more easily understand where our customers are and what they are shipping. CTO Statement \u2013 IT has never been a priority for us, so as our data has grown, we have not invested enough in our technology. I have a good staff to manage IT, but they are so busy managing our infrastructure that I cannot get them to do the things that really matter, such as organizing our data, building the analytics, and figuring out how to implement the CFO\u2018 s tracking technology. CFO Statement \u2013 Part of our competitive advantage is that we penalize ourselves for late shipments and deliveries. Knowing where out shipments are at all times has a direct correlation to our bottom line and profitability. Additionally, I don\u2018t want to commit capital to building out a server environment. Flowlogistic\u2018s management has determined that the current Apache Kafka servers cannot handle the data volume for their real-time inventory tracking system. You need to build a new system on Google Cloud Platform (GCP) that will feed the proprietary tracking software. The system must be able to ingest data from a variety of global sources, process and query in real-time, and store the data reliably. Which combination of GCP products should you choose?","answers":[{"ans":"A. Cloud Pub\/Sub, Cloud Dataflow, and Cloud Storage","val":true},{"ans":"B. Cloud Pub\/Sub, Cloud Dataflow, and Local SSD","val":false},{"ans":"C. Cloud Pub\/Sub, Cloud SQL, and Cloud Storage","val":false},{"ans":"D. Cloud Load Balancing, Cloud Dataflow, and Cloud Storage","val":false},{"ans":"E. Cloud Dataflow, Cloud SQL, and Cloud Storage","val":false}],"q_expl":"Correct option:  A. The problem statement \u201cFlowlogistic\u2018s management has determined that the current Apache Kafka servers cannot handle the data volume for their real-time inventory tracking system. As it says, \u201cwe cannot determine the data volume\u201c, but it doesn\u2018t say that we can\u2018t calculate it either. Requirement definition: The system must be able to ingest data from a variety of global sources process and query in real-time Store the data reliably. It says above, if you look at the Google page. Logging to multiple systems. for example, a Google Compute Engine instance can write logs to a monitoring system, to a database for later querying, and so on. https:\/\/cloud.google.com\/pubsub\/docs\/overview#scenarios stream processing with Dataflow https:\/\/cloud.google.com\/pubsub\/docs\/pubsub-dataflow?hl=en-419 The answer is A, since it is stated above."},{"label":"test_13","q_format":"single","q_text":"You operate a database that stores stock trades and an application that retrieves average stock price for a given company over an adjustable window of time. The data is stored in Cloud Bigtable where the datetime of the stock trade is the beginning of the row key. Your application has thousands of concurrent users, and you notice that performance is starting to degrade as more stocks are added. What should you do to improve the performance of your application?","answers":[{"ans":"A. Change the row key syntax in your Cloud Bigtable table to begin with the stock symbol.","val":true},{"ans":"B. Change the row key syntax in your Cloud Bigtable table to begin with a random number per second.","val":false},{"ans":"C. Change the data pipeline to use BigQuery for storing stock trades, and update your application.","val":false},{"ans":"D. Use Cloud Dataflow to write summary of each day\u2018s stock trades to an Avro file on Cloud Storage. Update your application to read from Cloud Storage and Cloud Bigtable to compute the responses.","val":false}],"q_expl":"Option A. Below document explains Having EXCHANGE and SYMBOL in the leading positions in the row key will naturally distribute activity. https:\/\/cloud.google.com\/bigtable\/docs\/schema-design-time-series"},{"label":"test_13","q_format":"single","q_text":"You have a query that filters a BigQuery table using a WHERE clause on timestamp and ID columns. By using bq query \u201c\u201c -dry_run you learn that the query triggers a full scan of the table, even though the filter on timestamp and ID select a tiny fraction of the overall data. You want to reduce the amount of data scanned by BigQuery with minimal changes to existing SQL queries. What should you do?","answers":[{"ans":"A. Create a separate table for each ID.","val":false},{"ans":"B. Use the LIMIT keyword to reduce the number of rows returned.","val":false},{"ans":"C. Recreate the table with a partitioning column and clustering column.","val":true},{"ans":"D. Use the bq query - -maximum_bytes_billed flag to restrict the number of bytes billed.","val":false}],"q_expl":"https:\/\/cloud.google.com\/bigquery\/docs\/best-practices-costs"},{"label":"test_13","q_format":"single","q_text":"You architect a system to analyze seismic data. Your extract, transform, and load (ETL) process runs as a series of MapReduce jobs on an Apache Hadoop cluster. The ETL process takes days to process a data set because some steps are computationally expensive. Then you discover that a sensor calibration step has been omitted. How should you change your ETL process to carry out sensor calibration systematically in the future?","answers":[{"ans":"A. Modify the transformMapReduce jobs to apply sensor calibration before they do anything else.","val":true},{"ans":"B. Introduce a new MapReduce job to apply sensor calibration to raw data, and ensure all other MapReduce jobs are chained after this.","val":false},{"ans":"C. Add sensor calibration data to the output of the ETL process, and document that all users need to apply sensor calibration themselves.","val":false},{"ans":"D. Develop an algorithm through simulation to predict variance of data output from the last MapReduce job based on calibration factors, and apply the correction to all data.","val":false}],"q_expl":"A. Modify the transform MapReduce jobs to apply sensor calibration before they do anything else. \nHere\u2018s why this solution is ideal:\nIntegration with Existing Process: Modifying the existing transform MapReduce jobs ensures the calibration step becomes an integral part of the data processing pipeline. This avoids the need for additional jobs or external data manipulation.\nEarly Correction: Applying calibration at the beginning of the transformation phase guarantees that all subsequent calculations and analysis are performed on the corrected data.\nOther options and why they are less suitable:\nB. New MapReduce Job: While introducing a separate job for calibration might be feasible, it adds complexity to the ETL process and requires additional job chaining logic.\nC. Calibration Data Output: Adding calibration data to the output wouldn\u2018t be ideal. Users would still need to manually apply the calibration, introducing an error-prone step and potentially requiring additional data manipulation depending on the format of the calibration data.\nD. Predictive Algorithm: Developing an algorithm to predict data variance based on calibration factors is complex and might not be entirely accurate. It\u2018s more efficient to directly apply the calibration during data transformation."},{"label":"test_13","q_format":"single","q_text":"MJTelco Case Study -Company Overview -MJTelco is a startup that plans to build networks in rapidly growing, underserved markets around the world. The company has patents for innovative optical communications hardware. Based on these patents, they can create many reliable, high-speed backbone links with inexpensive hardware.Company Background -Founded by experienced telecom executives, MJTelco uses technologies originally developed to overcome communications challenges in space. Fundamental to their operation, they need to create a distributed data infrastructure that drives real-time analysis and incorporates machine learning to continuously optimize their topologies. Because their hardware is inexpensive, they plan to overdeploy the network allowing them to account for the impact of dynamic regional politics on location availability and cost.Their management and operations teams are situated all around the globe creating many-to-many relationship between data consumers and provides in their system. After careful consideration, they decided public cloud is the perfect environment to support their needs.Solution Concept -MJTelco is running a successful proof-of-concept (PoC) project in its labs. They have two primary needs:? Scale and harden their PoC to support significantly more data flows generated when they ramp to more than 50,000 installations.? Refine their machine-learning cycles to verify and improve the dynamic models they use to control topology definition.MJTelco will also use three separate operating environments \u201c\u201c development\/test, staging, and production \u201c\u201c to meet the needs of running experiments, deploying new features, and serving production customers.Business Requirements -? Scale up their production environment with minimal cost, instantiating resources when and where needed in an unpredictable, distributed telecom user community.? Ensure security of their proprietary data to protect their leading-edge machine learning and analysis.? Provide reliable and timely access to data for analysis from distributed research workers? Maintain isolated environments that support rapid iteration of their machine-learning models without affecting their customers.Technical Requirements -Ensure secure and efficient transport and storage of telemetry dataRapidly scale instances to support between 10,000 and 100,000 data providers with multiple flows each.Allow analysis and presentation against data tables tracking up to 2 years of data storing approximately 100m records\/daySupport rapid iteration of monitoring infrastructure focused on awareness of data pipeline problems both in telemetry flows and in production learning cycles.CEO Statement -Our business model relies on our patents, analytics and dynamic machine learning. Our inexpensive hardware is organized to be highly reliable, which gives us cost advantages. We need to quickly stabilize our large distributed data pipelines to meet our reliability and capacity commitments.CTO Statement -Our public cloud services must operate as advertised. We need resources that scale and keep our data secure. We also need environments in which our data scientists can carefully study and quickly adapt our models. Because we rely on automation to process our data, we also need our development and test environments to work as we iterate.CFO Statement -The project is too large for us to maintain the hardware and software required for the data and analysis. Also, we cannot afford to staff an operations team to monitor so many data feeds, so we will rely on automation and infrastructure. Google Cloud\u2018s machine learning will allow our quantitative researchers to work on our high-value problems instead of problems with our data pipelines.You need to compose visualization for operations teams with the following requirements:? Telemetry must include data from all 50,000 installations for the most recent 6 weeks (sampling once every minute)? The report must not be more than 3 hours delayed from live data.? The actionable report should only show suboptimal links.? Most suboptimal links should be sorted to the top.? Suboptimal links can be grouped and filtered by regional geography.? User response time to load the report must be <5 seconds.You create a data source to store the last 6 weeks of data, and create visualizations that allow viewers to see multiple date ranges, distinct geographic regions, and unique installation types. You always show the latest data without any changes to your visualizations. You want to avoid creating and updating new visualizations each month. What should you do?","answers":[{"ans":"A. Look through the current data and compose a series of charts and tables, one for each possible combination of criteria.","val":false},{"ans":"B. Look through the current data and compose a small set of generalized charts and tables bound to criteria filters that allow value selection.","val":true},{"ans":"C. Export the data to a spreadsheet, compose a series of charts and tables, one for each possible combination of criteria, and spread them across multiple tabs.","val":false},{"ans":"D. Load the data into relational database tables, write a Google App Engine application that queries all rows, summarizes the data across each criteria, and then renders results using the Google Charts and visualization API.","val":false}],"q_expl":"B. Look through the current data and compose a small set of generalized charts and tables bound to criteria filters that allow value selection. \nHere\u2018s why this solution is ideal:\nDynamic Data Exploration: By creating a small set of generalized charts and tables, you avoid the need for a static set of visualizations for every possible combination of criteria. Instead, these charts and tables should be designed to be adaptable based on user selections through filters.\nCriteria-based Filtering: Implement filters that allow users to choose specific date ranges, geographic regions, and installation types. This enables them to explore the data dynamically without requiring pre-built visualizations for every single combination.\nScalability: This approach scales well as new criteria or data types might be introduced in the future. You can modify the filters and charts to accommodate the new information without needing to completely redesign the visualizations.\nLive Data Display: Ensure the visualizations are bound to a data source that reflects the latest data (within the 3-hour delay requirement). This allows users to see the most recent suboptimal links.\nOther options and why they are less suitable:\nA. Static Charts & Tables: Creating a static set of charts and tables for every possible combination of criteria would be very time-consuming to maintain and wouldn\u2018t allow for dynamic exploration.\nC. Spreadsheet & Charts: Using a spreadsheet wouldn\u2018t be scalable and wouldn\u2018t provide a user-friendly interface for filtering and exploring data.\nD. Relational Database & App Engine: While a relational database can store the data, writing a custom application to query and render results would be more complex compared to using pre-built filtering and visualization tools. Additionally, querying all rows might not be the most efficient approach, especially for large datasets."},{"label":"test_13","q_format":"single","q_text":"Why do you need to split a machine learning dataset into training data and test data?","answers":[{"ans":"A. So you can try two different sets of features","val":false},{"ans":"B. To make sure your model is generalized for more than just the training data","val":true},{"ans":"C. To allow you to create unit tests in your code","val":false},{"ans":"D. So you can use one dataset for a wide model and one for a deep model","val":false}],"q_expl":"The flaw with evaluating a predictive model on training data is that it does not inform you on how well the model has generalized to new unseen data. A model that is selected for its accuracy on the training dataset rather than its accuracy on an unseen test dataset is very likely to have lower accuracy on an unseen test dataset. The reason is that the model is not as generalized. It has specialized to the structure in the training dataset. This is called overfitting. Reference: https:\/\/machinelearningmastery.com\/a-simple-intuition-for-overfitting\/"},{"label":"test_13","q_format":"single","q_text":"You work for a manufacturing company that sources up to 750 different components, each from a different supplier. You\u2018ve collected a labeled dataset that has on average 1000 examples for each unique component. Your team wants to implement an app to help warehouse workers recognize incoming components based on a photo of the component. You want to implement the first working version of this app (as Proof-Of-Concept) within a few working days. What should you do?","answers":[{"ans":"A. Use Cloud Vision AutoML with the existing dataset.","val":true},{"ans":"B. Use Cloud Vision AutoML, but reduce your dataset twice.","val":false},{"ans":"C. Use Cloud Vision API by providing custom labels as recognition hints.","val":false},{"ans":"D. Train your own image recognition model leveraging transfer learning techniques.","val":false}],"q_expl":"A. Use Cloud Vision AutoML with the existing dataset. \nHere\u2018s why Cloud Vision AutoML is the best fit:\nRapid Development: Cloud Vision AutoML is a managed service that allows you to train a custom image recognition model without extensive coding or machine learning expertise. This is ideal for creating a PoC within a few working days.\nLarge Dataset Support: Cloud Vision AutoML can handle large datasets like yours, with 1000 examples per component (on average) totaling potentially millions of images.\nEase of Use: The user-friendly interface of Cloud Vision AutoML simplifies the process of uploading your labeled dataset, training the model, and deploying it for real-time predictions.\nOther options and why they are less suitable:\nB. Reduce Dataset Size: While reducing the dataset size might speed up training slightly, it could negatively impact the model\u2018s accuracy. With a large dataset, Cloud Vision AutoML can potentially achieve high accuracy without needing significant reduction.\nC. Cloud Vision API with Custom Labels: Cloud Vision API offers pre-trained models for various tasks, but it wouldn\u2018t be ideal for your specific need of recognizing 750 unique components. You would need to create custom labels and potentially build additional logic around the API for recognition. This approach would require more development time compared to Cloud Vision AutoML.\nD. Train Your Own Model: Training your own image recognition model from scratch using transfer learning techniques would be the most time-consuming option. While it could potentially offer more control, it requires significant machine learning expertise and wouldn\u2018t be feasible for a quick PoC."},{"label":"test_13","q_format":"multiple","q_text":"Business owners at your company have given you a database of bank transactions. Each row contains the user ID, transaction type, transaction location, and transaction amount. They ask you to investigate what type of machine learning can be applied to the data. Which three machine learning applications can you use? (Choose three.)","answers":[{"ans":"A. Supervised learning to determine which transactions are most likely to be fraudulent.","val":false},{"ans":"B. Unsupervised learning to determine which transactions are most likely to be fraudulent.","val":true},{"ans":"C. Clustering to divide the transactions into N categories based on feature similarity.","val":true},{"ans":"D. Supervised learning to predict the location of a transaction.","val":true},{"ans":"E. Reinforcement learning to predict the location of a transaction.","val":false},{"ans":"F. Unsupervised learning to predict the location of a transaction.","val":false}],"q_expl":"BCD makes sense, but I now agree that BCE is the correct answer. Say the model predict a location, guessing US or Sweden are both wrong when the answer is Canada. But US is closer, the distance from the correct location can be used to calculate a reward. Through reinforcement learning (E) the model could guess a location with better accuracy than supervised (D)."},{"label":"test_13","q_format":"single","q_text":"Which SQL keyword can be used to reduce the number of columns processed by BigQuery?","answers":[{"ans":"A. BETWEEN","val":false},{"ans":"B. WHERE","val":false},{"ans":"C. SELECT","val":true},{"ans":"D. LIMIT","val":false}],"q_expl":"SELECT allows you to query specific columns rather than the whole table. LIMIT, BETWEEN, and WHERE clauses will not reduce the number of columns processed by BigQuery. Reference: https:\/\/cloud.google.com\/bigquery\/launch-checklist#architecture_design_and_development_checklist"},{"label":"test_14","q_format":"single","q_text":"Which Cloud Dataflow \/ Beam feature should you use to aggregate data in an unbounded data source every hour based on the time when the data entered the pipeline?","answers":[{"ans":"A. An hourly watermark","val":false},{"ans":"B. An event time trigger","val":false},{"ans":"C. The with Allowed Lateness method","val":false},{"ans":"D. A processing time trigger","val":true}],"q_expl":"When collecting and grouping data into windows, Beam uses triggers to determine when to emit the aggregated results of each window. Processing time triggers. These triggers operate on the processing time the time when the data element is processed at any given stage in the pipeline. Event time triggers. These triggers operate on the event time, as indicated by the timestamp on each data element. Beams default trigger is event time-based. Reference: https:\/\/beam.apache.org\/documentation\/programming-guide\/#triggers"},{"label":"test_14","q_format":"single","q_text":"An organization maintains a Google BigQuery dataset that contains tables with user-level data. They want to expose aggregates of this data to other GoogleCloud projects, while still controlling access to the user-level data. Additionally, they need to minimize their overall storage cost and ensure the analysis cost for other projects is assigned to those projects. What should they do?","answers":[{"ans":"A. Create and share an authorized view that provides the aggregate results.","val":true},{"ans":"B. Create and share a new dataset and view that provides the aggregate results.","val":false},{"ans":"C. Create and share a new dataset and table that contains the aggregate results.","val":false},{"ans":"D. Create dataViewer Identity and Access Management (IAM) roles on the dataset to enable sharing.","val":false}],"q_expl":"Option A is Correct ans. Reason \u2013 In Question they want 2 things 1. \u201cstill controlling access to the user-level data\u201c Only Aggregation need to be given. 2. \u201censure the analysis cost for other projects is assigned to those projects.\u201c What this statement means is they need to ensure the Query job runner on auth view need to be charged. It is possible on Authorized view. Read this Useful SO Thread to understand \u2013 https:\/\/stackoverflow.com\/questions\/52201034\/bigquery-authorized-view-cost-billing-account Then why not Option B. Reason is It is not satisfying the 1st Requirement. If you share only view the Query runner still able to see the base table data. It can be controlled by IAM but remember it is another project you are sharing the view. So It might not be possible."},{"label":"test_14","q_format":"single","q_text":"You are designing a basket abandonment system for an ecommerce company. The system will send a message to a user based on these rules:? No interaction by the user on the site for 1 hour? Has added more than $30 worth of products to the basket? Has not completed a transactionYou use Google Cloud Dataflow to process the data and decide if a message should be sent. How should you design the pipeline?","answers":[{"ans":"A. Use a fixed-time window with a duration of 60 minutes.","val":false},{"ans":"B. Use a sliding time window with a duration of 60 minutes.","val":false},{"ans":"C. Use a session window with a gap time duration of 60 minutes.","val":true},{"ans":"D. Use a global window with a time based trigger with a delay of 60 minutes.","val":false}],"q_expl":"The correct answer is C. There are 3 windowing concepts in dataflow and each can be used for below use case 1) Fixed window 2) Sliding window and 3) Session window. Fixed window = any aggregation use cases, any batch analysis of data, relatively simple use cases. Sliding window = Moving averages of data Session window = user session data, click data and real time gaming analysis. The question here is about user session data and hence session window."},{"label":"test_14","q_format":"single","q_text":"You use BigQuery as your centralized analytics platform. New data is loaded every day, and an ETL pipeline modifies the original data and prepares it for the final users. This ETL pipeline is regularly modified and can generate errors, but sometimes the errors are detected only after 2 weeks. You need to provide a method to recover from these errors, and your backups should be optimized for storage costs. How should you organize your data in BigQuery and store your backups?","answers":[{"ans":"A. Organize your data in a single table, export, and compress and store the BigQuery data in Cloud Storage.","val":false},{"ans":"B. Organize your data in separate tables for each month, and export, compress, and store the data in Cloud Storage.","val":true},{"ans":"C. Organize your data in separate tables for each month, and duplicate your data on a separate dataset in BigQuery.","val":false},{"ans":"D. Organize your data in separate tables for each month, and use snapshot decorators to restore the table to a time prior to the corruption.","val":false}],"q_expl":"B seems the best solution (but C is also good candidate) D is incorrect \u2013 table decorators allow time travel back only up to 7 days (see https:\/\/cloud.google.com\/bigquery\/table-decorators) \u2013 if you want to keep older snapshots, you would have to save them into separate table yourself (and pay for storage)."},{"label":"test_14","q_format":"multiple","q_text":"Your company handles data processing for a number of different clients. Each client prefers to use their own suite of analytics tools, with some allowing direct query access via Google BigQuery. You need to secure the data so that clients cannot see each other\u2018s data. You want to ensure appropriate access to the data.Which three steps should you take? (Choose three.)","answers":[{"ans":"A. Load data into different partitions.","val":false},{"ans":"B. Load data into a different dataset for each client.","val":true},{"ans":"C. Put each client\u2018s BigQuery dataset into a different table.","val":false},{"ans":"D. Restrict a client\u2018s dataset to approved users.","val":true},{"ans":"E. Only allow a service account to access the datasets.","val":false},{"ans":"F. Use the appropriate identity and access management (IAM) roles for each client\u2018s users.","val":true}],"q_expl":"B. Load data into a different dataset for each client. : Isolating client data into separate datasets is a core principle of data segregation. This ensures clients cannot see data belonging to other clients.\nD. Restrict a client\u2018s dataset to approved users. : BigQuery IAM allows granular access control. You can restrict access to each client\u2018s dataset to only authorized users within that client\u2018s organization.\nF. Use the appropriate identity and access management (IAM) roles for each client\u2018s users. : Assigning specific IAM roles to client users ensures they only have the level of access needed for their tasks. For example, some users might only require read access, while others might need permissions to modify data.\nHere\u2018s why the other options are not essential for this specific scenario:\nA. Load data into different partitions. While partitioning data can be useful for performance optimization and data management, it wouldn\u2018t necessarily prevent clients from seeing each other\u2018s data if they have access to the same dataset.\nC. Put each client\u2018s BigQuery dataset into a different table. Similar to data partitioning, separate tables within the same dataset wouldn\u2018t provide complete data isolation between clients.\nE. Only allow a service account to access the datasets. While service accounts can be used for specific access needs, it might not be the most practical solution for allowing client users to directly query their data. Client users with appropriate IAM roles can access BigQuery directly."},{"label":"test_14","q_format":"multiple","q_text":"Which of these are examples of a value in a sparse vector? (Select 2 answers.)","answers":[{"ans":"A. [0, 5, 0, 0, 0, 0]","val":false},{"ans":"B. [0, 0, 0, 1, 0, 0, 1]","val":false},{"ans":"C. [0, 1]","val":true},{"ans":"D. [1, 0, 0, 0, 0, 0, 0]","val":true}],"q_expl":"Categorical features in linear models are typically translated into a sparse vector in which each possible value has a corresponding index or id. For example, if there are only three possible eye colors you can represent \u2018eye_color\u2018 as a length 3 vector: \u2018brown\u2018 would become [1, 0, 0], \u2018blue\u2018 would become [0, 1, 0] and \u2018green\u2018 would become [0, 0, 1]. These vectors are called \u201csparse\u201c because they may be very long, with many zeros, when the set of possible values is very large (such as all English words). [0, 0, 0, 1, 0, 0, 1] is not a sparse vector because it has two 1s in it. A sparse vector contains only a single 1. [0, 5, 0, 0, 0, 0] is not a sparse vector because it has a 5 in it. Sparse vectors only contain 0s and 1s. Reference: https:\/\/www.tensorflow.org\/tutorials\/linear#feature_columns_and_transformations"},{"label":"test_14","q_format":"single","q_text":"You are designing storage for 20 TB of text files as part of deploying a data pipeline on Google Cloud. Your input data is in CSV format. You want to minimize the cost of querying aggregate values for multiple users who will query the data in Cloud Storage with multiple engines. Which storage service and schema design should you use?","answers":[{"ans":"A. Use Cloud Bigtable for storage. Install the HBase shell on a Compute Engine instance to query the Cloud Bigtable data.","val":false},{"ans":"B. Use Cloud Bigtable for storage. Link as permanent tables in BigQuery for query.","val":false},{"ans":"C. Use Cloud Storage for storage. Link as permanent tables in BigQuery for query.","val":true},{"ans":"D. Use Cloud Storage for storage. Link as temporary tables in BigQuery for query.","val":false}],"q_expl":"answer C: BigQuery can access data in external sources, known as federated sources. Instead of first loading data into BigQuery, you can create a reference to an external source. External sources can be Cloud Bigtable, Cloud Storage, and Google Drive. When accessing external data, you can create either permanent or temporary external tables. Permanent tables are those that are created in a dataset and linked to an external source. Dataset-level access controls can be applied to these tables. When you are using a temporary table, a table is created in a special dataset and will be available for approxi- mately 24 hours. Temporary tables are useful for one-time operations, such as loading data into a data warehouse. \u201cDan Sullivan\u201c Book"},{"label":"test_14","q_format":"single","q_text":"You are planning to use Google\u2018s Dataflow SDK to analyze customer data such as displayed below. Your project requirement is to extract only the customer name from the data source and then write to an output PCollection.Tom,555 X street -Tim,553 Y street -Sam, 111 Z street -Which operation is best suited for the above data processing requirement?","answers":[{"ans":"A. ParDo","val":true},{"ans":"B. Sink API","val":false},{"ans":"C. Source API","val":false},{"ans":"D. Data extraction","val":false}],"q_expl":"In Google Cloud dataflow SDK, you can use the ParDo to extract only a customer name of each element in your PCollection. Reference: https:\/\/cloud.google.com\/dataflow\/model\/par-do"},{"label":"test_14","q_format":"single","q_text":"The CUSTOM tier for Cloud Machine Learning Engine allows you to specify the number of which types of cluster nodes?","answers":[{"ans":"A. Workers","val":false},{"ans":"B. Masters, workers, and parameter servers","val":false},{"ans":"C. Workers and parameter servers","val":true},{"ans":"D. Parameter servers","val":false}],"q_expl":"The CUSTOM tier is not a set tier, but rather enables you to use your own cluster specification. When you use this tier, set values to configure your processing cluster according to these guidelines: You must set TrainingInput.masterType to specify the type of machine to use for your master node. You may set TrainingInput.workerCount to specify the number of workers to use. You may set TrainingInput.parameterServerCount to specify the number of parameter servers to use. You can specify the type of machine for the master node, but you can\u2018t specify more than one master node. Reference: https:\/\/cloud.google.com\/ml-engine\/docs\/training-overview#job_configuration_parameters"},{"label":"test_14","q_format":"single","q_text":"Your startup has never implemented a formal security policy. Currently, everyone in the company has access to the datasets stored in Google BigQuery. Teams have freedom to use the service as they see fit, and they have not documented their use cases. You have been asked to secure the data warehouse. You need to discover what everyone is doing. What should you do first?","answers":[{"ans":"A. Use Google Stackdriver Audit Logs to review data access.","val":true},{"ans":"B. Get the identity and access management IIAM) policy of each table","val":false},{"ans":"C. Use Stackdriver Monitoring to see the usage of BigQuery query slots.","val":false},{"ans":"D. Use the Google Cloud Billing API to see what account the warehouse is being billed to.","val":false}],"q_expl":"A. Use Google Stackdriver Audit Logs to review data access. \nHere\u2018s why this approach is ideal for initial discovery:\nAudit Logs: Stackdriver Audit Logging provides a comprehensive record of user and service activities within Google Cloud Platform (GCP), including BigQuery. These logs detail events like data access, modifications, and administrative actions.\nIdentifying Access Patterns: By reviewing Stackdriver Audit Logs for BigQuery, you can gain insights into who is accessing the data warehouse, what data they are accessing, and what actions they are performing. This information is crucial for understanding current usage patterns.\nHere\u2018s why other options are less suitable for the first step:\nB. IAM Policy Review: While reviewing Identity and Access Management (IAM) policies for each table is important for long-term security, it wouldn\u2018t provide the level of detail needed for initial discovery of usage patterns across the entire BigQuery data warehouse.\nC. Stackdriver Monitoring: Stackdriver Monitoring focuses on monitoring resource health and performance metrics. While it might show BigQuery query slot usage, it wouldn\u2018t reveal which users or teams are running those queries or what data they are accessing.\nD. Google Cloud Billing API: The Google Cloud Billing API primarily provides information on billing costs associated with GCP resources. While it might reveal the account the BigQuery warehouse is billed to, it wouldn\u2018t offer insights into user access patterns and data usage within the warehouse."},{"label":"test_14","q_format":"single","q_text":"You need to move 2 PB of historical data from an on-premises storage appliance to Cloud Storage within six months, and your outbound network capacity is constrained to 20 Mb\/sec. How should you migrate this data to Cloud Storage?","answers":[{"ans":"A. Use Transfer Appliance to copy the data to Cloud Storage","val":true},{"ans":"B. Use gsutil cp \u201c\u201cJ to compress the content being uploaded to Cloud Storage","val":false},{"ans":"C. Create a private URL for the historical data, and then use Storage Transfer Service to copy the data to Cloud Storage","val":false},{"ans":"D. Use trickle or ionice along with gsutil cp to limit the amount of bandwidth gsutil utilizes to less than 20 Mb\/sec so it does not interfere with the production traffic","val":false}],"q_expl":"Vote for A A \u2013 Correct , Transfer Appliance for moving offline data, large data sets, or data from a source with limited bandwidth https:\/\/cloud.google.com\/storage-transfer\/docs\/overview B \u2013 Eliminated (Not recommended for large storage). recommended for < 1TB C - Its ONLINE, but we have bandwidth issue - So eliminated. D - Eliminated (Not recommended for large storage). recommended for < 1TB"},{"label":"test_14","q_format":"single","q_text":"You are responsible for writing your company\u2018s ETL pipelines to run on an Apache Hadoop cluster. The pipeline will require some checkpointing and splitting pipelines. Which method should you use to write the pipelines?","answers":[{"ans":"A. PigLatin using Pig","val":true},{"ans":"B. HiveQL using Hive","val":false},{"ans":"C. Java using MapReduce","val":false},{"ans":"D. Python using MapReduce","val":false}],"q_expl":"Option [A] \u2013 Pig Latin \u2013 Checkpoint & splits: https:\/\/books.google.co.in\/books?id=daGiDQAAQBAJ&pg=PT128&lpg=PT128&dq=pig+latin+splits+checkpoint&source=bl&ots=zTvVN_7Sh-&sig=ACfU3U1_Aug8U4ZfU7VUl9WG0kf78weHPQ&hl=en&sa=X&ved=2ahUKEwjGq_nz_Z7qAhU4zjgGHd-NCrQQ6AEwAHoECAcQAQ"},{"label":"test_14","q_format":"single","q_text":"Your company maintains a hybrid deployment with GCP, where analytics are performed on your anonymized customer data. The data are imported to CloudStorage from your data center through parallel uploads to a data transfer server running on GCP. Management informs you that the daily transfers take too long and have asked you to fix the problem. You want to maximize transfer speeds. Which action should you take?","answers":[{"ans":"A. Increase the CPU size on your server.","val":false},{"ans":"B. Increase the size of the Google Persistent Disk on your server.","val":false},{"ans":"C. Increase your network bandwidth from your datacenter to GCP.","val":true},{"ans":"D. Increase your network bandwidth from Compute Engine to Cloud Storage.","val":false}],"q_expl":"Answer: C Description : Speed of data transfer depends on Bandwidth Few things in computing highlight the hardware limitations of networks as transferring large amounts of data. Typically you can transfer 1 GB in eight seconds over a 1 Gbps network. If you scale that up to a huge dataset (for example, 100 TB), the transfer time is 12 days. Transferring huge datasets can test the limits of your infrastructure and potentially cause problems for your business."},{"label":"test_14","q_format":"single","q_text":"You\u2018re training a model to predict housing prices based on an available dataset with real estate properties. Your plan is to train a fully connected neural net, and you\u2018ve discovered that the dataset contains latitude and longitude of the property. Real estate professionals have told you that the location of the property is highly influential on price, so you\u2018d like to engineer a feature that incorporates this physical dependency.What should you do?","answers":[{"ans":"A. Provide latitude and longitude as input vectors to your neural net.","val":false},{"ans":"B. Create a numeric column from a feature cross of latitude and longitude.","val":false},{"ans":"C. Create a feature cross of latitude and longitude, bucketize at the minute level and use L1 regularization during optimization.","val":true},{"ans":"D. Create a feature cross of latitude and longitude, bucketize it at the minute level and use L2 regularization during optimization.","val":false}],"q_expl":"A and B don\u0092t make sense Now; the crossed feature represents a well defined city block. If the model learns that certain city blocks (within range of latitudes and longitudes) are more likely to be more expensive than others, it is a stronger signal than two features considered individually. BUT we\u0092ll have way too many dimensions (and that is bad). Would L2 regularization accomplish this task? Unfortunately not. L2 regularization encourages weights to be small, but doesn\u0092t force them to exactly 0.0. However, there is a regularization term called L1 regularization that serves as an approximation to L0, but has the advantage of being convex and thus efficient to compute. So we can use L1 regularization to encourage many of the uninformative coefficients in our model to be exactly 0, and thus reap RAM savings at inference time. https:\/\/developers.google.com\/machine-learning\/crash-course\/regularization-for-sparsity\/l1-regularization"},{"label":"test_14","q_format":"single","q_text":"Your company has recently grown rapidly and now ingesting data at a significantly higher rate than it was previously. You manage the daily batch MapReduce analytics jobs in Apache Hadoop. However, the recent increase in data has meant the batch jobs are falling behind. You were asked to recommend ways the development team could increase the responsiveness of the analytics without increasing costs. What should you recommend they do?","answers":[{"ans":"A. Rewrite the job in Pig.","val":false},{"ans":"B. Rewrite the job in Apache Spark.","val":true},{"ans":"C. Increase the size of the Hadoop cluster.","val":false},{"ans":"D. Decrease the size of the Hadoop cluster but also rewrite the job in Hive.","val":false}],"q_expl":"B. Rewrite the job in Apache Spark. \nHere\u2018s why Apache Spark is ideal for this situation:\nIn-memory Processing: Apache Spark leverages in-memory processing for intermediate data, significantly improving performance compared to traditional MapReduce jobs that rely heavily on disk I\/O. This can handle larger datasets and improve responsiveness.\nScalability: Spark can efficiently scale horizontally by adding more nodes to the cluster, allowing it to handle increasing data volumes without incurring the high costs of scaling up a traditional Hadoop cluster (Option C).\nBatch Processing: Spark can still handle batch processing tasks like your existing MapReduce jobs.\nHere\u2018s why other options are less suitable:\nA. Rewrite the job in Pig: While Pig is a higher-level data flow language for Apache Hadoop, it wouldn\u2018t necessarily offer significant performance improvements over MapReduce for this scenario.\nC. Increase the size of the Hadoop cluster: Scaling up the Hadoop cluster can be expensive, and it might not be the most efficient solution for the specific problem of in-memory processing limitations.\nD. Decrease the size of the cluster and rewrite in Hive: Reducing the cluster size would likely worsen performance, and Hive, another SQL-like query language for Hadoop, wouldn\u2018t necessarily address the core issue of slow MapReduce jobs due to disk-bound processing."},{"label":"test_14","q_format":"single","q_text":"You work for a mid-sized enterprise that needs to move its operational system transaction data from an on-premises database to GCP. The database is about 20TB in size. Which database should you choose?","answers":[{"ans":"A. Cloud SQL","val":true},{"ans":"B. Cloud Bigtable","val":false},{"ans":"C. Cloud Spanner","val":false},{"ans":"D. Cloud Datastore","val":false}],"q_expl":"Option A \u2013 Cloud SQL is the correct answer. Cloud SQL can store upto 30 TB. https:\/\/cloud.google.com\/sql\/docs\/quotas#:~:text=Cloud%20SQL%20storage%20limits&text=Up%20to%2030%2C720%20GB%2C%20depending,for%20PostgreSQL%20or%20SQL%20Server."},{"label":"test_14","q_format":"single","q_text":"Your company has hired a new data scientist who wants to perform complicated analyses across very large datasets stored in Google Cloud Storage and in aCassandra cluster on Google Compute Engine. The scientist primarily wants to create labelled data sets for machine learning projects, along with some visualization tasks. She reports that her laptop is not powerful enough to perform her tasks and it is slowing her down. You want to help her perform her tasks.What should you do?","answers":[{"ans":"A. Run a local version of Jupiter on the laptop.","val":false},{"ans":"B. Grant the user access to Google Cloud Shell.","val":false},{"ans":"C. Host a visualization tool on a VM on Google Compute Engine.","val":false},{"ans":"D. Deploy Google Cloud Datalab to a virtual machine (VM) on Google Compute Engine.","val":true}],"q_expl":"correct answer -> Deploy Google Cloud Datalab to a virtual machine (VM) on Google Compute Engine. Google says: Use Cloud Datalab to easily explore, visualize, analyze, and transform data using familiar languages, such as Python and SQL, interactively. Reference: https:\/\/cloud.google.com\/datalab\/docs ***This is exactly the tool needed in this scenario*** But what about her laptop slowing her down? Well, Cloud Datalab is packaged as a container and run in a VM (Virtual Machine) instance. Reference: https:\/\/cloud.google.com\/datalab\/docs\/concepts\/key-concepts"},{"label":"test_14","q_format":"single","q_text":"You are implementing several batch jobs that must be executed on a schedule. These jobs have many interdependent steps that must be executed in a specific order. Portions of the jobs involve executing shell scripts, running Hadoop jobs, and running queries in BigQuery. The jobs are expected to run for many minutes up to several hours. If the steps fail, they must be retried a fixed number of times. Which service should you use to manage the execution of these jobs?","answers":[{"ans":"A. Cloud Scheduler","val":false},{"ans":"B. Cloud Dataflow","val":false},{"ans":"C. Cloud Functions","val":false},{"ans":"D. Cloud Composer","val":true}],"q_expl":"D: the main point is that Cloud Composer should be used when there is inter-dependencies between the job, e.g. we need the output of a job to start another whenever the first finished, and use dependencies coming from first job."},{"label":"test_14","q_format":"single","q_text":"You need to deploy additional dependencies to all of a Cloud Dataproc cluster at startup using an existing initialization action. Company security policies require that Cloud Dataproc nodes do not have access to the Internet so public initialization actions cannot fetch resources. What should you do?","answers":[{"ans":"A. Deploy the Cloud SQL Proxy on the Cloud Dataproc master","val":false},{"ans":"B. Use an SSH tunnel to give the Cloud Dataproc cluster access to the Internet","val":false},{"ans":"C. Copy all dependencies to a Cloud Storage bucket within your VPC security perimeter","val":true},{"ans":"D. Use Resource Manager to add the service account used by the Cloud Dataproc cluster to the Network User role","val":false}],"q_expl":"Correct: C If you create a Dataproc cluster with internal IP addresses only, attempts to access the Internet in an initialization action will fail unless you have configured routes to direct the traffic through a NAT or a VPN gateway. Without access to the Internet, you can enable Private Google Access, and place job dependencies in Cloud Storage; cluster nodes can download the dependencies from Cloud Storage from internal IPs."},{"label":"test_14","q_format":"single","q_text":"You are designing the database schema for a machine learning-based food ordering service that will predict what users want to eat. Here is some of the information you need to store:? The user profile: What the user likes and doesn\u2018t like to eat? The user account information: Name, address, preferred meal times? The order information: When orders are made, from where, to whomThe database will be used to store all the transactional data of the product. You want to optimize the data schema. Which Google Cloud Platform product should you use?","answers":[{"ans":"A. BigQuery","val":true},{"ans":"B. Cloud SQL","val":false},{"ans":"C. Cloud Bigtable","val":false},{"ans":"D. Cloud Datastore","val":false}],"q_expl":"Firebase is a documental DB so can have a schema but \u201cCloud Datastore\u201c not. The question says it needs to \u201cstore\u201c transactional data, not doing transaction operations and the data will be used for ML. For all these reasones BigQuery is the best choice."},{"label":"test_14","q_format":"multiple","q_text":"You have Cloud Functions written in Node.js that pull messages from Cloud Pub\/Sub and send the data to BigQuery. You observe that the message processing rate on the Pub\/Sub topic is orders of magnitude higher than anticipated, but there is no error logged in Stackdriver Log Viewer. What are the two most likely causes of this problem? (Choose two.)","answers":[{"ans":"A. Publisher throughput quota is too small.","val":false},{"ans":"B. Total outstanding messages exceed the 10-MB maximum.","val":false},{"ans":"C. Error handling in the subscriber code is not handling run-time errors properly.","val":true},{"ans":"D. The subscriber code cannot keep up with the messages.","val":false},{"ans":"E. The subscriber code does not acknowledge the messages that it pulls.","val":true}],"q_expl":"Answer: C, E Description: C, E: By not acknowleding the pulled message, this result in it be putted back in Cloud Pub\/Sub, meaning the messages accumulate instead of being consumed and removed from Pub\/Sub. The same thing can happen ig the subscriber maintains the lease on the message it receives in case of an error. This reduces the overall rate of processing because messages get stuck on the first subscriber. Also, errors in Cloud Function do not show up in Stackdriver Log Viewer if they are not correctly handled. A: No problem with publisher rate as the observed result is a higher number of messages and not a lower number. B: if messages exceed the 10MB maximum, they cannot be published. D: Cloud Functions automatically scales so they should be able to keep up."},{"label":"test_14","q_format":"single","q_text":"Which of the following is NOT one of the three main types of triggers that Dataflow supports?","answers":[{"ans":"A. Trigger based on element size in bytes","val":true},{"ans":"B. Trigger that is a combination of other triggers","val":false},{"ans":"C. Trigger based on element count","val":false},{"ans":"D. Trigger based on time","val":false}],"q_expl":"There are three major kinds of triggers that Dataflow supports: 1. Time-based triggers 2. Data-driven triggers. You can set a trigger to emit results from a window when that window has received a certain number of data elements. 3. Composite triggers. These triggers combine multiple time-based or data-driven triggers in some logical way Reference: https:\/\/cloud.google.com\/dataflow\/model\/triggers"},{"label":"test_14","q_format":"single","q_text":"You use a dataset in BigQuery for analysis. You want to provide third-party companies with access to the same dataset. You need to keep the costs of data sharing low and ensure that the data is current. Which solution should you choose?","answers":[{"ans":"A. Create an authorized view on the BigQuery table to control data access, and provide third-party companies with access to that view.","val":true},{"ans":"B. Use Cloud Scheduler to export the data on a regular basis to Cloud Storage, and provide third-party companies with access to the bucket.","val":false},{"ans":"C. Create a separate dataset in BigQuery that contains the relevant data to share, and provide third-party companies with access to the new dataset.","val":false},{"ans":"D. Create a Cloud Dataflow job that reads the data in frequent time intervals, and writes it to the relevant BigQuery dataset or Cloud Storage bucket for third-party companies to use.","val":false}],"q_expl":"https:\/\/cloud.google.com\/bigquery\/docs\/share-access-views BigQuery is a petabyte-scale analytics data warehouse that you can use to run SQL queries over vast amounts of data in near realtime. Giving a view access to a dataset is also known as creating an authorized view in BigQuery\u0092. An authorized view allows you to share query results with particular users and groups without giving them access to the underlying tables. You can also use the view\u2018s SQL query to restrict the columns (fields) the users are able to query. When you create the view, it must be created in a dataset separate from the source data queried by the view. Because you can assign access controls only at the dataset level, if the view is created in the same dataset as the source data, your data analysts would have access to both the view and the data."},{"label":"test_14","q_format":"single","q_text":"You are building a data pipeline on Google Cloud. You need to prepare data using a casual method for a machine-learning process. You want to support a logistic regression model. You also need to monitor and adjust for null values, which must remain real-valued and cannot be removed. What should you do?","answers":[{"ans":"A. Use Cloud Dataprep to find null values in sample source data. Convert all nulls to \u201c\u0098none\u2018 using a Cloud Dataproc job.","val":false},{"ans":"B. Use Cloud Dataprep to find null values in sample source data. Convert all nulls to 0 using a Cloud Dataprep job.","val":true},{"ans":"C. Use Cloud Dataflow to find null values in sample source data. Convert all nulls to \u201c\u0098none\u2018 using a Cloud Dataprep job.","val":false},{"ans":"D. Use Cloud Dataflow to find null values in sample source data. Convert all nulls to 0 using a custom script.","val":false}],"q_expl":"B. Use Cloud Dataprep to find null values in sample source data. Convert all nulls to 0 using a Cloud Dataprep job. \nHere\u2018s why this solution is ideal:\nCloud Dataprep for Data Exploration: Cloud Dataprep is a user-friendly, visual tool specifically designed for data exploration and wrangling tasks. It allows you to easily identify null values in your sample source data.\nCost-Effective Null Imputation: Within Cloud Dataprep, you can use a transformation to replace null values with 0. This is a simple and efficient way to handle nulls for a logistic regression model, which can handle numeric values. Cloud Dataprep offers a serverless pricing model, so you only pay for the resources you use, making it cost-effective for this task.\nLogistic Regression and Numeric Imputation: Logistic regression models typically work well with numeric features. Replacing nulls with 0 provides a numerical value for the model to interpret.\nHere\u2018s why other options are less suitable:\nA. Convert nulls to \u201c\u0098none\u2018: Using a string value like \u201c\u2018none\u2018\u201c for nulls wouldn\u2018t be suitable for a logistic regression model as it expects numerical features.\nC & D. Cloud Dataflow with Custom Script: While Cloud Dataflow is a powerful tool for building streaming data pipelines, in this scenario, Cloud Dataprep offers a simpler and potentially more cost-effective solution for this specific data preparation task. Additionally, a custom script in Cloud Dataflow might introduce additional development and maintenance overhead."},{"label":"test_14","q_format":"single","q_text":"Your globally distributed auction application allows users to bid on items. Occasionally, users place identical bids at nearly identical times, and different application servers process those bids. Each bid event contains the item, amount, user, and timestamp. You want to collate those bid events into a single location in real time to determine which user bid first. What should you do?","answers":[{"ans":"A. Create a file on a shared file and have the application servers write all bid events to that file. Process the file with Apache Hadoop to identify which user bid first.","val":false},{"ans":"B. Have each application server write the bid events to Cloud Pub\/Sub as they occur. Push the events from Cloud Pub\/Sub to a custom endpoint that writes the bid event information into Cloud SQL.","val":true},{"ans":"C. Set up a MySQL database for each application server to write bid events into. Periodically query each of those distributed MySQL databases and update a master MySQL database with bid event information.","val":false},{"ans":"D. Have each application server write the bid events to Google Cloud Pub\/Sub as they occur. Use a pull subscription to pull the bid events using Google Cloud Dataflow. Give the bid for each item to the user in the bid event that is processed first.","val":false}],"q_expl":"B. Have each application server write the bid events to Cloud Pub\/Sub as they occur. Push the events from Cloud Pub\/Sub to a custom endpoint that writes the bid event information into Cloud SQL.\nHere\u2018s why this solution is ideal:\nReal-time Event Capture: Cloud Pub\/Sub acts as a real-time messaging service. Each application server can publish bid events to a Pub\/Sub topic as they occur, ensuring minimal latency.\nCentralized Processing: By subscribing to the Pub\/Sub topic with a custom endpoint, you can push the events to a central location for processing. This custom endpoint can be a Cloud Function that writes the bid event information into Cloud SQL.\nIdentifying First Bid: Within Cloud SQL (or your chosen database), you can implement logic to identify the first bid for each item based on the timestamp. This could involve enforcing unique constraints on a combination of item ID and timestamp to ensure only the first bid for an item is persisted.\nOther options and why they are less suitable:\nA. Shared File and Hadoop: A shared file wouldn\u2018t be ideal for real-time processing. Processing the file with Apache Hadoop would introduce batch delays, potentially missing the first bid in case of very close timestamps.\nC. Distributed MySQL Databases: Maintaining separate MySQL databases on each application server and periodically querying them to update a central database would be complex and inefficient. It wouldn\u2018t guarantee real-time identification of the first bid.\nD. Cloud Pub\/Sub, Cloud Dataflow & Cloud SQL: While Cloud Pub\/Sub and Cloud SQL are involved, Cloud Dataflow adds unnecessary complexity in this scenario. Cloud Functions offer a simpler way to process the Pub\/Sub messages and write them to Cloud SQL. Additionally, giving the bid to the user processed first might not be ideal, as other factors like bid amount might also play a role in determining the winner."},{"label":"test_14","q_format":"multiple","q_text":"Your company produces 20,000 files every hour. Each data file is formatted as a comma separated values (CSV) file that is less than 4 KB. All files must be ingested on Google Cloud Platform before they can be processed. Your company site has a 200 ms latency to Google Cloud, and your Internet connection bandwidth is limited as 50 Mbps. You currently deploy a secure FTP (SFTP) server on a virtual machine in Google Compute Engine as the data ingestion point. A local SFTP client runs on a dedicated machine to transmit the CSV files as is. The goal is to make reports with data from the previous day available to the executives by 10:00 a.m. each day. This design is barely able to keep up with the current volume, even though the bandwidth utilization is rather low.You are told that due to seasonality, your company expects the number of files to double for the next three months. Which two actions should you take? (Choose two.)","answers":[{"ans":"A. Introduce data compression for each file to increase the rate file of file transfer.","val":true},{"ans":"B. Contact your internet service provider (ISP) to increase your maximum bandwidth to at least 100 Mbps.","val":false},{"ans":"C. Redesign the data ingestion process to use gsutil tool to send the CSV files to a storage bucket in parallel.","val":true},{"ans":"D. Assemble 1,000 files into a tape archive (TAR) file. Transmit the TAR files instead, and disassemble the CSV files in the cloud upon receiving them.","val":false},{"ans":"E. Create an S3-compatible storage endpoint in your network, and use Google Cloud Storage Transfer Service to transfer on-premises data to the designated storage bucket.","val":false}],"q_expl":"Here\u2019s the breakdown of the potential solutions and why two of them are the best choices:\nA. Introduce data compression for each file to increase the rate of file transfer: This is a good choice. Compressing individual files will significantly reduce their size, allowing you to transfer more files within the existing bandwidth limitations.\nB. Contact your internet service provider (ISP) to increase your maximum bandwidth to at least 100 Mbps: This might be helpful, but it\u2019s not the most cost-effective solution for the given scenario. Bandwidth utilization is currently low, and the bottleneck seems to be the processing power for transferring a large number of files, not the overall bandwidth. Upgrading internet might not solve the problem entirely.\nC. Redesign the data ingestion process to use gsutil tool to send the CSV files to a storage bucket in parallel. This is another good choice. The gsutil tool allows you to upload files to Google Cloud Storage in parallel, significantly improving transfer speeds compared to a single SFTP client.\nD. Assemble 1,000 files into a tape archive (TAR) file. Transmit the TAR files instead, and disassemble the CSV files in the cloud upon receiving them. This could be helpful, but it adds unnecessary processing overhead. While it reduces the number of files transferred, assembling and disassembling the TAR files takes additional time and resources.\u00a0\nE. Create an S3-compatible storage endpoint in your network, and use Google Cloud Storage Transfer Service to transfer on-premises data to the designated storage bucket. This solution is not ideal for this scenario. Google Cloud Storage already offers native functionality for transferring data, and creating an S3 endpoint adds unnecessary complexity.\u00a0\nTherefore, the two best actions to take are:\n\nIntroduce data compression for each file.\nRedesign the data ingestion process to use gsutil tool to send the CSV files to a storage bucket in parallel.\n\nThese improvements will significantly improve data transfer speeds without requiring expensive bandwidth upgrades."},{"label":"test_14","q_format":"single","q_text":"Your company is loading comma-separated values (CSV) files into Google BigQuery. The data is fully imported successfully; however, the imported data is not matching byte-to-byte to the source file. What is the most likely cause of this problem?","answers":[{"ans":"A. The CSV data loaded in BigQuery is not flagged as CSV.","val":false},{"ans":"B. The CSV data has invalid rows that were skipped on import.","val":true},{"ans":"C. The CSV data loaded in BigQuery is not using BigQuery\u2018s default encoding.","val":false},{"ans":"D. The CSV data has not gone through an ETL phase before loading into BigQuery.","val":false}],"q_expl":"The most likely cause of the problem is B. The CSV data has invalid rows that were skipped on import.\nWhen importing CSV data into BigQuery, the system may encounter invalid rows that do not conform to the expected format. These invalid rows can be skipped during the import process, leading to discrepancies between the source file and the imported data.\nHere\u2019s why the other options are less likely:\nA. The CSV data loaded in BigQuery is not flagged as CSV: BigQuery automatically recognizes CSV files and handles them accordingly.\nC. The CSV data loaded in BigQuery is not using BigQuery\u2018s default encoding: While encoding issues can occur, they typically manifest as character encoding problems rather than byte-to-byte mismatches.\nD. The CSV data has not gone through an ETL phase before loading into BigQuery: An ETL (Extract, Transform, Load) process can help clean and prepare data before loading it into BigQuery. However, if the data is already clean and valid, an ETL process would not be necessary to address the byte-to-byte mismatch.\nTherefore, the most likely cause of the problem is the presence of invalid rows in the CSV data that were skipped during the import process.\n\u00a0https:\/\/cloud.google.com\/bigquery\/docs\/loading-data-cloud-storage-csv#details_of_loading_csv_data"},{"label":"test_14","q_format":"single","q_text":"Your company is currently setting up data pipelines for their campaign. For all the Google Cloud Pub\/Sub streaming data, one of the important business requirements is to be able to periodically identify the inputs and their timings during their campaign. Engineers have decided to use windowing and transformation in Google Cloud Dataflow for this purpose. However, when testing this feature, they find that the Cloud Dataflow job fails for the all streaming insert. What is the most likely cause of this problem?","answers":[{"ans":"A. They have not assigned the timestamp, which causes the job to fail","val":true},{"ans":"B. They have not set the triggers to accommodate the data coming in late, which causes the job to fail","val":false},{"ans":"C. They have not applied a global windowing function, which causes the job to fail when the pipeline is created","val":false},{"ans":"D. They have not applied a non-global windowing function, which causes the job to fail when the pipeline is created","val":false}],"q_expl":"The most likely cause of the Cloud Dataflow job failing for all streaming inserts is:\nA. They have not assigned the timestamp, which causes the job to fail.\nHere\u2019s why:\nTimestamps are crucial for windowing: Windowing functions in Dataflow rely on timestamps to group data into time-based windows. Without a timestamp, the data cannot be correctly assigned to windows, leading to errors and failures.\nMissing timestamp: If the data being ingested into Pub\/Sub doesn\u2019t have a timestamp, Dataflow won\u2019t be able to process it correctly, resulting in job failures.\nThe other options are less likely:\nB. They have not set the triggers to accommodate the data coming in late: While triggers can be used to handle late data, they are not the primary reason for job failures in this case.\nC. They have not applied a global windowing function: A global windowing function is not necessary for this scenario. A fixed-size or sliding window function would be more appropriate for grouping data based on time intervals.\nD. They have not applied a non-global windowing function: A non-global windowing function would be appropriate for this scenario, as it allows you to group data based on time intervals. However, the lack of a timestamp would still cause the job to fail.\nTherefore, the most likely cause of the job failures is the missing timestamp in the ingested data. Ensuring that the data has a timestamp and that the Dataflow job is configured to use it correctly will resolve the issue."},{"label":"test_14","q_format":"single","q_text":"You plan to deploy Cloud SQL using MySQL. You need to ensure high availability in the event of a zone failure. What should you do?","answers":[{"ans":"A. Create a Cloud SQL instance in one zone, and create a failover replica in another zone within the same region.","val":true},{"ans":"B. Create a Cloud SQL instance in one zone, and create a read replica in another zone within the same region.","val":false},{"ans":"C. Create a Cloud SQL instance in one zone, and configure an external read replica in a zone in a different region.","val":false},{"ans":"D. Create a Cloud SQL instance in a region, and configure automatic backup to a Cloud Storage bucket in the same region.","val":false}],"q_expl":"The correct answer is:\nA. Create a Cloud SQL instance in one zone, and create a failover replica in another zone within the same region.\nThis will ensure high availability in the event of a zone failure. If the primary Cloud SQL instance becomes unavailable due to a zone failure, the failover replica can be automatically promoted to become the new primary instance, ensuring minimal downtime.\nHere\u2019s why the other options are not as suitable:\n\nB. Create a Cloud SQL instance in one zone, and create a read replica in another zone within the same region: While this provides read redundancy, it does not guarantee high availability in the event of a zone failure. If the primary instance becomes unavailable, the read replica cannot be promoted to become the new primary.\nC. Create a Cloud SQL instance in one zone, and configure an external read replica in a zone in a different region: While this provides read redundancy and disaster recovery, it does not guarantee high availability in the event of a zone failure within the same region. If the primary instance becomes unavailable due to a zone failure, the external read replica may not be able to take over immediately.\nD. Create a Cloud SQL instance in a region, and configure automatic backup to a Cloud Storage bucket in the same region: While automatic backups are important for data recovery, they do not provide high availability in the event of a zone failure. If the primary instance becomes unavailable, there may be a delay in restoring from the backup.\n\nTherefore, creating a Cloud SQL instance with a failover replica in a different zone within the same region is the most effective way to ensure high availability for your MySQL database."},{"label":"test_14","q_format":"single","q_text":"You are working on a niche product in the image recognition domain. Your team has developed a model that is dominated by custom C++ TensorFlow ops your team has implemented. These ops are used inside your main training loop and are performing bulky matrix multiplications. It currently takes up to several days to train a model. You want to decrease this time significantly and keep the cost low by using an accelerator on Google Cloud. What should you do?","answers":[{"ans":"A. Use Cloud TPUs without any additional adjustment to your code.","val":false},{"ans":"B. Use Cloud TPUs after implementing GPU kernel support for your customs ops.","val":false},{"ans":"C. Use Cloud GPUs after implementing GPU kernel support for your customs ops.","val":true},{"ans":"D. Stay on CPUs, and increase the size of the cluster you\u2018re training your model on.","val":false}],"q_expl":"The most effective solution to decrease the training time of your custom C++ TensorFlow ops while keeping costs low is:\nC. Use Cloud GPUs after implementing GPU kernel support for your customs ops.\nHere\u2019s why:\n\nCloud GPUs: GPUs are specifically designed for accelerating machine learning tasks, including matrix multiplications. They can significantly speed up the training process compared to CPUs.\nCustom op GPU kernel support: Since your model is dominated by custom C++ TensorFlow ops, implementing GPU kernel support for these ops will allow them to leverage the power of the GPU, resulting in a significant speedup.\nCost-effective: Cloud GPUs are generally more cost-effective than Cloud TPUs, especially for smaller models.\n\nWhile Cloud TPUs are also powerful accelerators, they may not be the most cost-effective option for this scenario, especially if your model is not very large or complex. Additionally, implementing GPU kernel support for your custom ops is essential for achieving significant speedups on Cloud GPUs.\nTherefore, using Cloud GPUs with GPU kernel support for your custom ops is the best way to decrease the training time of your model while keeping costs low."},{"label":"test_14","q_format":"multiple","q_text":"You are choosing a NoSQL database to handle telemetry data submitted from millions of Internet-of-Things (IoT) devices. The volume of data is growing at 100TB per year, and each data entry has about 100 attributes. The data processing pipeline does not require atomicity, consistency, isolation, and durability (ACID).However, high availability and low latency are required.You need to analyze the data by querying against individual fields. Which three databases meet your requirements? (Choose three.)","answers":[{"ans":"A. Redis","val":false},{"ans":"B. HBase","val":true},{"ans":"C. MySQL","val":false},{"ans":"D. MongoDB","val":true},{"ans":"E. Cassandra","val":true},{"ans":"F. HDFS with Hive","val":false}],"q_expl":"Answer is BDE \u2013 A. Redis \u2013 Redis is an in-memory non-relational key-value store. Redis is a great choice for implementing a highly available in-memory cache to decrease data access latency, increase throughput, and ease the load off your relational or NoSQL database and application. Since the question does not ask cache, A is discarded. B. HBase \u2013 Meets reqs C. MySQL \u2013 they do not need ACID, so not needed. D. MongoDB \u2013 Meets reqs E. Cassandra \u2013 Apache Cassandra is an open source NoSQL distributed database trusted by thousands of companies for scalability and high availability without compromising performance. Linear scalability and proven fault-tolerance on commodity hardware or cloud infrastructure make it the perfect platform for mission-critical data. F. HDFS with Hive \u2013 Hive allows users to read, write, and manage petabytes of data using SQL. Hive is built on top of Apache Hadoop, which is an open-source framework used to efficiently store and process large datasets. As a result, Hive is closely integrated with Hadoop, and is designed to work quickly on petabytes of data. HIVE IS NOT A DATABSE."},{"label":"test_14","q_format":"single","q_text":"You are building a model to make clothing recommendations. You know a user\u2018s fashion preference is likely to change over time, so you build a data pipeline to stream new data back to the model as it becomes available. How should you use this data to train the model?","answers":[{"ans":"A. Continuously retrain the model on just the new data.","val":false},{"ans":"B. Continuously retrain the model on a combination of existing data and the new data.","val":true},{"ans":"C. Train on the existing data while using the new data as your test set.","val":false},{"ans":"D. Train on the new data while using the existing data as your test set.","val":false}],"q_expl":"Vote for B. In Recommendation System \u2013 Matrix stored in database \u2013 which store the users rating and other details like how much time user spent on that page.We generally recommend on basis of matrix. And In production that matrix updated (or model get retrained once a day or once a week) \u2013 Because there could be new users rating. or we can say new data. So model retrained fully i.e. on Existing Data + New Data => and generate a new matrix (user\/item matrix)"},{"label":"test_14","q_format":"single","q_text":"What are two of the benefits of using denormalized data structures in BigQuery?","answers":[{"ans":"A. Reduces the amount of data processed, reduces the amount of storage required","val":false},{"ans":"B. Increases query speed, makes queries simpler","val":true},{"ans":"C. Reduces the amount of storage required, increases query speed","val":false},{"ans":"D. Reduces the amount of data processed, increases query speed","val":false}],"q_expl":"Denormalization increases query speed for tables with billions of rows because BigQuery\u2018s performance degrades when doing JOINs on large tables, but with a denormalized data structure, you don\u2018t have to use JOINs, since all of the data has been combined into one table. Denormalization also makes queries simpler because you do not have to use JOIN clauses. Denormalization increases the amount of data processed and the amount of storage required because it creates redundant data. Reference: https:\/\/cloud.google.com\/solutions\/bigquery-data-warehouse#denormalizing_data"},{"label":"test_14","q_format":"single","q_text":"Which of the following IAM roles does your Compute Engine account require to be able to run pipeline jobs?","answers":[{"ans":"A. dataflow.worker","val":true},{"ans":"B. dataflow.compute","val":false},{"ans":"C. dataflow.developer","val":false},{"ans":"D. dataflow.viewer","val":false}],"q_expl":"The dataflow.worker role provides the permissions necessary for a Compute Engine service account to execute work units for a Dataflow pipeline Reference: https:\/\/cloud.google.com\/dataflow\/access-control"},{"label":"test_14","q_format":"multiple","q_text":"Your organization has been collecting and analyzing data in Google BigQuery for 6 months. The majority of the data analyzed is placed in a time-partitioned table named events_partitioned. To reduce the cost of queries, your organization created a view called events, which queries only the last 14 days of data. The view is described in legacy SQL. Next month, existing applications will be connecting to BigQuery to read the events data via an ODBC connection. You need to ensure the applications can connect. Which two actions should you take? (Choose two.)","answers":[{"ans":"A. Create a new view over events using standard SQL","val":false},{"ans":"B. Create a new partitioned table using a standard SQL query","val":false},{"ans":"C. Create a new view over events_partitioned using standard SQL","val":true},{"ans":"D. Create a service account for the ODBC connection to use for authentication","val":true},{"ans":"E. Create a Google Cloud Identity and Access Management (Cloud IAM) role for the ODBC connection and shared \u201cevents\u201c","val":false}],"q_expl":"C = A standard SQL query cannot reference a view defined using legacy SQL syntax. D = For the ODBC drivers is needed a service account which will get a standard Bigquery role"},{"label":"test_14","q_format":"single","q_text":"You are integrating one of your internal IT applications and Google BigQuery, so users can query BigQuery from the application\u2018s interface. You do not want individual users to authenticate to BigQuery and you do not want to give them access to the dataset. You need to securely access BigQuery from your IT application. What should you do?","answers":[{"ans":"A. Create groups for your users and give those groups access to the dataset","val":false},{"ans":"B. Integrate with a single sign-on (SSO) platform, and pass each user\u2018s credentials along with the query request","val":false},{"ans":"C. Create a service account and grant dataset access to that account. Use the service account\u2018s private key to access the dataset","val":true},{"ans":"D. Create a dummy user and grant dataset access to that user. Store the username and password for that user in a file on the files system, and use those credentials to access the BigQuery dataset","val":false}],"q_expl":"C. Create a service account and grant dataset access to that account. Use the service account\u2018s private key to access the dataset \nHere\u2018s why this solution is ideal for secure BigQuery access:\nService Account Security: Service accounts are designed specifically for applications to access Google Cloud Platform (GCP) resources without requiring individual user accounts and passwords. This eliminates the risk of compromised user credentials.\nGranular Access Control: You can grant the service account access to specific BigQuery datasets or even specific tables within a dataset. This ensures the application only has the necessary permissions to perform its tasks.\nPrivate Key Management: The service account\u2018s private key acts as the secure access token. You can store it securely within your application environment (e.g., GCP Secret Manager) and use it to authenticate the application with BigQuery.\nOther options and why they are less secure:\nA. User Groups: Granting access through user groups wouldn\u2018t eliminate individual user authentication and could potentially grant wider access than intended.\nB. Single Sign-On (SSO) with User Credentials: Passing individual user credentials within the query request introduces unnecessary risk. If compromised, anyone could potentially access BigQuery using those credentials.\nD. Dummy User: Creating a dummy user with stored credentials on the file system is a security risk. Usernames and passwords should not be stored in plain text. Additionally, a single dummy user wouldn\u2018t allow for granular access control.\nBy using a service account with proper access control and private key management, you establish a secure and reliable connection between your IT application and BigQuery."},{"label":"test_14","q_format":"single","q_text":"You are building a new application that you need to collect data from in a scalable way. Data arrives continuously from the application throughout the day, and you expect to generate approximately 150 GB of JSON data per day by the end of the year. Your requirements are:? Decoupling producer from consumer? Space and cost-efficient storage of the raw ingested data, which is to be stored indefinitely? Near real-time SQL query? Maintain at least 2 years of historical data, which will be queried with SQLWhich pipeline should you use to meet these requirements?","answers":[{"ans":"A. Create an application that provides an API. Write a tool to poll the API and write data to Cloud Storage as gzipped JSON files.","val":false},{"ans":"B. Create an application that writes to a Cloud SQL database to store the data. Set up periodic exports of the database to write to Cloud Storage and load into BigQuery.","val":false},{"ans":"C. Create an application that publishes events to Cloud Pub\/Sub, and create Spark jobs on Cloud Dataproc to convert the JSON data to Avro format, stored on HDFS on Persistent Disk.","val":false},{"ans":"D. Create an application that publishes events to Cloud Pub\/Sub, and create a Cloud Dataflow pipeline that transforms the JSON event payloads to Avro, writing the data to Cloud Storage and BigQuery.","val":true}],"q_expl":"D. Create an application that publishes events to Cloud Pub\/Sub, and create a Cloud Dataflow pipeline that transforms the JSON event payloads to Avro, writing the data to Cloud Storage and BigQuery. \nHere\u2018s why this approach aligns well with your needs:\nDecoupled Producer and Consumer: Cloud Pub\/Sub acts as a decoupling mechanism. Your application publishes data to a topic in Pub\/Sub, and the Cloud Dataflow pipeline subscribes to that topic, ensuring the application doesn\u2018t need to be concerned about downstream processing.\nScalable and Cost-effective Storage: Cloud Storage offers scalable and cost-effective storage for your raw JSON data. Gzip compression can further optimize storage costs.\nNear Real-time Processing: Cloud Dataflow is a managed service for building streaming data pipelines. You can process the incoming JSON data and transform it to Avro format (more efficient for BigQuery) close to real-time.\nBigQuery for SQL Queries: BigQuery is a serverless data warehouse that allows you to store and query the Avro data using SQL. It can handle large datasets efficiently and is optimized for historical data analysis.\nMaintaining Historical Data: BigQuery is ideal for storing at least 2 years of historical data and performing SQL queries on it.\nOther options and why they are less suitable:\nA. Cloud Storage with Polling: Manually polling an API and writing data to Cloud Storage wouldn\u2018t be scalable for continuous data ingestion. It also wouldn\u2018t provide any data processing or transformation capabilities.\nB. Cloud SQL with Exports: Cloud SQL is a relational database service, not ideal for storing large amounts of raw JSON data indefinitely. Periodic exports to Cloud Storage and BigQuery would create additional complexity and potential delays.\nC. Cloud Dataproc with Spark: While Cloud Dataproc with Spark can be used for data processing, it requires managing a cluster and wouldn\u2018t be as serverless and scalable as Cloud Dataflow for this use case. Additionally, storing data on HDFS on Persistent Disks might not be as cost-effective as Cloud Storage for long-term raw data storage."},{"label":"test_14","q_format":"single","q_text":"You launched a new gaming app almost three years ago. You have been uploading log files from the previous day to a separate Google BigQuery table with the table name format LOGS_yyyymmdd. You have been using table wildcard functions to generate daily and monthly reports for all time ranges. Recently, you discovered that some queries that cover long date ranges are exceeding the limit of 1,000 tables and failing. How can you resolve this issue?","answers":[{"ans":"A. Convert all daily log tables into date-partitioned tables","val":false},{"ans":"B. Convert the sharded tables into a single partitioned table","val":true},{"ans":"C. Enable query caching so you can cache data from previous months","val":false},{"ans":"D. Create separate views to cover each month, and query from these views","val":false}],"q_expl":"https:\/\/cloud.google.com\/bigquery\/docs\/creating-partitioned-tables#converting_date-sharded_tables_into_ingestion-time_partitioned_tables"},{"label":"test_14","q_format":"single","q_text":"Your infrastructure includes a set of YouTube channels. You have been tasked with creating a process for sending the YouTube channel data to Google Cloud for analysis. You want to design a solution that allows your world-wide marketing teams to perform ANSI SQL and other types of analysis on up-to-date YouTube channels log data. How should you set up the log data transfer into Google Cloud?","answers":[{"ans":"A. Use Storage Transfer Service to transfer the offsite backup files to a Cloud Storage Multi-Regional storage bucket as a final destination.","val":true},{"ans":"B. Use Storage Transfer Service to transfer the offsite backup files to a Cloud Storage Regional bucket as a final destination.","val":false},{"ans":"C. Use BigQuery Data Transfer Service to transfer the offsite backup files to a Cloud Storage Multi-Regional storage bucket as a final destination.","val":false},{"ans":"D. Use BigQuery Data Transfer Service to transfer the offsite backup files to a Cloud Storage Regional storage bucket as a final destination.","val":false}],"q_expl":"Correct Answer: A Destination is GCS and having multi-regional so A is the best option available. Even since BigQuery Data Transfer Service initially supports Google application sources like Google Ads, Campaign Manager, Google Ad Manager and YouTube but it does not support destination anything other than bq data set"},{"label":"test_14","q_format":"single","q_text":"How can you get a neural network to learn about relationships between categories in a categorical feature?","answers":[{"ans":"A. Create a multi-hot column","val":false},{"ans":"B. Create a one-hot column","val":false},{"ans":"C. Create a hash bucket","val":false},{"ans":"D. Create an embedding column","val":true}],"q_expl":"There are two problems with one-hot encoding. First, it has high dimensionality, meaning that instead of having just one value, like a continuous feature, it has many values, or dimensions. This makes computation more time-consuming, especially if a feature has a very large number of categories. The second problem is that it doesnt encode any relationships between the categories. They are completely independent from each other, so the network has no way of knowing which ones are similar to each other. Both of these problems can be solved by representing a categorical feature with an embedding column. The idea is that each category has a smaller vector with, lets say, 5 values in it. But unlike a one-hot vector, the values are not usually 0. The values are weights, similar to the weights that are used for basic features in a neural network. The difference is that each category has a set of weights (5 of them in this case). You can think of each value in the embedding vector as a feature of the category. So, if two categories are very similar to each other, then their embedding vectors should be very similar too. Reference: https:\/\/cloudacademy.com\/google\/introduction-to-google-cloud-machine-learning-engine-course\/a-wide-and-deep-model.html"},{"label":"test_14","q_format":"single","q_text":"You want to process payment transactions in a point-of-sale application that will run on Google Cloud Platform. Your user base could grow exponentially, but you do not want to manage infrastructure scaling.Which Google database service should you use?","answers":[{"ans":"A. Cloud SQL","val":true},{"ans":"B. BigQuery","val":false},{"ans":"C. Cloud Bigtable","val":false},{"ans":"D. Cloud Datastore","val":false}],"q_expl":"Correct Options:\nCloud SQL: \nCloud SQL is a fully managed relational database service that supports MySQL, PostgreSQL, and SQL Server. It is suitable for transactional processing and can handle a large user base without requiring manual infrastructure scaling. Cloud SQL provides high availability, automatic backups, and disaster recovery, making it a reliable choice for payment transactions\nIncorrect Options:\nBigQuery: \nBigQuery is a data warehousing and analytics service, not designed for transactional processing. It is optimized for large-scale data analysis and is not suitable for handling payment transactions in real-time.\nCloud Bigtable: \nCloud Bigtable is a NoSQL database designed for massive writes and is not suitable for transactional processing. It is ideal for storing large amounts of single-keyed data with low latency, but it does not support the level of consistency and scalability required for payment transactions.\nCloud Datastore: \nCloud Datastore is a NoSQL database service that supports transactions but is not designed for high-performance transactions. It is suitable for applications with simple data models and does not provide the same level of scalability and high availability as Cloud SQL or AlloyDB"},{"label":"test_14","q_format":"single","q_text":"What are all of the BigQuery operations that Google charges for?","answers":[{"ans":"A. Storage, queries, and streaming inserts","val":true},{"ans":"B. Storage, queries, and loading data from a file","val":false},{"ans":"C. Storage, queries, and exporting data","val":false},{"ans":"D. Queries and streaming inserts","val":false}],"q_expl":"Google charges for storage, queries, and streaming inserts. Loading data from a file and exporting data are free operations. Reference: https:\/\/cloud.google.com\/bigquery\/pricing"},{"label":"test_14","q_format":"single","q_text":"You have spent a few days loading data from comma-separated values (CSV) files into the Google BigQuery table CLICK_STREAM. The column DT stores the epoch time of click events. For convenience, you chose a simple schema where every field is treated as the STRING type. Now, you want to compute web session durations of users who visit your site, and you want to change its data type to the TIMESTAMP. You want to minimize the migration effort without making future queries computationally expensive. What should you do?","answers":[{"ans":"A. Delete the table CLICK_STREAM, and then re-create it such that the column DT is of the TIMESTAMP type. Reload the data.","val":false},{"ans":"B. Add a column TS of the TIMESTAMP type to the table CLICK_STREAM, and populate the numeric values from the column TS for each row. Reference the column TS instead of the column DT from now on.","val":false},{"ans":"C. Create a view CLICK_STREAM_V, where strings from the column DT are cast into TIMESTAMP values. Reference the view CLICK_STREAM_V instead of the table CLICK_STREAM from now on.","val":false},{"ans":"D. Add two columns to the table CLICK STREAM: TS of the TIMESTAMP type and IS_NEW of the BOOLEAN type. Reload all data in append mode. For each appended row, set the value of IS_NEW to true. For future queries, reference the column TS instead of the column DT, with the WHERE clause ensuring that the value of IS_NEW must be true.","val":false},{"ans":"E. Construct a query to return every row of the table CLICK_STREAM, while using the built-in function to cast strings from the column DT into TIMESTAMP values. Run the query into a destination table NEW_CLICK_STREAM, in which the column TS is the TIMESTAMP type. Reference the table NEW_CLICK_STREAM instead of the table CLICK_STREAM from now on. In the future, new data is loaded into the table NEW_CLICK_STREAM.","val":true}],"q_expl":"E : Export and load data into a new table You can also change a column\u2018s data type by exporting your table data to Cloud Storage, and then loading the data into a new table with a schema definition that specifies the correct data type for the column. You can also use the load job to overwrite the existing table. Advantages You are not charged for the export job or the load job. Currently, BigQuery load and export jobs are free. If you use the load job to overwrite the original table, you incur storage costs for one table instead of two, but you lose the original data. Disadvantages If you load the data into a new table, you incur storage costs for the original table and the new table (unless you delete the old one). You incur costs for storing the exported data in Cloud Storage."},{"label":"test_14","q_format":"single","q_text":"You designed a database for patient records as a pilot project to cover a few hundred patients in three clinics. Your design used a single database table to represent all patients and their visits, and you used self-joins to generate reports. The server resource utilization was at 50%. Since then, the scope of the project has expanded. The database must now store 100 times more patient records. You can no longer run the reports, because they either take too long or they encounter errors with insufficient compute resources. How should you adjust the database design?","answers":[{"ans":"A. Add capacity (memory and disk space) to the database server by the order of 200.","val":false},{"ans":"B. Shard the tables into smaller ones based on date ranges, and only generate reports with prespecified date ranges.","val":false},{"ans":"C. Normalize the master patient-record table into the patient table and the visits table, and create other necessary tables to avoid self-join.","val":true},{"ans":"D. Partition the table into smaller tables, with one for each clinic. Run queries against the smaller table pairs, and use unions for consolidated reports.","val":false}],"q_expl":"C is correct because this option provides the least amount of inconvenience over using pre-specified date ranges or one table per clinic while also increasing performance due to avoiding self-joins. A is not correct because adding additional compute resources is not a recommended way to resolve database schema problems. B is not correct because this will reduce the functionality of the database and make running reports more difficult. D is not correct because this will likely increase the number of tables so much that it will be more difficult to generate reports vs. the correct option. https:\/\/cloud.google.com\/bigquery\/docs\/best-practices-performance-patterns https:\/\/cloud.google.com\/bigquery\/docs\/reference\/standard-sql\/query-syntax#explicit-alias-visibility"},{"label":"test_14","q_format":"single","q_text":"If a dataset contains rows with individual people and columns for year of birth, country, and income, how many of the columns are continuous and how many are categorical?","answers":[{"ans":"A. 1 continuous and 2 categorical","val":true},{"ans":"B. 3 categorical","val":false},{"ans":"C. 3 continuous","val":false},{"ans":"D. 2 continuous and 1 categorical","val":false}],"q_expl":"The dataset contains:\n2 categorical columns:\nCountry (This is categorical because it represents distinct categories like US, Canada, etc., not numerical values)\nIncome (This can be argued to be continuous on a spectrum, but in most cases, income is binned into categories for analysis, making it categorical)\n1 continuous column:\nYear of Birth (This is a numerical value representing a specific year)\nTherefore, the most appropriate answer is A. 1 continuous and 2 categorical."},{"label":"test_14","q_format":"single","q_text":"You are designing storage for very large text files for a data pipeline on Google Cloud. You want to support ANSI SQL queries. You also want to support compression and parallel load from the input locations using Google recommended practices. What should you do?","answers":[{"ans":"A. Transform text files to compressed Avro using Cloud Dataflow. Use BigQuery for storage and query.","val":false},{"ans":"B. Transform text files to compressed Avro using Cloud Dataflow. Use Cloud Storage and BigQuery permanent linked tables for query.","val":true},{"ans":"C. Compress text files to gzip using the Grid Computing Tools. Use BigQuery for storage and query.","val":false},{"ans":"D. Compress text files to gzip using the Grid Computing Tools. Use Cloud Storage, and then import into Cloud Bigtable for query.","val":false}],"q_expl":"B. The question is focused on designing storage for very large files, with support for compression, ANSI SQL queries, and parallel loading from the input locations. This can be met using GCS for storage and Bigquery permanent tables with external data source in GCS."},{"label":"test_14","q_format":"single","q_text":"You want to analyze hundreds of thousands of social media posts daily at the lowest cost and with the fewest steps.You have the following requirements:? You will batch-load the posts once per day and run them through the Cloud Natural Language API.? You will extract topics and sentiment from the posts.? You must store the raw posts for archiving and reprocessing.? You will create dashboards to be shared with people both inside and outside your organization.You need to store both the data extracted from the API to perform analysis as well as the raw social media posts for historical archiving. What should you do?","answers":[{"ans":"A. Store the social media posts and the data extracted from the API in BigQuery.","val":false},{"ans":"B. Store the social media posts and the data extracted from the API in Cloud SQL.","val":false},{"ans":"C. Store the raw social media posts in Cloud Storage, and write the data extracted from the API into BigQuery.","val":true},{"ans":"D. Feed to social media posts into the API directly from the source, and write the extracted data from the API into BigQuery.","val":false}],"q_expl":"C. Store the raw social media posts in Cloud Storage, and write the data extracted from the API into BigQuery. \nHere\u2018s why this solution is ideal:\nCost-Effectiveness: Cloud Storage offers a cost-effective way to store large amounts of unstructured data like raw social media posts. BigQuery is also optimized for storing and querying large datasets compared to Cloud SQL.\nSeparation of Concerns: Segregating raw data (Cloud Storage) and processed data (BigQuery) improves organization and simplifies management.\nArchiving and Reprocessing: Storing raw social media posts in Cloud Storage allows for easy archiving and potential future reprocessing with the Cloud Natural Language API or other tools.\nData Sharing: BigQuery is well-suited for creating dashboards that can be shared with users both inside and outside the organization. You can leverage tools like Google Data Studio to create interactive dashboards on top of your BigQuery data.\nWhy other options are less suitable:\nA. BigQuery for Everything: While BigQuery can store raw data, Cloud Storage is more cost-effective for this purpose.\nB. Cloud SQL: Cloud SQL, a relational database, wouldn\u2018t be ideal for storing large volumes of unstructured social media posts.\nD. Direct Feed to API: Feeding social media posts directly to the API from the source might bypass potential data quality checks or pre-processing steps before analysis. Additionally, you wouldn\u2018t have the raw data readily available for archiving or reprocessing."},{"label":"test_14","q_format":"single","q_text":"You are building a model to predict whether or not it will rain on a given day. You have thousands of input features and want to see if you can improve training speed by removing some features while having a minimum effect on model accuracy. What can you do?","answers":[{"ans":"A. Eliminate features that are highly correlated to the output labels.","val":false},{"ans":"B. Combine highly co-dependent features into one representative feature.","val":true},{"ans":"C. Instead of feeding in each feature individually, average their values in batches of 3.","val":false},{"ans":"D. Remove the features that have null values for more than 50% of the training records.","val":false}],"q_expl":"Vote for \u2018B\u2018 combining features to createte a new feature is a step of \u201cFeature construction\u201c or decomposing or splitting features to create new features. Ideally, PCA should be apply if we want to reduce the dimension. Removing those columns \/ features \u2013 where Data is miss > 50% (may improve the speed) \u2013 but will decrease the accuracy as well. So instead of dropping features where we have missing data, we need to impute something"},{"label":"test_14","q_format":"single","q_text":"Which of the following is NOT true about Dataflow pipelines?","answers":[{"ans":"A. Dataflow pipelines are tied to Dataflow, and cannot be run on any other runner","val":true},{"ans":"B. Dataflow pipelines can consume data from other Google Cloud services","val":false},{"ans":"C. Dataflow pipelines can be programmed in Java","val":false},{"ans":"D. Dataflow pipelines use a unified programming model, so can work both with streaming and batch data sources","val":false}],"q_expl":"Dataflow pipelines can also run on alternate runtimes like Spark and Flink, as they are built using the Apache Beam SDKs Reference: https:\/\/cloud.google.com\/dataflow\/"},{"label":"test_14","q_format":"single","q_text":"After migrating ETL jobs to run on BigQuery, you need to verify that the output of the migrated jobs is the same as the output of the original. You\u2018ve loaded a table containing the output of the original job and want to compare the contents with output from the migrated job to show that they are identical. The tables do not contain a primary key column that would enable you to join them together for comparison.What should you do?","answers":[{"ans":"A. Select random samples from the tables using the RAND() function and compare the samples.","val":false},{"ans":"B. Select random samples from the tables using the HASH() function and compare the samples.","val":false},{"ans":"C. Use a Dataproc cluster and the BigQuery Hadoop connector to read the data from each table and calculate a hash from non-timestamp columns of the table after sorting. Compare the hashes of each table.","val":true},{"ans":"D. Create stratified random samples using the OVER() function and compare equivalent samples from each table.","val":false}],"q_expl":"C Using Cloud Storage with big data Cloud Storage is a key part of storing and working with Big Data on Google Cloud. Examples include: Loading data into BigQuery. Using Dataproc, which automatically installs the HDFS-compatible Cloud Storage connector, enabling the use of Cloud Storage buckets in parallel with HDFS. Using a bucket to hold staging files and temporary data for Dataflow pipelines. For Dataflow, a Cloud Storage bucket is required. For BigQuery and Dataproc, using a Cloud Storage bucket is optional but recommended. gsutil is a command-line tool that enables you to work with Cloud Storage buckets and objects easily and robustly, in particular in big data scenarios. For example, with gsutil you can copy many files in parallel with a single command, copy large files efficiently, calculate checksums on your data, and measure performance from your local computer to Cloud Storage."},{"label":"test_14","q_format":"single","q_text":"Suppose you have a table that includes a nested column called \u201ccity\u201c inside a column called \u201cperson\u201c, but when you try to submit the following query in BigQuery, it gives you an error.SELECT person FROM \u2032project1.example.table1\u2032 WHERE city = \u201cLondon\u201cHow would you correct the error?","answers":[{"ans":"A. Add \u201c, UNNEST(person)\u201c before the WHERE clause.","val":true},{"ans":"B. Change \u201cperson\u201c to \u201cperson.city\u201c.","val":false},{"ans":"C. Change \u201cperson\u201c to \u201ccity.person\u201c.","val":false},{"ans":"D. Add \u201c, UNNEST(city)\u201c before the WHERE clause.","val":false}],"q_expl":"To access the person.city column, you need to \u201cUNNEST(person)\u201c and JOIN it to table1 using a comma. Reference: https:\/\/cloud.google.com\/bigquery\/docs\/reference\/standard-sql\/migrating-from-legacy-sql#nested_repeated_results"},{"label":"test_14","q_format":"single","q_text":"You are creating a model to predict housing prices. Due to budget constraints, you must run it on a single resource-constrained virtual machine. Which learning algorithm should you use?","answers":[{"ans":"A. Linear regression","val":true},{"ans":"B. Logistic classification","val":false},{"ans":"C. Recurrent neural network","val":false},{"ans":"D. Feedforward neural network","val":false}],"q_expl":"correct answer -> Linear Regression Linear regression is a statistical method that allows to summarize and study relationships between two continuous (quantitative) variables: One variable, denoted X, is regarded as the independent variable. The other variable denoted y is regarded as the dependent variable. Linear regression uses one independent variable X to explain or predict the outcome of the dependent variable y. Whenever you are told to predict some future value of a process which is currently running, you can go with a regression algorithm. Refeerence: https:\/\/towardsdatascience.com\/do-you-know-how-to-choose-the-right-machine-learning-algorithm-among-7-different-types-295d0b0c7f60"},{"label":"test_14","q_format":"single","q_text":"Your weather app queries a database every 15 minutes to get the current temperature. The frontend is powered by Google App Engine and server millions of users. How should you design the frontend to respond to a database failure?","answers":[{"ans":"A. Issue a command to restart the database servers.","val":false},{"ans":"B. Retry the query with exponential backoff, up to a cap of 15 minutes.","val":true},{"ans":"C. Retry the query every second until it comes back online to minimize staleness of data.","val":false},{"ans":"D. Reduce the query frequency to once every hour until the database comes back online.","val":false}],"q_expl":"Correct answer is B. App engine create applications that use Cloud SQL database connections effectively. Below is what is written in google cloud documnetation. If your application attempts to connect to the database and does not succeed, the database could be temporarily unavailable. In this case, sending too many simultaneous connection requests might waste additional database resources and increase the time needed to recover. Using exponential backoff prevents your application from sending an unresponsive number of connection requests when it can\u2018t connect to the database. This retry only makes sense when first connecting, or when first grabbing a connection from the pool. If errors happen in the middle of a transaction, the application must do the retrying, and it must retry from the beginning of a transaction. So even if your pool is configured properly, the application might still see errors if connections are lost. reference link is https:\/\/cloud.google.com\/sql\/docs\/mysql\/manage-connections"},{"label":"test_14","q_format":"single","q_text":"Your company is selecting a system to centralize data ingestion and delivery. You are considering messaging and data integration systems to address the requirements. The key requirements are:? The ability to seek to a particular offset in a topic, possibly back to the start of all data ever captured? Support for publish\/subscribe semantics on hundreds of topics? Retain per-key orderingWhich system should you choose?","answers":[{"ans":"A. Apache Kafka","val":true},{"ans":"B. Cloud Storage","val":false},{"ans":"C. Cloud Pub\/Sub","val":false},{"ans":"D. Firebase Cloud Messaging","val":false}],"q_expl":"https:\/\/cloud.google.com\/pubsub\/docs\/replay-overview A pub sub can retain message only for 7 days maximum"},{"label":"test_14","q_format":"single","q_text":"You are designing a data processing pipeline. The pipeline must be able to scale automatically as load increases. Messages must be processed at least once and must be ordered within windows of 1 hour. How should you design the solution?","answers":[{"ans":"A. Use Apache Kafka for message ingestion and use Cloud Dataproc for streaming analysis.","val":false},{"ans":"B. Use Apache Kafka for message ingestion and use Cloud Dataflow for streaming analysis.","val":false},{"ans":"C. Use Cloud Pub\/Sub for message ingestion and Cloud Dataproc for streaming analysis.","val":false},{"ans":"D. Use Cloud Pub\/Sub for message ingestion and Cloud Dataflow for streaming analysis.","val":true}],"q_expl":"D: Pub\/Sub + Dataflow https:\/\/cloud.google.com\/solutions\/stream-analytics\/ https:\/\/cloud.google.com\/blog\/products\/data-analytics\/streaming-analytics-now-simpler-more-cost-effective-cloud-dataflow"},{"label":"test_14","q_format":"single","q_text":"You are designing a cloud-native historical data processing system to meet the following conditions:? The data being analyzed is in CSV, Avro, and PDF formats and will be accessed by multiple analysis tools including Cloud Dataproc, BigQuery, and ComputeEngine.? A streaming data pipeline stores new data daily.? Peformance is not a factor in the solution.? The solution design should maximize availability.How should you design data storage for this solution?","answers":[{"ans":"A. Create a Cloud Dataproc cluster with high availability. Store the data in HDFS, and peform analysis as needed.","val":false},{"ans":"B. Store the data in BigQuery. Access the data using the BigQuery Connector on Cloud Dataproc and Compute Engine.","val":false},{"ans":"C. Store the data in a regional Cloud Storage bucket. Access the bucket directly using Cloud Dataproc, BigQuery, and Compute Engine.","val":false},{"ans":"D. Store the data in a multi-regional Cloud Storage bucket. Access the data directly using Cloud Dataproc, BigQuery, and Compute Engine.","val":true}],"q_expl":"D. Store the data in a multi-regional Cloud Storage bucket. Access the data directly using Cloud Dataproc, BigQuery, and Compute Engine. \nHere\u2018s why this option is ideal:\nMultiple Data Formats: Cloud Storage is a flexible object storage service that can handle various data formats like CSV, Avro, and PDFs efficiently.\nMulti-regional Availability: By storing data in a multi-regional Cloud Storage bucket, you achieve maximum availability. Even if a region experiences an outage, the data remains accessible from other regions.\nAccessibility by Tools: Cloud Dataproc, BigQuery, and Compute Engine can all directly access data stored in Cloud Storage buckets. BigQuery and Dataproc offer connectors for seamless interaction with Cloud Storage.\nHere\u2018s why the other options are less suitable:\nA. Cloud Dataproc with HDFS: While Cloud Dataproc offers HDFS (Hadoop Distributed File System), it\u2018s not ideal for your scenario. HDFS is typically used within a Dataproc cluster, and managing HDFS adds complexity. Additionally, the requirement specifies maximizing availability, which HDFS within a single Dataproc cluster wouldn\u2018t provide.\nB. BigQuery: BigQuery, a serverless data warehouse, is primarily designed for structured data analysis. While it can handle some semi-structured formats like CSV, it\u2018s not ideal for storing raw PDFs directly. Cloud Storage is a better fit for storing various data formats in this case.\nC. Regional Cloud Storage: While a regional bucket offers some level of availability, a multi-regional bucket provides a higher level of redundancy and disaster tolerance, which is ideal based on the problem statement."},{"label":"test_14","q_format":"single","q_text":"Which of these sources can you not load data into BigQuery from?","answers":[{"ans":"A. File upload","val":false},{"ans":"B. Google Drive","val":false},{"ans":"C. Google Cloud Storage","val":false},{"ans":"D. Google Cloud SQL","val":true}],"q_expl":"You can load data into BigQuery from a file upload, Google Cloud Storage, Google Drive, or Google Cloud Bigtable. It is not possible to load data into BigQuery directly from Google Cloud SQL. One way to get data from Cloud SQL to BigQuery would be to export data from Cloud SQL to Cloud Storage and then load it from there. Reference: https:\/\/cloud.google.com\/bigquery\/loading-data"},{"label":"test_14","q_format":"single","q_text":"Your company is streaming real-time sensor data from their factory floor into Bigtable and they have noticed extremely poor performance. How should the row key be redesigned to improve Bigtable performance on queries that populate real-time dashboards?","answers":[{"ans":"A. Use a row key of the form .","val":false},{"ans":"B. Use a row key of the form .","val":false},{"ans":"C. Use a row key of the form #.","val":false},{"ans":"D. Use a row key of the form >##.","val":true}],"q_expl":"The correct answer is D. Refer to the link https:\/\/cloud.google.com\/bigtable\/docs\/schema-design for Big table schema design. C is not the right answer becuase Timestamps If you often need to retrieve data based on the time when it was recorded, it\u2018s a good idea to include a timestamp as part of your row key. Using the timestamp by itself as the row key is not recommended, as most writes would be pushed onto a single node. For the same reason, avoid placing a timestamp at the start of the row key. For example, your application might need to record performance-related data, such as CPU and memory usage, once per second for a large number of machines. Your row key for this data could combine an identifier for the machine with a timestamp for the data (for example, machine_4223421#1425330757685)"},{"label":"test_14","q_format":"multiple","q_text":"Which of the following are feature engineering techniques? (Select 2 answers)","answers":[{"ans":"A. Hidden feature layers","val":false},{"ans":"B. Feature prioritization","val":false},{"ans":"C. Crossed feature columns","val":true},{"ans":"D. Bucketization of a continuous feature","val":true}],"q_expl":"Selecting and crafting the right set of feature columns is key to learning an effective model. Bucketization is a process of dividing the entire range of a continuous feature into a set of consecutive bins\/buckets, and then converting the original numerical feature into a bucket ID (as a categorical feature) depending on which bucket that value falls into. Using each base feature column separately may not be enough to explain the data. To learn the differences between different feature combinations, we can add crossed feature columns to the model. Reference: https:\/\/www.tensorflow.org\/tutorials\/wide#selecting_and_engineering_features_for_the_model"},{"label":"test_15","q_format":"single","q_text":"How should you manage user permissions for a BigQuery data warehouse with multiple users where each team should only see certain tables based on their team membership?","answers":[{"ans":"Create SQL views for each team in the same dataset as the data and grant data viewer access to the SQL views for the users\/groups.","val":false},{"ans":"Create authorized views for each team in the same dataset as the data and grant data viewer access to the authorized views for the users\/groups.","val":false},{"ans":"Grant data viewer access to the users\/groups at the table level for each table.","val":false},{"ans":"Create authorized views for each team in datasets created for each team. Grant data viewer access to the authorized views for the dataset where the data resides, and grant data viewer access to the datasets where the authorized views reside for the users\/groups","val":true}],"q_expl":"When we achieve requirement option A, my point of view is , Option D is the fool-proof method than option A. Ref: https:\/\/cloud.google.com\/bigquery\/docs\/authorized-views"},{"label":"test_15","q_format":"single","q_text":"How would you design a data processing pipeline that can automatically scale as the load increases, processes messages at least once, and maintains message order within 1-hour windows?","answers":[{"ans":"Utilize Apache Kafka to ingest messages and Cloud Dataproc for streaming analysis.","val":false},{"ans":"Use Apache Kafka for message ingestion and use Cloud Dataflow for streaming analysis.","val":false},{"ans":"Use Cloud Pub\/Sub for message ingestion and Cloud Dataflow for streaming analysis","val":true},{"ans":"Use Cloud Pub\/Sub for message ingestion and Cloud Dataproc for streaming analysis","val":false}],"q_expl":"Option A is incorrect because Cloud Dataproc is not a suitable choice for streaming analysis, it is more appropriate for batch processing. Option B is a possibility, as Apache Kafka can be used for message ingestion, but Cloud Dataflow is a better option for streaming analysis due to its ability to automatically scale. Option C is incorrect because while Cloud Pub\/Sub can be used for message ingestion, Cloud Dataproc is not a suitable choice for streaming analysis. Option D: Cloud Pub\/Sub is a managed messaging service that allows for the ingestion of messages. It automatically scales to handle high throughput, and messages can be delivered in a ordered fashion. This meets the requirement of processing messages at least once and within a 1-hour window. Cloud Dataflow is a fully-managed service for executing batch and streaming data processing pipelines. It can handle the real-time processing of large volumes of data, and can scale up or down automatically"},{"label":"test_15","q_format":"single","q_text":"Which option should be followed to control access to personally identifiable information (PII) of clients, as mandated by government regulations in the banking industry and required by your company\u2018s data protection standards, while using Cloud Data Loss Prevention (Cloud DLP) and Google-recommended service accounts?","answers":[{"ans":"Assign the required Identity and Access Management (IAM) roles to every employee, and create a single service account to access project resources.","val":true},{"ans":"Use Cloud Storage to comply with major data protection standards. Use one service account shared by all users","val":false},{"ans":"Use Cloud Storage to comply with major data protection standards. Use multiple service accounts attached to IAM groups to grant the appropriate access to each group","val":false},{"ans":"Use one service account to access a Cloud SQL database, and use separate service accounts for each human user","val":false}],"q_expl":"Option A suggests assigning the required Identity and Access Management (IAM) roles to every employee and creating a single service account to access project resources. This approach allows for more centralized management of access control, and IAM roles can be assigned based on the specific access needs of each employee"},{"label":"test_15","q_format":"single","q_text":"An online retailer wants to implement a chatbot to improve their customer service. The chatbot needs to be able to handle both text and voice inquiries, and you want to use a low-code or no-code option to easily train it to provide answers based on keywords. Which of the following options should you choose?","answers":[{"ans":"Build a Python application in App Engine using the Cloud Speech-to-Text API.","val":false},{"ans":"Use Dialogflow for simple queries and the Cloud Speech-to-Text API for complex queries","val":false},{"ans":"Build a Python application in a Compute Engine instance using the Cloud Speech-to-Text API.","val":false},{"ans":"Use Dialogflow to implement the chatbot and define the intents based on the most common queries.","val":true}],"q_expl":"Dialogflow is a natural language understanding platform that makes it easy to design and integrate a conversational user interface into your mobile app, web application, device, bot, interactive voice response system, and so on. Using Dialogflow, you can provide new and engaging ways for users to interact with your product. Dialogflow can analyze multiple types of input from your customers, including text or audio inputs (like from a phone or voice recording). It can also respond to your customers in a couple of ways, either through text or with synthetic speech. Ref: https:\/\/cloud.google.com\/dialogflow\/docs"},{"label":"test_15","q_format":"single","q_text":"You have a Cloud Dataproc cluster and you need to deploy additional dependencies to all of its nodes at startup, but company security policies prohibit Cloud Dataproc nodes from accessing the Internet. You have an existing initialization action for this purpose. What should you do to deploy these dependencies?","answers":[{"ans":"Install Cloud SQL Proxy on the Cloud Dataproc master","val":false},{"ans":"Add the service account used by the Cloud Dataproc cluster to the Network User role using Resource Manager.","val":false},{"ans":"Store all dependencies in a Cloud Storage bucket located within your VPC security perimeter","val":true},{"ans":"Establish an SSH tunnel to provide the Cloud Dataproc cluster with internet access.","val":false}],"q_expl":"Without access to the Internet, you can enable Private Google Access, and place job dependencies in Cloud Storage; cluster nodes can download the dependencies from Cloud Storage from internal IPs. Ref: https:\/\/cloud.google.com\/dataproc\/docs\/concepts\/configuring-clusters\/network#create_a_cloud_dataproc_cluster_with_internal_ip_address_only"},{"label":"test_15","q_format":"single","q_text":"You are tasked with building a new application that requires scalable collection of continuously arriving data, expected to generate approximately 150 GB of JSON data per day by the end of the year. Your requirements include decoupling the producer from the consumer, cost-efficient storage of raw ingested data which should be stored indefinitely, near real-time SQL query capability, and maintaining at least 2 years of historical data that will be queried with SQL. Which pipeline should you use to meet these requirements?","answers":[{"ans":"Create an application that publishes events to Cloud Pub\/Sub, and set up Spark jobs on Cloud Dataproc to convert the JSON data to Avro format, stored on HDFS on Persistent Disk","val":false},{"ans":"Create an application that publishes events to Cloud Pub\/Sub, and set up a Cloud Dataflow pipeline that transforms the JSON event payloads to Avro, writing the data to Cloud Storage and BigQuery","val":true},{"ans":"Build an application that writes to a Cloud SQL database to store the data, with periodic exports of the database to write to Cloud Storage and load into BigQuery.","val":false},{"ans":"Develop an application with an API and a tool to poll the API and write data to Cloud Storage as gzipped JSON files.","val":false}],"q_expl":"Create an application that publishes events to Cloud Pub\/Sub and create a Cloud Dataflow pipeline that transforms the JSON event payloads to Avro, writing the data to Cloud Storage and BigQuery. This option uses Cloud Pub\/Sub to decouple the producer from the consumer, ensuring that the application can generate data continuously without disrupting the downstream processes. The JSON data is then transformed to Avro using a Cloud Dataflow pipeline, which provides near real-time processing of data. The transformed data is then written to both Cloud Storage and BigQuery, ensuring that the data is stored cost-effectively and can be queried with SQL. This pipeline also meets the requirement of maintaining at least 2 years of historical data, as both Cloud Storage and BigQuery are designed to handle large amounts of data for long periods of time."},{"label":"test_15","q_format":"single","q_text":"Given a dataset of real estate properties and their associated features, including latitude and longitude, and the goal of training a neural network to predict housing prices, what approach should be taken to incorporate the physical dependency of location on price. Which of the following options would be most appropriate for this task?","answers":[{"ans":"providing latitude and longitude as input vectors to the neural net","val":false},{"ans":"creating a feature cross of latitude and longitude and bucketizing it at the minute level while using L1 regularization during optimization, or","val":true},{"ans":"creating a numeric column from a feature cross of latitude and longitude","val":false},{"ans":"creating a feature cross of latitude and longitude and bucketizing it at the minute level while using L2 regularization during optimization?","val":false}],"q_expl":"Use L1 regularization when you need to assign greater importance to more influential features. It shrinks less important feature to 0. L2 regularization performs better when all input features influence the output & all with the weights are of equal size. Ref: https:\/\/developers.google.com\/machine-learning\/crash-course\/regularization-for-sparsity\/l1-regularization"},{"label":"test_15","q_format":"single","q_text":"How should you set up a windowed pipeline in DataFlow to compute a moving average of a company\u2018s stock price every 5 seconds using the past 30 seconds\u2018 worth of data from Pub\/Sub?","answers":[{"ans":"Use a sliding window with a duration of 5 seconds and emit results using a trigger of AfterProcessingTime.pastFirstElementInPane().plusDelayOf(Duration.standardSeconds(30)).","val":false},{"ans":"Use a sliding window with a duration of 30 seconds and a period of 5 seconds and emit results using a trigger of AfterWatermark.pastEndOfWindow().","val":true},{"ans":"Use a fixed window with a duration of 30 seconds and emit results using a trigger of AfterWatermark.pastEndOfWindow().plusDelayOf(Duration.standardSeconds(5)).","val":false},{"ans":"Use a fixed window with a duration of 5 seconds and emit results using a trigger of AfterProcessingTime.pastFirstElementInPane().plusDelayOf(Duration.standardSeconds(30))","val":false}],"q_expl":"You set the following windows with the Apache Beam SDK or Dataflow SQL streaming extensions: Hopping windows (called sliding windows in Apache Beam) A hopping window represents a consistent time interval in the data stream. Hopping windows can overlap, whereas tumbling windows are disjoint. For example, a hopping window can start every thirty seconds and capture one minute of data. The frequency with which hopping windows begin is called the period. This example has a one-minute window and thirty-second period. Ref: https:\/\/cloud.google.com\/dataflow\/docs\/concepts\/streaming-pipelines#hopping-windows"},{"label":"test_15","q_format":"single","q_text":"You manage a data pipeline in BigQuery for your analytics platform, where new data is loaded daily and transformed using an ETL pipeline. The ETL pipeline is frequently updated and may produce errors that can remain undetected for up to two weeks. You need to ensure that you can recover from such errors and optimize storage costs for backups. Which approach should you take?","answers":[{"ans":"Create separate tables for each month, then duplicate the data in a separate dataset in BigQuery","val":false},{"ans":"Create separate tables for each month, then export, compress, and store the data in Cloud Storage","val":true},{"ans":"Create separate tables for each month and use snapshot decorators to restore a table to an earlier version before the error occurred","val":false},{"ans":"Store all data in a single table, then export and compress the data in Cloud Storage","val":false}],"q_expl":"By creating separate tables for each month, you can easily identify and recover data from a particular time period if an error occurs. Exporting, compressing, and storing data in Cloud Storage can help optimize storage costs. Duplicating data to a separate dataset or storing all data in a single table can quickly lead to increased storage costs and make it difficult to pinpoint where errors occurred. Snapshot decorators allow you to restore a table to an earlier state, but they can be expensive and are typically used for short-term recovery rather than long-term backups"},{"label":"test_15","q_format":"single","q_text":"You are operating a stock trading database and application that retrieves average stock prices for a given company over an adjustable window of time. The database is stored in Cloud Bigtable, with the datetime of the stock trade as the beginning of the row key. The application is experiencing degraded performance as more stocks are added and concurrent users increase. What should you do to improve the performance of the application?","answers":[{"ans":"Change the Cloud Bigtable row key syntax to begin with the stock symbol","val":true},{"ans":"Change the Cloud Bigtable row key syntax to begin with a random number per second","val":false},{"ans":"Use Cloud Dataflow to summarize daily stock trades and store them in an Avro file on Cloud Storage. Update the application to read from both Cloud Storage and Cloud Bigtable to compute the responses","val":false},{"ans":"Change the data pipeline to use BigQuery for storing stock trades and update the application","val":false}],"q_expl":"To improve the performance of your application for retrieving average stock prices over an adjustable window of time, you should consider option A: A. Change the row key syntax in your Cloud Bigtable table to begin with the stock symbol. Here\u2018s why this option is a good choice: Data Organization: Storing data with the stock symbol at the beginning of the row key is often a good practice when you need to query data related to specific stocks. This allows for efficient retrieval of data for a particular stock symbol. Query Optimization: Placing the stock symbol at the beginning of the row key allows you to perform range scans efficiently, as you can easily filter and retrieve data for a specific stock symbol. Avoiding Hotspots: Option B suggests using a random number per second as the row key. While this can distribute the data more evenly, it may not be as efficient for your use case because you need to retrieve data for specific stocks. Randomizing the row keys can introduce hotspots and make it more challenging to efficiently retrieve data for specific stock symbols. Option C and D: These options suggest using BigQuery and Cloud Dataflow, respectively, to store and process your data. While they can be powerful for certain use cases, they introduce additional complexity and may not be necessary if Cloud Bigtable can handle your workload efficiently with a well-designed row key. In summary, organizing your data in Cloud Bigtable with the stock symbol at the beginning of the row key will help improve the performance of your application, especially when you need to retrieve data for specific stocks."},{"label":"test_15","q_format":"single","q_text":"What is the most effective way to design a pipeline that generates a globally unique identifier (GUID) for new website users using a service that receives data points and returns a GUID. The pipeline should handle tens of thousands of messages per second and should be multi-threaded to minimize backpressure on the system?","answers":[{"ans":"Use HTTP calls to call the service.","val":false},{"ans":"Batch the job into ten-second increments.","val":true},{"ans":"Create a new object in the startBundle method of DoFn","val":false},{"ans":"Create a static pipeline in the class definition","val":false}],"q_expl":"For example, imagine a pipeline that\u2018s processing tens of thousands of messages per second in steady state. If you made a callout per element, you would need the system to deal with the same number of API calls per second. Also, if the call takes on average 1 sec, that would cause massive backpressure on the pipeline. In these circumstances you should consider batching these requests, instead. Ref: https:\/\/cloud.google.com\/blog\/products\/data-analytics\/guide-to-common-cloud-dataflow-use-case-patterns-part-1"},{"label":"test_15","q_format":"single","q_text":"You are working on a new Google Cloud pipeline to stream IoT data from Cloud Pub\/Sub to BigQuery via Cloud Dataflow. While previewing the data, you noticed that approximately 2% of the data is corrupt. To filter out this corrupt data, what modification should you make to the Cloud Dataflow pipeline (Consider the following options)?","answers":[{"ans":"Incorporate a SideInput that returns a Boolean value for corrupt elements.","val":false},{"ans":"Implement a Partition transform in Cloud Dataflow to separate corrupt data from valid data","val":false},{"ans":"Add a GroupByKey transform to Cloud Dataflow to group valid data together and discard corrupt data.","val":false},{"ans":"Add a ParDo transform within the Cloud Dataflow to discard corrupt elements.","val":true}],"q_expl":"The ParDo transform in Cloud Dataflow can be used to filter out corrupt elements by adding a DoFn that checks each element and filters out the corrupt ones. This is the most efficient way to remove corrupt elements from the data stream, as it allows the pipeline to continue processing only the valid data. Option A (incorporating a SideInput) could also be used, but it may not be the most efficient solution since it involves adding an extra input and requires additional processing time. Option C (implementing a Partition transform) may not be the best solution, as it separates corrupt data from valid data, but doesn\u2018t actually remove it from the pipeline. Option D (adding a GroupByKey transform) would group all of the valid data together, but wouldn\u2018t actually remove the corrupt data from the pipeline, so it\u2018s not the correct option."},{"label":"test_15","q_format":"single","q_text":"How can you optimize the performance of a complicated analytical Spark job with shuffling operations and initial data in parquet format (average size of 200-400 MB each) after migrating it from an on-prem Hadoop cluster to Dataproc on GCS, while keeping in mind the cost sensitivity of your organization. The Spark job is currently running on preemptible VMs with only two non-preemptible workers?","answers":[{"ans":"Change from using HDDs to SSDs, copy initial data from GCS to HDFS, run the Spark job, and copy the results back to GCS.","val":false},{"ans":"Change from using HDDs to SSDs and modify the configuration of preemptible VMs to increase the boot disk size.","val":false},{"ans":"Switch from using parquet files to TFRecords formats, which are approximately 200 MB per file.","val":false},{"ans":"Ensure that the parquet files are at least 1 GB in size.","val":true}],"q_expl":"C&D is not the requirement of cost effetcive solution. And shuffling is required. https:\/\/cloud.google.com\/dataproc\/docs\/support\/spark-job-tuning#limit_the_number_of_files"},{"label":"test_15","q_format":"single","q_text":"How can you set up access to BigQuery data for different departments in your company while adhering to the following requirements. 1.Each department should only have access to their data. 2.Each department has one or more leads who should be able to create and update tables for their team. 3.Each department has data analysts who should be able to query but not modify data.?","answers":[{"ans":"Create a dataset for each department. Assign the department leads the OWNER role, and assign the data analysts the WRITER role on their dataset.","val":false},{"ans":"Create a table for each department. Assign the department leads the Owner role, and assign the data analysts the Editor role on the project the table is in.","val":false},{"ans":"Create a dataset for each department. Assign the department leads the WRITER role, and assign the data analysts the READER role on their dataset.","val":true},{"ans":"Create a table for each department. Assign the department leads the Editor role, and assign the data analysts the Viewer role on the project the table is in","val":false}],"q_expl":"The \u201cOWNER\u201c , WRITER, READER are Google\u2018s \u201cprimitive roles\u201c before IAM roles came up and are applicable at \u201cDataset\u201c levels for Big Query. Big Query now supports IAM roles at the \u201cproject\u201c level \u2013 which are \u201cVIEWER, EDITER, OWNER\u201c. Since, the questions asks for permissions at \u201cdepartment level\u201c \u2013 Options C and D are ruled out because that is being granted a \u201ctable\u201c level. Between Option A and B \u2013 B looks to be correct assuming we are granting primitive roles at Dataset level. Reference: https:\/\/cloud.google.com\/bigquery\/docs\/access-control-basic-roles#dataset-primitive-roles"},{"label":"test_15","q_format":"single","q_text":"You have a BigQuery table and you run a query with a WHERE clause that filters the data based on a timestamp and ID column. However, after using the bq query \u201c-dry_run\u201c command, you discover that the query triggers a full scan of the table, even though the filter only selects a small fraction of the data. You want to minimize the amount of data scanned by BigQuery while keeping your SQL queries intact. Which approach should you take?","answers":[{"ans":"Recreate the table with a partitioning column and clustering column","val":true},{"ans":"Use the LIMIT keyword to reduce the number of rows returned","val":false},{"ans":"Create a separate table for each ID","val":false},{"ans":"Use the bq query --maximum_bytes_billed flag to restrict the number of bytes billed","val":false}],"q_expl":"Option C is the right choice : https:\/\/cloud.google.com\/bigquery\/docs\/partitioned-tables A partitioned table is a special table that is divided into segments, called partitions, that make it easier to manage and query your data. By dividing a large table into smaller partitions, you can improve query performance, and you can control costs by reducing the number of bytes read by a query. https:\/\/cloud.google.com\/bigquery\/docs\/clustered-tables Clustered tables in BigQuery are tables that have a user-defined column sort order using clustered columns. Clustered tables can improve query performance and reduce query costs."},{"label":"test_15","q_format":"single","q_text":"You are in charge of a system that ingests messages from IoT devices globally. Currently, all messages are ingested by a single on-premises Kafka cluster located in the us-east region. However, due to sporadic spikes in message volume caused by batched messages, it has become difficult and expensive to manage. Which Google Cloud architecture would be recommended for this scenario?","answers":[{"ans":"Connect an IoT gateway to Cloud Pub\/Sub and use Cloud Dataflow to process the messages.","val":true},{"ans":"Virtualize a Kafka cluster on Compute Engine in us-east and use Cloud Load Balancing to connect with IoT devices globally","val":false},{"ans":"Connect Cloud Dataflow to the Kafka cluster to scale message processing.","val":false},{"ans":"Use Edge TPUs as sensor devices for storing and transmitting messages.","val":false}],"q_expl":"Option C:The recommended Google Cloud architecture for ingesting messages from IoT devices globally while avoiding the challenges caused by sporadic spikes in message volume is option C: Connect an IoT gateway to Cloud Pub\/Sub and use Cloud Dataflow to process the messages. This architecture allows an IoT gateway to batch the messages from IoT devices and send them to Cloud Pub\/Sub, which can then scale to handle any incoming load. Cloud Dataflow can then process the messages from Cloud Pub\/Sub, providing a scalable, managed solution for processing incoming messages. Option A, using Edge TPUs as sensor devices for storing and transmitting messages, is not recommended as it involves storing data at the edge and might not scale well in the long term. Option B, connecting Cloud Dataflow to the Kafka cluster to scale message processing, might not solve the problem caused by the spikes in message volume, as the Kafka cluster might still struggle to handle the volume of messages. Option D, virtualizing a Kafka cluster on Compute Engine in us-east and using Cloud Load Balancing to connect with IoT devices globally, might not be as scalable and cost-effective as using Cloud Pub\/Sub and Cloud Dataflow. Additionally, it might not solve the issue caused by spikes in message volume, as the Kafka cluster might still face issues handling the volume of messages."},{"label":"test_15","q_format":"single","q_text":"What is the best primary key strategy for a new transaction table in Cloud Spanner that stores product sales data, considering performance?","answers":[{"ans":"Use the original order identification number from the sales system, which is a monotonically increasing integer, as the primary key.","val":false},{"ans":"Use the current epoch time as the primary key","val":false},{"ans":"Concatenate the product name and the current epoch time as the primary key.","val":false},{"ans":"Generate a random universally unique identifier number (version 4 UUID) as the primary key","val":true}],"q_expl":"Ref: https:\/\/cloud.google.com\/spanner\/docs\/schema-design"},{"label":"test_15","q_format":"single","q_text":"How can you efficiently process and load application events published to a Pub\/Sub topic into BigQuery, ensuring scalability for large volumes of events, and aggregating them across hourly intervals?","answers":[{"ans":"Use a Cloud Function triggered by Pub\/Sub to perform data processing every time a new message is published.","val":false},{"ans":"Schedule a batch Dataflow job to run hourly, pulling available messages from the Pub\/Sub topic and performing the necessary aggregations","val":false},{"ans":"Use a streaming Dataflow job that reads continuously from the Pub\/Sub topic and performs the necessary aggregations using tumbling windows.","val":true},{"ans":"Schedule a Cloud Function to run hourly, pulling available messages from the Pub\/Sub topic and performing the necessary aggregations","val":false}],"q_expl":"Ref: https:\/\/cloud.google.com\/dataflow\/docs\/concepts\/streaming-pipelines#tumbling-windows"},{"label":"test_15","q_format":"multiple","q_text":"Your Cloud Functions written in Node.js retrieve messages from Cloud Pub\/Sub and forward the data to BigQuery. You notice that the message processing rate on the Pub\/Sub topic is much higher than expected, and no errors are reported in Cloud Logging. Which two of the following are the most likely causes of this issue. (Choose two.)?","answers":[{"ans":"The total number of outstanding messages exceeds the 10 MB limit","val":false},{"ans":"The subscriber code does not handle runtime errors properly","val":true},{"ans":"The subscriber code does not acknowledge the messages it pulls.","val":true},{"ans":"The publisher throughput quota is too small","val":false},{"ans":"The subscriber code is unable to keep up with the message volume.","val":false}],"q_expl":"A. The publisher throughput quota is too small: This option is unlikely to be the cause of the problem, as a low throughput quota would cause issues with publishing messages rather than subscribing to messages. Thus, it is not one of the two most likely causes of the issue. B. The total number of outstanding messages exceeds the 10 MB limit: This option is also unlikely to be the cause of the problem, as the 10 MB limit applies to the total size of messages in a single request, rather than the message processing rate. Thus, it is not one of the two most likely causes of the issue. C. The subscriber code does not handle runtime errors properly: This option is a possible cause of the issue, as if the subscriber code is not handling errors properly, it may be failing silently instead of logging the errors. This can result in a higher message processing rate than expected without any errors being reported in Cloud Logging. D. The subscriber code is unable to keep up with the message volume: This option is also a likely cause of the issue, as a high message processing rate can overwhelm the subscriber code, causing it to fall behind and potentially miss messages. If the subscriber code is not optimized for high message volumes, it may not be able to keep up with the incoming messages. E. The subscriber code does not acknowledge the messages it pulls: This option is also a possible cause of the issue, as failing to acknowledge messages can cause them to be redelivered, which can result in a higher message processing rate than expected. If the subscriber code is not acknowledging messages properly, it may be causing the message processing rate to increase beyond what was anticipated. Therefore, the two most likely causes of the problem are option C (the subscriber code does not handle runtime errors properly) and Option E (the subscriber code does not acknowledge the messages it pulls)"},{"label":"test_15","q_format":"single","q_text":"You have a platform on your on-premises environment that generates millions of structured JSON text files totaling 100 GB daily. However, the on-premises environment cannot be accessed from the public internet. You want to explore and query the platform data using Google Cloud products. What is the recommended solution?","answers":[{"ans":"Use the BigQuery Data Transfer Service dataset copy feature to directly transfer all data into BigQuery.","val":false},{"ans":"Use Cloud Scheduler to transfer the data daily to Cloud Storage, then import the data into BigQuery using the BigQuery Data Transfer Service.","val":false},{"ans":"Use a Transfer Appliance to transfer the data to Cloud Storage, then import the data into BigQuery using the BigQuery Data Transfer Service.","val":false},{"ans":"Use Transfer Service for on-premises data to transfer the data to Cloud Storage, then import the data into BigQuery using the BigQuery Data Transfer Service.","val":true}],"q_expl":"The Transfer Service for on-premises data is designed specifically for securely transferring large volumes of data from on-premises environments to Google Cloud Storage. Since the on-premises environment cannot be accessed from the public internet, this service can be used to establish a secure connection and transfer the data. After transferring the data to Cloud Storage, the BigQuery Data Transfer Service can be used to import the data into BigQuery. This service provides an easy way to schedule and automate data imports from various sources into BigQuery. Option A involves using Cloud Scheduler to copy data from the on-premises environment to Cloud Storage, which may not be a feasible solution due to the lack of internet access. Option B involves using a Transfer Appliance, which may not be necessary for transferring 100 GB of data daily. Option D involves directly transferring all data into BigQuery, which may not be efficient or practical for large volumes of data"},{"label":"test_15","q_format":"multiple","q_text":"You are using Dataflow to process data from a Pub\/Sub topic and write it to a BigQuery dataset located in the EU. However, during peak periods, your pipeline is experiencing delays due to all three n1-standard-1 workers being at maximum CPU utilization. Which two actions can you take to increase your pipeline\u2018s performance?","answers":[{"ans":"Create a temporary buffer table in Cloud Spanner to write to before transferring to BigQuery","val":false},{"ans":"Increase the maximum number of workers in your pipeline","val":true},{"ans":"Create a temporary buffer table in Bigtable to write to before transferring to BigQuery","val":false},{"ans":"Use a larger instance type for your Dataflow workers","val":true},{"ans":"Change the zone of your Dataflow pipeline to run in us-central1","val":false}],"q_expl":"A: Increasing the maximum number of workers can help distribute the workload and prevent CPU utilization from becoming the bottleneck. B: Using a larger instance type for the Dataflow workers can also provide more CPU and memory resources to handle the increased workload. C: Changing the zone of the Dataflow pipeline to us-central1 may not be effective, as it may increase network latency and add more overhead. D and E: Creating a temporary buffer table in Bigtable or Cloud Spanner can potentially improve the pipeline\u2018s performance, but it may also add additional complexity and overhead. These options should be considered if increasing the workers or using a larger instance type do not provide enough improvement"},{"label":"test_15","q_format":"multiple","q_text":"You are tasked with building a storage system to ingest vehicle telemetry data in real-time using Cloud Datastore, while also ensuring the system accounts for long-term data growth and is cost-effective. Additionally, you need to create snapshots of the data periodically to enable point-in-time recovery or cloning a copy of the data to a different environment. You want to archive these snapshots for an extended period. Which two methods can achieve this (Choose two of the following options)?","answers":[{"ans":"Develop an application that leverages Cloud Datastore client libraries to read all the entities, treating each entity as a BigQuery table row via BigQuery streaming insert. Assign an export timestamp for each export and attach it as an extra column for each row. Ensure that the BigQuery table is partitioned using the export timestamp column","val":false},{"ans":"Use managed export to export the data from Cloud Datastore and import it into a BigQuery table created solely for the export, and then delete the temporary export files","val":false},{"ans":"Develop an application that uses Cloud Datastore client libraries to read all the entities and format the exported data into a JSON file. Apply compression before storing the data in Cloud Source Repositories","val":false},{"ans":"Use managed export to export the data from Cloud Datastore and store it in a Cloud Storage bucket with Nearline or Coldline storage class","val":true},{"ans":"Use managed export to export the data from Cloud Datastore and import it to Cloud Datastore in a separate project under a unique namespace reserved for the export","val":true}],"q_expl":"The two methods that can accomplish ingesting and archiving vehicle telemetry data in real-time using Cloud Datastore, while keeping costs low, and creating periodic snapshots for point-in-time recovery or cloning are A and B"},{"label":"test_15","q_format":"single","q_text":"How can you efficiently deploy a data processing application on Google Kubernetes Engine (GKE), ensuring that containers are launched with the latest available configurations from a container registry, while also ensuring that your GKE nodes have GPUs, local SSDs, and 8 Gbps bandwidth, and you can manage the deployment process?","answers":[{"ans":"Use gcloud commands to provision the infrastructure and enable autoscaling of containers on GKE","val":false},{"ans":"Use gcloud commands and Compute Engine startup scripts to pull container images and provision the infrastructure","val":false},{"ans":"Use Cloud Scheduler and Dataflow to provision the data pipeline and run the job.","val":false},{"ans":"Use Cloud Build with Terraform build to provision the infrastructure and launch containers with the latest available configurations from the container registry","val":true}],"q_expl":"The correct answer is B. Use Cloud Build with Terraform build to provision the infrastructure and launch containers with the latest available configurations from the container registry. Using Cloud Build with Terraform build is a best practice for automating infrastructure deployment in a reproducible way. By using Terraform to define the infrastructure as code, you can ensure that your GKE nodes have GPUs, local SSDs, and 8 Gbps bandwidth, and that the latest available container images are used. Cloud Build can then be used to schedule a job that provisions the infrastructure and launches the containers, allowing for an efficient deployment process. Option A is incorrect because Compute Engine startup scripts are not the best way to provision infrastructure on GKE. Option C is incorrect because it does not take advantage of the benefits of using Terraform and Cloud Build. Option D is incorrect because Dataflow is not the appropriate tool for provisioning infrastructure on GKE."},{"label":"test_15","q_format":"single","q_text":"How can you create a secure queuing system for an online brokerage company\u2018s high-volume trade processing architecture in Google Cloud, where jobs are triggered and run through the company\u2018s Python API while minimizing resource usage?","answers":[{"ans":"Utilize Cloud Composer to subscribe to a Pub\/Sub topic and execute the Python API.","val":false},{"ans":"Develop an application running on a Compute Engine instance to make a push subscription to the Pub\/Sub topic","val":false},{"ans":"Implement a Pub\/Sub push subscription to trigger a Cloud Function that passes data to the Python API","val":true},{"ans":"Create an application that builds a queue in a NoSQL database.","val":false}],"q_expl":"Implement a Pub\/Sub push subscription to trigger a Cloud Function that passes data to the Python API. This option allows for a secure and efficient queuing system that triggers jobs, with the Pub\/Sub push subscription acting as the trigger. The Cloud Function can then pass the data to the Python API to execute trades. This approach avoids the need to write and host an application on a Compute Engine instance, and the use of a NoSQL database, which can add complexity and cost. Additionally, using Cloud Composer to subscribe to a Pub\/Sub topic may be overkill for this scenario, as it is a tool that is primarily used for managing complex workflows."},{"label":"test_15","q_format":"single","q_text":"If a company has a hybrid cloud strategy and a complex data pipeline that involves moving data between different cloud providers and leveraging their services, which cloud-native service would be the best option to orchestrate the entire pipeline?","answers":[{"ans":"Cloud Dataflow","val":false},{"ans":"Cloud Dataproc","val":false},{"ans":"Cloud Composer","val":true},{"ans":"Cloud Dataprep","val":false}],"q_expl":"Of the four options given, Cloud Composer would be the most suitable cloud-native service to orchestrate the entire pipeline. Cloud Composer is a fully managed workflow orchestration service that allows users to create, schedule, and monitor workflows across various cloud services and on-premises resources. It offers a wide range of pre-built connectors to different cloud providers, including Amazon Web Services, Google Cloud Platform, and Microsoft Azure, making it an ideal choice for companies with a hybrid cloud strategy. Moreover, it provides a user-friendly interface for building and monitoring workflows, and supports both Apache Airflow and Kubernetes, giving users flexibility in choosing their preferred workflow management tool. On the other hand, Cloud Dataflow is a fully managed service for executing batch and streaming data processing pipelines, while Cloud Dataproc is a fully managed service for running Apache Hadoop and Apache Spark jobs on clusters. Cloud Dataprep, on the other hand, is a data preparation service that allows users to explore, clean, and transform their data. While these services are useful in specific scenarios, they are not designed to orchestrate an entire data pipeline involving multiple cloud services and on-premises resources."},{"label":"test_15","q_format":"single","q_text":"What is the recommended way to ensure high availability in the event of a zone failure when deploying Cloud SQL using MySQL?","answers":[{"ans":"Create a Cloud SQL instance in one zone, and create a read replica in another zone within the same region","val":false},{"ans":"Create a Cloud SQL instance in one zone, and create a failover replica in another zone within the same region.","val":true},{"ans":"Create a Cloud SQL instance in one zone, and configure an external read replica in a zone in a different region","val":false},{"ans":"Create a Cloud SQL instance in a region, and configure automatic backup to a Cloud Storage bucket in the same region.","val":false}],"q_expl":"The recommended way to ensure high availability in the event of a zone failure when deploying Cloud SQL using MySQL is to create a Cloud SQL instance in one zone, and create a failover replica in another zone within the same region (Option A). T his provides automatic failover to the replica instance in the event of an outage in the primary zone, minimizing downtime and ensuring data is highly available. Additionally this setup also provides regional redundancy to guard against an outage in the entire region."},{"label":"test_15","q_format":"single","q_text":"You work for a bank. You have a labelled dataset that contains information on already granted loan application and whether these applications have been defaulted. You have been asked to train a model to predict default rates for credit applicants.What should you do?","answers":[{"ans":"A. Increase the size of the dataset by collecting additional data.","val":false},{"ans":"B. Train a linear regression to predict a credit default risk score.","val":true},{"ans":"C. Remove the bias from the data and collect applications that have been declined loans.","val":false},{"ans":"D. Match loan applicants with their social profiles to enable feature engineering.","val":false}],"q_expl":"The best approach to this task would be:\nB. Train a linear regression to predict a credit default risk score.\nHere\u2019s why:\n\nLinear regression: It\u2019s a suitable algorithm for predicting a continuous numerical value like a credit default risk score. It\u2019s relatively simple to implement and interpret, making it a good starting point for this task.\nLabelled dataset: You already have a dataset with labels (default or not defaulted), which is essential for supervised learning algorithms like linear regression.\nRisk score prediction: The goal is to predict a risk score, not a binary classification (default or not default). Linear regression can directly output a continuous score.\n\nWhile increasing the dataset size or removing bias might be beneficial, they are not the most direct and actionable steps given the current context. Matching loan applicants with their social profiles might introduce additional features but would require careful consideration of privacy and ethical implications.\nRemember that model training is an iterative process. You might need to experiment with different algorithms, feature engineering techniques, and model evaluation metrics to find the best approach for your specific problem."},{"label":"test_15","q_format":"single","q_text":"You\u2018ve migrated a Hadoop job from an on-prem cluster to dataproc and GCS. Your Spark job is a complicated analytical workload that consists of many shuffing operations and initial data are parquet files (on average 200-400 MB size each). You see some degradation in performance after the migration to Dataproc, so you\u2018d like to optimize for it. You need to keep in mind that your organization is very cost-sensitive, so you\u2018d like to continue using Dataproc on preemptibles (with 2 non-preemptible workers only) for this workload.What should you do?","answers":[{"ans":"A. Increase the size of your parquet files to ensure them to be 1 GB minimum.","val":false},{"ans":"B. Switch to TFRecords formats (appr. 200MB per file) instead of parquet files.","val":false},{"ans":"C. Switch from HDDs to SSDs, copy initial data from GCS to HDFS, run the Spark job and copy results back to GCS.","val":true},{"ans":"D. Switch from HDDs to SSDs, override the preemptible VMs configuration to increase the boot disk size.","val":false}],"q_expl":"The most effective and cost-sensitive approach to optimize your Spark job on Dataproc with preemptibles would be:\nC. Switch from HDDs to SSDs, copy initial data from GCS to HDFS, run the Spark job and copy results back to GCS.\nHere\u2019s a breakdown of why this is the best option:\n\nSSD performance: SSDs offer significantly faster read and write speeds compared to HDDs. This will dramatically improve the performance of your Spark job, especially for shuffling operations that involve frequent disk I\/O.\nData locality: Copying the initial data from GCS to HDFS ensures that the data is located on the same nodes as the Spark executors, reducing network latency and improving performance.\nPreemptible VMs: Using preemptible VMs is a cost-effective way to run Spark jobs on Dataproc. By optimizing the job\u2019s performance, you can reduce the overall execution time and minimize the cost of using preemptibles.\nGCS for storage: Using GCS for storage is a cost-effective and scalable solution. It\u2019s ideal for storing large datasets and can be easily integrated with Dataproc.\n\nWhile increasing the size of parquet files might improve performance to some extent, it\u2019s not always the most efficient solution, especially if the files are already large. Switching to TFRecords might not provide significant performance benefits for this particular workload, and it would require additional effort to convert the data format. Overriding the preemptible VMs configuration to increase the boot disk size might not be necessary if you\u2019re using SSDs for storage.\nBy following this approach, you can optimize your Spark job\u2019s performance on Dataproc while minimizing costs, ensuring that your organization\u2019s budget constraints are met."},{"label":"test_15","q_format":"single","q_text":"Your company is running their first dynamic campaign, serving different offers by analyzing real-time data during the holiday season. The data scientists are collecting terabytes of data that rapidly grows every hour during their 30-day campaign. They are using Google Cloud Dataflow to preprocess the data and collect the feature (signals) data that is needed for the machine learning model in Google Cloud Bigtable. The team is observing suboptimal performance with reads and writes of their initial load of 10 TB of data. They want to improve this performance while minimizing cost. What should they do?","answers":[{"ans":"A. Redefine the schema by evenly distributing reads and writes across the row space of the table.","val":true},{"ans":"B. The performance issue should be resolved over time as the site of the BigDate cluster is increased.","val":false},{"ans":"C. Redesign the schema to use a single row key to identify values that need to be updated frequently in the cluster.","val":false},{"ans":"D. Redesign the schema to use row keys based on numeric IDs that increase sequentially per user viewing the offers.","val":false}],"q_expl":"A as the schema needs to be redesigned to distribute the reads and writes evenly across each table. Refer GCP documentation \u2013 Bigtable Performance: https:\/\/cloud.google.com\/bigtable\/docs\/performance The table\u2018s schema is not designed correctly. To get good performance from Cloud Bigtable, it\u2018s essential to design a schema that makes it possible to distribute reads and writes evenly across each table. See Designing Your Schema for more information. https:\/\/cloud.google.com\/bigtable\/docs\/schema-design Option B is wrong as increasing the size of cluster would increase the cost. Option C is wrong as single row key for frequently updated identifiers reduces performance Option D is wrong as sequential IDs would degrade the performance. A safer approach is to use a reversed version of the user\u2018s numeric ID, which spreads traffic more evenly across all of the nodes for your Cloud Bigtable table."},{"label":"test_15","q_format":"multiple","q_text":"You have a data pipeline that writes data to Cloud Bigtable using well-designed row keys. You want to monitor your pipeline to determine when to increase the size of you Cloud Bigtable cluster. Which two actions can you take to accomplish this? (Choose two.)","answers":[{"ans":"A. Review Key Visualizer metrics. Increase the size of the Cloud Bigtable cluster when the Read pressure index is above 100.","val":false},{"ans":"B. Review Key Visualizer metrics. Increase the size of the Cloud Bigtable cluster when the Write pressure index is above 100.","val":false},{"ans":"C. Monitor the latency of write operations. Increase the size of the Cloud Bigtable cluster when there is a sustained increase in write latency.","val":true},{"ans":"D. Monitor storage utilization. Increase the size of the Cloud Bigtable cluster when utilization increases above 70% of max capacity.","val":true},{"ans":"E. Monitor latency of read operations. Increase the size of the Cloud Bigtable cluster of read operations take longer than 100 ms.","val":false}],"q_expl":"Answer is C & D. C \u0096> Adding more nodes to a cluster (not replication) can improve the write performance https:\/\/cloud.google.com\/bigtable\/docs\/performance D \u0096> since Google recommends adding nodes when storage utilization is > 70% https:\/\/cloud.google.com\/bigtable\/docs\/modifying-instance#nodes"},{"label":"test_15","q_format":"multiple","q_text":"MJTelco Case Study -Company Overview -MJTelco is a startup that plans to build networks in rapidly growing, underserved markets around the world. The company has patents for innovative optical communications hardware. Based on these patents, they can create many reliable, high-speed backbone links with inexpensive hardware.Company Background -Founded by experienced telecom executives, MJTelco uses technologies originally developed to overcome communications challenges in space. Fundamental to their operation, they need to create a distributed data infrastructure that drives real-time analysis and incorporates machine learning to continuously optimize their topologies. Because their hardware is inexpensive, they plan to overdeploy the network allowing them to account for the impact of dynamic regional politics on location availability and cost.Their management and operations teams are situated all around the globe creating many-to-many relationship between data consumers and provides in their system. After careful consideration, they decided public cloud is the perfect environment to support their needs.Solution Concept -MJTelco is running a successful proof-of-concept (PoC) project in its labs. They have two primary needs:? Scale and harden their PoC to support significantly more data flows generated when they ramp to more than 50,000 installations.? Refine their machine-learning cycles to verify and improve the dynamic models they use to control topology definition.MJTelco will also use three separate operating environments \u201c\u201c development\/test, staging, and production \u201c\u201c to meet the needs of running experiments, deploying new features, and serving production customers.Business Requirements -? Scale up their production environment with minimal cost, instantiating resources when and where needed in an unpredictable, distributed telecom user community.? Ensure security of their proprietary data to protect their leading-edge machine learning and analysis.? Provide reliable and timely access to data for analysis from distributed research workers? Maintain isolated environments that support rapid iteration of their machine-learning models without affecting their customers.Technical Requirements -? Ensure secure and efficient transport and storage of telemetry data? Rapidly scale instances to support between 10,000 and 100,000 data providers with multiple flows each.? Allow analysis and presentation against data tables tracking up to 2 years of data storing approximately 100m records\/day? Support rapid iteration of monitoring infrastructure focused on awareness of data pipeline problems both in telemetry flows and in production learning cycles.CEO Statement -Our business model relies on our patents, analytics and dynamic machine learning. Our inexpensive hardware is organized to be highly reliable, which gives us cost advantages. We need to quickly stabilize our large distributed data pipelines to meet our reliability and capacity commitments.CTO Statement -Our public cloud services must operate as advertised. We need resources that scale and keep our data secure. We also need environments in which our data scientists can carefully study and quickly adapt our models. Because we rely on automation to process our data, we also need our development and test environments to work as we iterate.CFO Statement -The project is too large for us to maintain the hardware and software required for the data and analysis. Also, we cannot afford to staff an operations team to monitor so many data feeds, so we will rely on automation and infrastructure. Google Cloud\u2018s machine learning will allow our quantitative researchers to work on our high-value problems instead of problems with our data pipelines.You create a new report for your large team in Google Data Studio 360. The report uses Google BigQuery as its data source. It is company policy to ensure employees can view only the data associated with their region, so you create and populate a table for each region. You need to enforce the regional access policy to the data.Which two actions should you take? (Choose two.)","answers":[{"ans":"A. Ensure all the tables are included in global dataset.","val":false},{"ans":"B. Ensure each table is included in a dataset for a region.","val":true},{"ans":"C. Adjust the settings for each table to allow a related region-based security group view access.","val":false},{"ans":"D. Adjust the settings for each view to allow a related region-based security group view access.","val":false},{"ans":"E. Adjust the settings for each dataset to allow a related region-based security group view access.","val":true}],"q_expl":"B. Ensure each table is included in a dataset for a region. : Segregating data into regional datasets aligns with MJTelco\u2018s policy of restricting employee access to data associated with their specific region. BigQuery datasets can be organized to enforce granular access control.\nE. Adjust the settings for each dataset to allow a related region-based security group view access. : BigQuery IAM allows you to define access permissions at the dataset level. You can create security groups for each region and grant view access to the corresponding regional dataset. This ensures employees can only see data within their designated region.\nHere\u2018s why the other options are not recommended:\nA. Global Dataset: A single global dataset would not provide regional access control.\nC & D. Security Groups with Views: While security groups can be used for access control, using them with views wouldn\u2018t achieve the desired outcome in this scenario. Views are virtual tables based on existing tables, and access control is typically set at the dataset level in BigQuery."},{"label":"test_15","q_format":"single","q_text":"You are developing an application that uses a recommendation engine on Google Cloud. Your solution should display new videos to customers based on past views. Your solution needs to generate labels for the entities in videos that the customer has viewed. Your design must be able to provide very fast filtering suggestions based on data from other customer preferences on several TB of data. What should you do?","answers":[{"ans":"A. Build and train a complex classification model with Spark MLlib to generate labels and filter the results. Deploy the models using Cloud Dataproc. Call the model from your application.","val":false},{"ans":"B. Build and train a classification model with Spark MLlib to generate labels. Build and train a second classification model with Spark MLlib to filter results to match customer preferences. Deploy the models using Cloud Dataproc. Call the models from your application.","val":false},{"ans":"C. Build an application that calls the Cloud Video Intelligence API to generate labels. Store data in Cloud Bigtable, and filter the predicted labels to match the user\u2018s viewing history to generate preferences.","val":true},{"ans":"D. Build an application that calls the Cloud Video Intelligence API to generate labels. Store data in Cloud SQL, and join and filter the predicted labels to match the user\u2018s viewing history to generate preferences.","val":false}],"q_expl":"Reference https:\/\/cloud.google.com\/video-intelligence\/docs\/feature-label-detection"},{"label":"test_15","q_format":"single","q_text":"Which of these is not a supported method of putting data into a partitioned table?","answers":[{"ans":"A. If you have existing data in a separate file for each day, then create a partitioned table and upload each file into the appropriate partition.","val":false},{"ans":"B. Run a query to get the records for a specific day from an existing table and for the destination table, specify a partitioned table ending with the day in the format \u201c$YYYYMMDD\u201c.","val":false},{"ans":"C. Create a partitioned table and stream new records to it every day.","val":false},{"ans":"D. Use ORDER BY to put a table\u2018s rows into chronological order and then change the table\u2018s type to \u201cPartitioned\u201c.","val":true}],"q_expl":"You cannot change an existing table into a partitioned table. You must create a partitioned table from scratch. Then you can either stream data into it every day and the data will automatically be put in the right partition, or you can load data into a specific partition by using \u201c$YYYYMMDD\u201c at the end of the table name. Reference: https:\/\/cloud.google.com\/bigquery\/docs\/partitioned-tables"},{"label":"test_15","q_format":"single","q_text":"You are designing storage for two relational tables that are part of a 10-TB database on Google Cloud. You want to support transactions that scale horizontally.You also want to optimize data for range queries on non-key columns. What should you do?","answers":[{"ans":"A. Use Cloud SQL for storage. Add secondary indexes to support query patterns.","val":false},{"ans":"B. Use Cloud SQL for storage. Use Cloud Dataflow to transform data to support query patterns.","val":false},{"ans":"C. Use Cloud Spanner for storage. Add secondary indexes to support query patterns.","val":true},{"ans":"D. Use Cloud Spanner for storage. Use Cloud Dataflow to transform data to support query patterns.","val":false}],"q_expl":"A is not correct because Cloud SQL does not natively scale horizontally. B is not correct because Cloud SQL does not natively scale horizontally. C is correct because Cloud Spanner scales horizontally, and you can create secondary indexes for the range queries that are required. D is not correct because Dataflow is a data pipelining tool to move and transform data, but the use case is centered around querying."},{"label":"test_15","q_format":"single","q_text":"You have Google Cloud Dataflow streaming pipeline running with a Google Cloud Pub\/Sub subscription as the source. You need to make an update to the code that will make the new Cloud Dataflow pipeline incompatible with the current version. You do not want to lose any data when making this update. What should you do?","answers":[{"ans":"A. Update the current pipeline and use the drain flag.","val":true},{"ans":"B. Update the current pipeline and provide the transform mapping JSON object.","val":false},{"ans":"C. Create a new pipeline that has the same Cloud Pub\/Sub subscription and cancel the old pipeline.","val":false},{"ans":"D. Create a new pipeline that has a new Cloud Pub\/Sub subscription and cancel the old pipeline.","val":false}],"q_expl":"Correct Option : A Explanation:-This option is correct as the key requirement is not to lose the data, the Dataflow pipeline can be stopped using the Drain option. Drain options would cause Dataflow to stop any new processing, but would also allow the existing processing to complete"},{"label":"test_15","q_format":"single","q_text":"If you want to create a machine learning model that predicts the price of a particular stock based on its recent price history, what type of estimator should you use?","answers":[{"ans":"A. Unsupervised learning","val":false},{"ans":"B. Regressor","val":true},{"ans":"C. Classifier","val":false},{"ans":"D. Clustering estimator","val":false}],"q_expl":"Regression is the supervised learning task for modeling and predicting continuous, numeric variables. Examples include predicting real-estate prices, stock price movements, or student test scores. Classification is the supervised learning task for modeling and predicting categorical variables. Examples include predicting employee churn, email spam, financial fraud, or student letter grades. Clustering is an unsupervised learning task for finding natural groupings of observations (i.e. clusters) based on the inherent structure within your dataset. Examples include customer segmentation, grouping similar items in e-commerce, and social network analysis. Reference: https:\/\/elitedatascience.com\/machine-learning-algorithms"},{"label":"test_15","q_format":"single","q_text":"Your company needs to upload their historic data to Cloud Storage. The security rules don\u2018t allow access from external IPs to their on-premises resources. After an initial upload, they will add new data from existing on-premises applications every day. What should they do?","answers":[{"ans":"A. Execute gsutil rsync from the on-premises servers.","val":true},{"ans":"B. Use Cloud Dataflow and write the data to Cloud Storage.","val":false},{"ans":"C. Write a job template in Cloud Dataproc to perform the data transfer.","val":false},{"ans":"D. Install an FTP server on a Compute Engine VM to receive the files and move them to Cloud Storage.","val":false}],"q_expl":"A is the better and most simple IF there is no problem in having gsutil in our servers. B and C no way, the comms will go GCP\u0096Home, which sais is not allowed. D is valid, we can send the files with http:\/\/ftp\u0085BUT ftp is not secure, and we\u0092ll need to move them to the cloud storage afterwards, which is not detailed in the answer. https:\/\/cloud.google.com\/storage\/docs\/gsutil\/commands\/rsync"},{"label":"test_15","q_format":"single","q_text":"When running a pipeline that has a BigQuery source, on your local machine, you continue to get permission denied errors. What could be the reason for that?","answers":[{"ans":"A. Your gcloud does not have access to the BigQuery resources","val":true},{"ans":"B. BigQuery cannot be accessed from local machines","val":false},{"ans":"C. You are missing gcloud on your machine","val":false},{"ans":"D. Pipelines cannot be run locally","val":false}],"q_expl":"When reading from a Dataflow source or writing to a Dataflow sink using DirectPipelineRunner, the Cloud Platform account that you configured with the gcloud executable will need access to the corresponding source\/sink Reference: https:\/\/cloud.google.com\/dataflow\/java-sdk\/JavaDoc\/com\/google\/cloud\/dataflow\/sdk\/runners\/DirectPipelineRunner"},{"label":"test_15","q_format":"single","q_text":"Flowlogistic Case Study -Company Overview -Flowlogistic is a leading logistics and supply chain provider. They help businesses throughout the world manage their resources and transport them to their final destination. The company has grown rapidly, expanding their offerings to include rail, truck, aircraft, and oceanic shipping.Company Background -The company started as a regional trucking company, and then expanded into other logistics market. Because they have not updated their infrastructure, managing and tracking orders and shipments has become a bottleneck. To improve operations, Flowlogistic developed proprietary technology for tracking shipments in real time at the parcel level. However, they are unable to deploy it because their technology stack, based on Apache Kafka, cannot support the processing volume. In addition, Flowlogistic wants to further analyze their orders and shipments to determine how best to deploy their resources.Solution Concept -Flowlogistic wants to implement two concepts using the cloud:? Use their proprietary technology in a real-time inventory-tracking system that indicates the location of their loads? Perform analytics on all their orders and shipment logs, which contain both structured and unstructured data, to determine how best to deploy resources, which markets to expand info. They also want to use predictive analytics to learn earlier when a shipment will be delayed.Existing Technical Environment -Flowlogistic architecture resides in a single data center:? Databases8 physical servers in 2 clusters- SQL Server \u201c\u201c user data, inventory, static data3 physical servers- Cassandra \u201c\u201c metadata, tracking messages10 Kafka servers \u201c\u201c tracking message aggregation and batch insert? Application servers \u201c\u201c customer front end, middleware for order\/customs60 virtual machines across 20 physical servers- Tomcat \u201c\u201c Java services- Nginx \u201c\u201c static content- Batch servers? Storage appliances- iSCSI for virtual machine (VM) hosts- Fibre Channel storage area network (FC SAN) \u201c\u201c SQL server storage- Network-attached storage (NAS) image storage, logs, backups? 10 Apache Hadoop \/Spark servers- Core Data Lake- Data analysis workloads? 20 miscellaneous servers- Jenkins, monitoring, bastion hosts,Business Requirements -? Build a reliable and reproducible environment with scaled panty of production.? Aggregate data in a centralized Data Lake for analysis? Use historical data to perform predictive analytics on future shipments? Accurately track every shipment worldwide using proprietary technology? Improve business agility and speed of innovation through rapid provisioning of new resources? Analyze and optimize architecture for performance in the cloud? Migrate fully to the cloud if all other requirements are metTechnical Requirements -? Handle both streaming and batch data? Migrate existing Hadoop workloads? Ensure architecture is scalable and elastic to meet the changing demands of the company.? Use managed services whenever possible? Encrypt data flight and at rest? Connect a VPN between the production data center and cloud environmentSEO Statement -We have grown so quickly that our inability to upgrade our infrastructure is really hampering further growth and efficiency. We are efficient at moving shipments around the world, but we are inefficient at moving data around.We need to organize our information so we can more easily understand where our customers are and what they are shipping.CTO Statement -IT has never been a priority for us, so as our data has grown, we have not invested enough in our technology. I have a good staff to manage IT, but they are so busy managing our infrastructure that I cannot get them to do the things that really matter, such as organizing our data, building the analytics, and figuring out how to implement the CFO\u2018 s tracking technology.CFO Statement -Part of our competitive advantage is that we penalize ourselves for late shipments and deliveries. Knowing where out shipments are at all times has a direct correlation to our bottom line and profitability. Additionally, I don\u2018t want to commit capital to building out a server environment.Flowlogistic\u2018s CEO wants to gain rapid insight into their customer base so his sales team can be better informed in the field. This team is not very technical, so they\u2018ve purchased a visualization tool to simplify the creation of BigQuery reports. However, they\u2018ve been overwhelmed by all the data in the table, and are spending a lot of money on queries trying to find the data they need. You want to solve their problem in the most cost-effective way. What should you do?","answers":[{"ans":"A. Export the data into a Google Sheet for virtualization.","val":false},{"ans":"B. Create an additional table with only the necessary columns.","val":false},{"ans":"C. Create a view on the table to present to the virtualization tool.","val":true},{"ans":"D. Create identity and access management (IAM) roles on the appropriate columns, so only they appear in a query.","val":false}],"q_expl":"Answer is C. A logical view can be created with only the required columns which is required for visualization. B is not the right option as you will create a table and make it static. What happens when the original data is updated. This new table will not have the latest data and hence view is the best possible option here."},{"label":"test_15","q_format":"single","q_text":"You are operating a Cloud Dataflow streaming pipeline. The pipeline aggregates events from a Cloud Pub\/Sub subscription source, within a window, and sinks the resulting aggregation to a Cloud Storage bucket. The source has consistent throughput. You want to monitor an alert on behavior of the pipeline with CloudStackdriver to ensure that it is processing data. Which Stackdriver alerts should you create?","answers":[{"ans":"A. An alert based on a decrease of subscription\/num_undelivered_messages for the source and a rate of change increase of instance\/storage\/ used_bytes for the destination","val":false},{"ans":"B. An alert based on an increase of subscription\/num_undelivered_messages for the source and a rate of change decrease of instance\/storage\/ used_bytes for the destination","val":true},{"ans":"C. An alert based on a decrease of instance\/storage\/used_bytes for the source and a rate of change increase of subscription\/ num_undelivered_messages for the destination","val":false},{"ans":"D. An alert based on an increase of instance\/storage\/used_bytes for the source and a rate of change decrease of subscription\/ num_undelivered_messages for the destination","val":false}],"q_expl":"The answer is B. subscription\/num_undelivered_messages: the number of messages that subscribers haven\u2018t processed https:\/\/cloud.google.com\/pubsub\/docs\/monitoring#monitoring_forwarded_undeliverable_messages"},{"label":"test_15","q_format":"single","q_text":"The _________ for Cloud Bigtable makes it possible to use Cloud Bigtable in a Cloud Dataflow pipeline.","answers":[{"ans":"A. Cloud Dataflow connector","val":true},{"ans":"B. DataFlow SDK","val":false},{"ans":"C. BiqQuery API","val":false},{"ans":"D. BigQuery Data Transfer Service","val":false}],"q_expl":"The Cloud Dataflow connector for Cloud Bigtable makes it possible to use Cloud Bigtable in a Cloud Dataflow pipeline. You can use the connector for both batch and streaming operations. Reference: https:\/\/cloud.google.com\/bigtable\/docs\/dataflow-hbase"},{"label":"test_15","q_format":"single","q_text":"You are deploying 10,000 new Internet of Things devices to collect temperature data in your warehouses globally. You need to process, store and analyze these very large datasets in real time. What should you do?","answers":[{"ans":"A. Send the data to Google Cloud Datastore and then export to BigQuery.","val":false},{"ans":"B. Send the data to Google Cloud Pub\/Sub, stream Cloud Pub\/Sub to Google Cloud Dataflow, and store the data in Google BigQuery.","val":true},{"ans":"C. Send the data to Cloud Storage and then spin up an Apache Hadoop cluster as needed in Google Cloud Dataproc whenever analysis is required.","val":false},{"ans":"D. Export logs in batch to Google Cloud Storage and then spin up a Google Cloud SQL instance, import the data from Cloud Storage, and run an analysis as needed.","val":false}],"q_expl":"B is more correct than other options so B is the answer. But if this is actual use case you have to deal with use Cloud BigTable instead of bigquery. So the pipeline will be like this. IOT-Devices -> Cloud Pub\/Sub -> Cloud BigTable -> Cloud Data Studio (For real-time analytics)"},{"label":"test_15","q_format":"single","q_text":"What are two methods that can be used to denormalize tables in BigQuery?","answers":[{"ans":"A. 1) Split table into multiple tables; 2) Use a partitioned table","val":false},{"ans":"B. 1) Join tables into one table; 2) Use nested repeated fields","val":true},{"ans":"C. 1) Use a partitioned table; 2) Join tables into one table","val":false},{"ans":"D. 1) Use nested repeated fields; 2) Use a partitioned table","val":false}],"q_expl":"The conventional method of denormalizing data involves simply writing a fact, along with all its dimensions, into a flat table structure. For example, if you are dealing with sales transactions, you would write each individual fact to a record, along with the accompanying dimensions such as order and customer information. The other method for denormalizing data takes advantage of BigQuerys native support for nested and repeated structures in JSON or Avro input data. Expressing records using nested and repeated structures can provide a more natural representation of the underlying data. In the case of the sales order, the outer part of a JSON structure would contain the order and customer information, and the inner part of the structure would contain the individual line items of the order, which would be represented as nested, repeated elements. Reference: https:\/\/cloud.google.com\/solutions\/bigquery-data-warehouse#denormalizing_data"},{"label":"test_15","q_format":"single","q_text":"You have historical data covering the last three years in BigQuery and a data pipeline that delivers new data to BigQuery daily. You have noticed that when theData Science team runs a query filtered on a date column and limited to 30\u201c\u201c90 days of data, the query scans the entire table. You also noticed that your bill is increasing more quickly than you expected. You want to resolve the issue as cost-effectively as possible while maintaining the ability to conduct SQL queries.What should you do?","answers":[{"ans":"A. Re-create the tables using DDL. Partition the tables by a column containing a TIMESTAMP or DATE Type.","val":true},{"ans":"B. Recommend that the Data Science team export the table to a CSV file on Cloud Storage and use Cloud Datalab to explore the data by reading the files directly.","val":false},{"ans":"C. Modify your pipeline to maintain the last 30\u201c\u201c90 days of data in one table and the longer history in a different table to minimize full table scans over the entire history.","val":false},{"ans":"D. Write an Apache Beam pipeline that creates a BigQuery table per day. Recommend that the Data Science team use wildcards on the table name suffixes to select the data they need.","val":false}],"q_expl":"I will go with Option A, although at first instance I felt Option C would be correct. Option A : Because partitioning will help to address both the concerns mentioned in the question \u2013 i.e. faster query and reducing cost. Option C : Modifying the data pipeline to store last 30-90 days data would have possible, if there was a point mentioned that only the latest data (30-90 days) is kept and the older data \u2013 beyond 90 days is moved to the master table. Since that point is mot mentioned, we will land up having multiple \u2013 30-90 days data in separate tables + the master table."},{"label":"test_15","q_format":"single","q_text":"What is the recommended way to replicate web application log data from an on-prem Apache Kafka cluster to Google Cloud for analysis in BigQuery and Cloud Storage, without deploying Kafka Connect plugins?","answers":[{"ans":"Set up a Kafka cluster on GCE VM instances and configure the on-prem cluster to mirror the topics to the GCE cluster. Then use either a Dataproc cluster or Dataflow job to read from Kafka and write to GCS.","val":true},{"ans":"Install the Pub\/Sub Kafka connector on the on-prem Kafka cluster and configure Pub\/Sub as a Source connector. Use a Dataflow job to read from Pub\/Sub and write to GCS.","val":false},{"ans":"Create a Kafka cluster on GCE VM instances with the Pub\/Sub Kafka connector configured as a Sink connector. After that, use either a Dataproc cluster or Dataflow job to read from Kafka and write to GCS.","val":false},{"ans":"Install the Pub\/Sub Kafka connector on the on-prem Kafka cluster and configure Pub\/Sub as a Sink connector. Use a Dataflow job to read from Pub\/Sub and write to GCS","val":false}],"q_expl":"The recommended way to replicate web application log data from an on-prem Apache Kafka cluster to Google Cloud for analysis in BigQuery and Cloud Storage, without deploying Kafka Connect plugins, is to deploy a Kafka cluster on GCE VM instances and configure the on-prem cluster to mirror the topics to the GCE cluster. Then use either a Dataproc cluster or Dataflow job to read from Kafka and write to GCS."},{"label":"test_15","q_format":"single","q_text":"You need to automate the execution of a complex data pipeline on Google Cloud that involves multiple Dataproc and Dataflow jobs with interdependencies. The pipeline should run daily, and you prefer to use managed services wherever possible. Which tool should you use to automate the pipeline?","answers":[{"ans":"Cloud Composer","val":true},{"ans":"Cloud Scheduler","val":false},{"ans":"Cloud Functions","val":false},{"ans":"Dataproc Workflow Templates","val":false}],"q_expl":"Cloud Composer is a fully managed workflow orchestration service that allows you to author, schedule, and monitor complex workflows on Google Cloud. It provides a managed Apache Airflow service that makes it easy to create and manage DAGs (Directed Acyclic Graphs) for your pipeline. With Cloud Composer, you can easily define and schedule your pipeline to run on a daily basis, while also leveraging managed services such as Dataproc and Dataflow for processing. It also provides advanced features such as built-in monitoring, alerting, and logging to help you debug and optimize your pipeline."},{"label":"test_15","q_format":"single","q_text":"You\u2018re working on a deep learning model to predict customer buying behavior on your ecommerce site. Your evaluation shows that the model is overfitting, and you want to improve its accuracy with new data. Which of the following actions should you take?","answers":[{"ans":"Increase the size of the training dataset and remove some input features","val":true},{"ans":"Increase the size of the training dataset and add more input features","val":false},{"ans":"Decrease the size of the training dataset and add more input features","val":false},{"ans":"Decrease the size of the training dataset and remove some input features.","val":false}],"q_expl":"Ref: https:\/\/machinelearningmastery.com\/impact-of-dataset-size-on-deep-learning-model-skill-and-performance-estimates\/"},{"label":"test_15","q_format":"single","q_text":"Imagine you have a requirement to insert data with minute-level resolution from 100,000 different sensors into a BigQuery table. In order to meet real-time analysis needs for aggregated trends, you need the data to be available within 1 minute of ingestion. With the expectation of significant growth in data volume, which of the following options should you choose?","answers":[{"ans":"Insert batches of data using the INSERT statement every 60 seconds.","val":false},{"ans":"Stream sensor data into the BigQuery table using a Cloud Dataflow pipeline","val":true},{"ans":"Apply updates to existing data with the MERGE statement every 60 seconds.","val":false},{"ans":"Load a batch of sensor data using bq load every 60 seconds","val":false}],"q_expl":"B is the optimal choice in this scenario. Using a Cloud Dataflow pipeline to stream the sensor data into the BigQuery table would allow for real-time analysis of aggregated trends within 1 minute of data ingestion, as required in the question. In addition, using a pipeline can accommodate the expected significant growth in data volume. Option A, which involves loading batches of sensor data every 60 seconds using the bq load command, is a slower method compared to streaming and does not offer real-time analysis of trends. Option C, which involves inserting batches of data using the INSERT statement every 60 seconds, may lead to performance issues when dealing with large data volumes. Option D is useful for updating existing data, but not for inserting new data"},{"label":"test_15","q_format":"single","q_text":"A data scientist has created a BigQuery ML model and wants you to build an ML pipeline to serve predictions to a REST API application. The API needs to serve predictions for individual user IDs with a latency of under 100 milliseconds. The following query generates predictions: SELECT predicted_label, user_id FROM ML.PREDICT (MODEL \u2018dataset.model\u2018, table user_features). Which approach should you take to build the ML pipeline?","answers":[{"ans":"Add a WHERE clause to the query, and grant the BigQuery Data Viewer role to the application service account.","val":true},{"ans":"Create a Dataflow pipeline using BigQueryIO to read results from the query. Grant the Dataflow Worker role to the application service account.","val":false},{"ans":"Create a Dataflow pipeline using BigQueryIO to read predictions for all users from the query. Write the results to Bigtable using BigtableIO. Grant the Bigtable Reader role to the application service account so that the application can read predictions for individual users from Bigtable","val":false},{"ans":"Create an Authorized View with the provided query. Share the dataset that contains the view with the application service account.","val":false}],"q_expl":"The correct approach is to add a WHERE clause to the query to filter results by the user ID and grant the BigQuery Data Viewer role to the application service account. This approach ensures that the REST API application receives only the required predictions and meets the latency requirement. The other options are invalid because: B. Creating an authorized view with the provided query and sharing the dataset containing the view with the application service account is not an optimal solution because the latency may exceed 100 milliseconds due to the overhead of accessing the view. C. Creating a Dataflow pipeline using BigQueryIO to read results from the query and granting the Dataflow Worker role to the application service account is not necessary since there is no need to manipulate or transform the query results. D. Creating a Dataflow pipeline using BigQueryIO to read predictions for all users from the query and writing the results to Bigtable using BigtableIO and granting the Bigtable Reader role to the application service account is not necessary since it is not required to store the predictions for individual users in Bigtable, and this approach may add unnecessary latency."},{"label":"test_15","q_format":"single","q_text":"How can you address a disk I\/O intensive Hadoop job running slowly with Cloud Dataproc in a managed Hadoop system that uses Cloud Storage connector for storing input, output, and intermediary data?","answers":[{"ans":"Increase the CPU cores of the virtual machine instances of the Hadoop cluster to scale up networking bandwidth for each instance","val":false},{"ans":"Increase the memory allocation of the Hadoop cluster to hold the intermediary data of the slow Hadoop job in memory","val":false},{"ans":"Allocate additional network interface cards (NICs), and set up link aggregation in the operating system to use the combined throughput while working with Cloud Storage.","val":false},{"ans":"Allocate enough persistent disk space to the Hadoop cluster and store the intermediate data of the slow Hadoop job on native Hadoop Distributed File System (HDFS)","val":true}],"q_expl":"Allocate sufficient persistent disk space to the Hadoop cluster, and store the intermediate data of that particular Hadoop job on native HDFS: This option could work if the issue is related to the Cloud Storage connector being slow or not optimized for the specific workload. By storing the intermediate data on native HDFS, the job may run faster as the disk I\/O is reduced. Ref: https:\/\/cloud.google.com\/compute\/docs\/disks\/performance#optimize_disk_performance. https:\/\/cloud.google.com\/solutions\/migration\/hadoop\/hadoop-gcp-migration-jobs"},{"label":"test_15","q_format":"single","q_text":"You have a TensorFlow machine learning model that is trained on Compute Engine virtual machines (n2-standard-32), which takes two days to complete. The model requires custom TensorFlow operations that must run partially on a CPU. What can you do to reduce the training time in a cost-effective manner?","answers":[{"ans":"Upgrade the VM type to e2-standard-32","val":false},{"ans":"Use a VM with a GPU hardware accelerator to train the model.","val":true},{"ans":"Use a VM with a TPU hardware accelerator to train the model.","val":false},{"ans":"Upgrade the VM type to n2-highmem-32","val":false}],"q_expl":"Given that the TensorFlow model has custom TensorFlow operations that must run partially on a CPU, the best option to reduce the training time in a cost-effective manner would be to train the model using a VM with a GPU hardware accelerator. Therefore, option C is the correct answer. GPU accelerators can significantly speed up TensorFlow training on virtual machines, especially when there are a lot of computations to be performed. The n2-highmem-32 and e2-standard-32 virtual machine types may have more memory and CPU resources, respectively, but they do not have the specialized hardware required to accelerate TensorFlow training like GPUs and TPUs. TPUs are specialized hardware accelerators designed for TensorFlow, but they may not be cost-effective for this scenario, as they can be more expensive than GPU-based VMs. Therefore, training the TensorFlow model using a VM with a GPU hardware accelerator is the most cost-effective option to reduce the training time. It is worth noting that selecting the appropriate GPU type based on the workload is also important to achieve the best performance and cost-effectiveness."},{"label":"test_15","q_format":"single","q_text":"You are tasked with storing and managing transaction data for a large bank with operations across North America. You need a data storage solution that can ensure ACID compliance and provide SQL access to the data. Which of the following options is appropriate?","answers":[{"ans":"Use Cloud Spanner to store transaction data and utilize locking read-write transactions","val":true},{"ans":"Use Cloud Spanner to store transaction data and enable stale reads to reduce latency","val":false},{"ans":"Use Cloud SQL to store transaction data and use federated queries in BigQuery for analysis","val":false},{"ans":"Store transaction data in BigQuery and disable the query cache to ensure consistency","val":false}],"q_expl":"The appropriate solution for this use case is to use Cloud Spanner to store transaction data and utilize locking read-write transactions. Cloud Spanner is a fully managed relational database that offers the scalability and performance benefits of NoSQL databases while also providing ACID compliance and SQL access to the data. Additionally, locking read-write transactions ensure consistency and protect against data corruption during write operations."},{"label":"test_15","q_format":"single","q_text":"You have started a batch job in Dataflow, and it has processed a few elements before suddenly failing and shutting down. You investigate the Dataflow monitoring interface and find errors related to a specific DoFn in your pipeline. Which of the following is the most likely cause of these errors?","answers":[{"ans":"Errors in the worker code","val":true},{"ans":"Validation of the job","val":false},{"ans":"Insufficient permissions","val":false},{"ans":"Construction of the pipeline or graph","val":false}],"q_expl":"The most likely cause of errors related to a particular DoFn in a failed Dataflow batch job after processing a few elements is B: Errors\/Exceptions in worker code. This means that there is an issue with the code written in the DoFn that is causing it to throw an exception and fail. It could be due to incorrect input\/output types, null pointer exceptions, or other coding errors. The error messages in the Dataflow monitoring interface should provide more information about the specific cause of the error, which can help in debugging the issue"},{"label":"test_15","q_format":"single","q_text":"You have a Dataprep recipe that was created on a sample of data in a BigQuery table. How can you reuse this recipe on a daily basis for new data with the same schema, after a variable load job has completed?","answers":[{"ans":"Export the recipe as a Dataprep template, and schedule a job using Cloud Scheduler","val":false},{"ans":"Export the Dataprep job as a Dataflow template, and incorporate it into a Cloud Composer job.","val":true},{"ans":"Use an App Engine cron job to schedule the execution of the Dataprep job","val":false},{"ans":"Schedule a Dataprep job using a cron schedule","val":false}],"q_expl":"Exporting the Dataprep job as a Dataflow template allows the recipe to be integrated into a Dataflow pipeline that can be scheduled and executed with Cloud Composer. Cloud Composer is a fully managed workflow orchestration service that provides advanced features for scheduling, monitoring, and retrying data pipelines. Using this option provides greater flexibility and control over scheduling than using a simple cron schedule or an App Engine cron job. Exporting the recipe as a Dataprep template and scheduling a job with Cloud Scheduler (option C) is also a valid option, but using Dataflow with Cloud Composer provides more advanced features for managing and monitoring the pipeline."},{"label":"test_15","q_format":"single","q_text":"Can you recommend tools for migrating a data warehouse to Google Cloud and ensuring real-time updates to the warehouse from transactional systems. The files being transferred are not large in number but are each 90 GB in size?","answers":[{"ans":"Use BigQuery Data Transfer Service for the migration; Pub\/Sub and Dataproc for real-time updates","val":false},{"ans":"Use gsutil for the migration; Pub\/Sub and Dataflow for real-time updates","val":true},{"ans":"Use Storage Transfer Service for the migration; Pub\/Sub and Cloud Data Fusion for real-time updates","val":false},{"ans":"Use gsutil for both the migration and real-time updates","val":false}],"q_expl":"The gsutil tool is the standard tool for small- to medium-sized transfers (less than 1 TB) over a typical enterprise-scale network, from a private data center to Google Cloud. Use Gsutil when there is enough bandwidth to meet your project deadline for less than 1 TB of data. Storage Transfer Service is for much larger volumes for migration. Moreover, Cloud Data Fusion and Dataproc are not ideal for real-time updates. BigQuery Data Transfer Service does not support all on-prem sources. https:\/\/cloud.google.com\/architecture\/migration-to-google-cloud-transferring-your-large-datasets#gsutil_for_smaller_transfers_of_on-premises_data"},{"label":"test_15","q_format":"single","q_text":"What is the most fault-tolerant and cost-effective managed service for migrating an on-premises Apache Hadoop deployment to the cloud for long-running batch jobs?","answers":[{"ans":"Install Hadoop and Spark on a 10-node Compute Engine instance group with standard instances. Install the Cloud Storage connector, and store the data in Cloud Storage. Change references in scripts from hdfs:\/\/ to gs:\/\/","val":false},{"ans":"Deploy a Dataproc cluster. Use an SSD persistent disk and 50% preemptible workers. Store data in Cloud Storage, and change references in scripts from hdfs:\/\/ to gs:\/\/","val":false},{"ans":"Install Hadoop and Spark on a 10-node Compute Engine instance group with preemptible instances. Store data in HDFS. Change references in scripts from hdfs:\/\/ to gs:\/\/","val":false},{"ans":"Deploy a Dataproc cluster. Use a standard persistent disk and 50% preemptible workers. Store data in Cloud Storage, and change references in scripts from hdfs:\/\/ to gs:\/\/","val":true}],"q_expl":"Dataproc is a managed Spark and Hadoop service that lets you take advantage of open source data tools for batch processing, querying, streaming, and machine learning. Dataproc automation helps you create clusters quickly, manage them easily, and save money by turning clusters off when you don\u2018t need them. With less time and money spent on administration, you can focus on your jobs and your data"},{"label":"test_16","q_format":"single","q_text":"As an employee of a shipping company, you need to build a scalable solution using cloud-native managed services to prevent the transmission of personally identifiable information (PII) from handheld scanners to analytics systems. What should you do?","answers":[{"ans":"Install a third-party data validation tool on Compute Engine virtual machines to check the incoming data for sensitive information.","val":false},{"ans":"Build a Cloud Function that reads the topics and makes a call to the Cloud Data Loss Prevention (Cloud DLP) API. Use the tagging and confidence levels to either pass or quarantine the data in a bucket for review.","val":true},{"ans":"Use Cloud Logging to analyze the data passed through the entire pipeline to identify transactions that may contain sensitive information.","val":false},{"ans":"Create an authorized view in BigQuery to restrict access to tables with sensitive data.","val":false}],"q_expl":"The best option is D. Building a Cloud Function that reads the topics and makes a call to the Cloud Data Loss Prevention (Cloud DLP) API is the most efficient way to prevent the transmission of PII. The function can analyze the data and use the tagging and confidence levels to either pass or quarantine the data in a bucket for review. This approach is scalable, cloud-native, and utilizes managed services provided by Google Cloud. Option A is not the best choice as it only restricts access to tables with sensitive data and does not prevent the data from being transmitted. Option B is not the best choice as it involves installing a third-party tool, which may be time-consuming and may not integrate well with other cloud services. Option C is not the best choice as it analyzes the data after it has been passed through the entire pipeline, and does not prevent the transmission of PII"},{"label":"test_16","q_format":"single","q_text":"What is the most suitable GCP database to move a 20 TB on-premises operational system transaction data to GCP?","answers":[{"ans":"Cloud Bigtable","val":false},{"ans":"Cloud SQL","val":true},{"ans":"Cloud Spanner","val":false},{"ans":"Cloud Datastore","val":false}],"q_expl":"Ref: https:\/\/cloud.google.com\/sql\/docs\/quotas#:~:text=Cloud%20SQL%20storage%20limits&text=Up%20to%2030%2C720%20GB%2C%20depending,for%20PostgreSQL%20or%20SQL%20Server"},{"label":"test_16","q_format":"single","q_text":"You have a large amount of historical data in BigQuery and a daily data pipeline that continuously adds new data to the table. The Data Science team runs SQL queries on this data, but they are noticing that queries filtered on a date column and limited to 30-90 days of data are scanning the entire table. Additionally, you have observed that your billing is increasing more quickly than expected. What is the most cost-effective solution to resolve this issue while maintaining the ability to conduct SQL queries?","answers":[{"ans":"Create two separate tables, one for the last 30-90 days of data and another for the longer history, to minimize full table scans over the entire history.","val":false},{"ans":"Use DDL to recreate the tables and partition them by a column containing a TIMESTAMP or DATE type","val":true},{"ans":"Recommend that the Data Science team export the table to a CSV file on Cloud Storage and use Cloud Datalab to explore the data by reading the files directly.","val":false},{"ans":"Write an Apache Beam pipeline that creates a new BigQuery table every day and advise the Data Science team to use wildcards on the table name suffixes to select the data they need.","val":false}],"q_expl":"The correct answer is option A: Create new tables using DDL and partition them by a column containing a TIMESTAMP or DATE type. Partitioning the tables in BigQuery is an efficient way to handle large amounts of data by organizing them into smaller, more manageable sections based on a specific column such as a date or timestamp. Partitioning allows queries to only scan the relevant partitions rather than the entire table, reducing costs and improving query performance. Option B (exporting the table to a CSV file in Cloud Storage and using Cloud Datalab) is not ideal as it adds additional steps and complexity to the process, and may not be as efficient as partitioning the tables. Option C (splitting the data into two separate tables) can also work, but it can be more difficult to manage and may not be as efficient as partitioning. Option D (creating a new table every day) can result in a large number of tables, which can be difficult to manage and may not be efficient in terms of cost or query performance"},{"label":"test_16","q_format":"single","q_text":"You have a requirement to create a near real-time inventory dashboard that reads the main inventory tables in your BigQuery data warehouse. The historical inventory data is stored as inventory balances by item and location, and you have several thousand updates to inventory every hour. Your goal is to ensure that the data is accurate and the dashboard has maximum performance. Which of the following options should you choose?","answers":[{"ans":"Use BigQuery UPDATE statements to update the inventory balances as they are changing.","val":false},{"ans":"Use the BigQuery bulk loader to batch load inventory changes into a daily inventory movement table. Calculate balances in a view that joins it to the historical inventory balance table. Update the inventory balance table nightly.","val":false},{"ans":"Use the BigQuery streaming to stream changes into a daily inventory movement table. Calculate balances in a view that joins it to the historical inventory balance table. Update the inventory balance table nightly.","val":true},{"ans":"Partition the inventory balance table by item to reduce the amount of data scanned with each inventory update.","val":false}],"q_expl":"The best option to create a near real-time inventory dashboard with maximum performance and accurate data is option C: Use the BigQuery streaming to stream changes into a daily inventory movement table. Calculate balances in a view that joins it to the historical inventory balance table. Update the inventory balance table nightly. Option A, which suggests using BigQuery UPDATE statements to update the inventory balances as they are changing, is not recommended as it may lead to poor query performance, particularly when several thousand updates to inventory occur every hour. Option B, which suggests partitioning the inventory balance table by item to reduce the amount of data scanned with each inventory update, may improve performance for some queries, but it does not address the need for near real-time updates and accurate data. Option D suggests using the BigQuery bulk loader to batch load inventory changes into a daily inventory movement table, calculating balances in a view that joins it to the historical inventory balance table, and updating the inventory balance table nightly. This option may not provide up-to-date information for real-time analysis since it involves batch loading and nightly updates. Therefore, option C is the best solution as it streams changes into a daily inventory movement table, calculates balances in a view that joins it to the historical inventory balance table, and updates the inventory balance table nightly. This approach ensures that the dashboard has access to the most recent and accurate data while maintaining query performance."},{"label":"test_16","q_format":"single","q_text":"How can you compare the output of the original ETL job with the output of the migrated job after migrating them to run on BigQuery, given that the tables do not have a primary key column to join them together for comparison, but you have loaded a table containing the output of the original job?","answers":[{"ans":"Select random samples from the tables using the HASH() function and compare the samples","val":false},{"ans":"Select random samples from the tables using the RAND() function and compare the samples.","val":false},{"ans":"Use a Dataproc cluster and the BigQuery Hadoop connector to read the data from each table and calculate a hash from non-timestamp columns of the table after sorting. Compare the hashes of each table.","val":true},{"ans":"Create stratified random samples using the OVER() function and compare equivalent samples from each table","val":false}],"q_expl":"Option C is the best option because it uses a Dataproc cluster and the BigQuery Hadoop connector to read the data from each table and calculate a hash from non-timestamp columns of the table after sorting. Comparing the hashes of each table ensures that the contents are identical, even if the tables do not contain a primary key column Option A is not a reliable way to compare the contents of the two tables because it only checks a small subset of the data. Option B is not a reliable way to compare the contents of the two tables because the same hash value can be generated for different data, leading to false positives. Option D suggests creating stratified random samples using the OVER() function, but it is not clear how these samples would be used to compare the two tables. Additionally, this method may not provide a representative sample of the data, leading to incorrect comparisons."},{"label":"test_16","q_format":"single","q_text":"How should you design data storage for a cloud-native historical data processing system that must be accessible by multiple analysis tools, including Dataproc, BigQuery, and Compute Engine, for data in CSV, Avro, and PDF formats. The solution must maximize availability, but performance is not a factor, and a batch pipeline moves daily data. Which option should you choose for data storage?","answers":[{"ans":"Store the data in a multi-regional Cloud Storage bucket and directly access it using Dataproc, BigQuery, and Compute Engine","val":true},{"ans":"Store the data in BigQuery and use the BigQuery Connector on Dataproc and Compute Engine to access the data","val":false},{"ans":"Create a high-availability Dataproc cluster, store the data in HDFS, and perform analysis as needed","val":false},{"ans":"Store the data in a regional Cloud Storage bucket and directly access it using Dataproc, BigQuery, and Compute Engine","val":false}],"q_expl":"Store the data in a multi-regional Cloud Storage bucket and access it directly using Dataproc, BigQuery, and Compute Engine. This option is the best choice because it maximizes availability and can be accessed directly by all the required analysis tools. Storing the data in a multi-regional Cloud Storage bucket ensures high availability and provides data redundancy across multiple regions. Option A, which involves creating a Dataproc cluster with high availability and storing the data in HDFS, is not the best choice as it is expensive and more complicated to set up compared to storing the data in a multi-regional Cloud Storage bucket. Option B, storing the data in BigQuery and accessing it using the BigQuery Connector on Dataproc and Compute Engine, is not the best choice as it may not be suitable for processing unstructured data in PDF format. Option C, storing the data in a regional Cloud Storage bucket and accessing it directly using Dataproc, BigQuery, and Compute Engine, is also not the best choice as it does not provide data redundancy across multiple regions, which could affect availability in case of a regional outage"},{"label":"test_16","q_format":"single","q_text":"How should you structure the data when migrating an application that tracks library books and information about each book, such as author or year published, from an on-premises data warehouse to BigQuery, to ensure optimal speed of queries about the author of each book that has been borrowed, while following Google\u2018s recommended practice for schema design?","answers":[{"ans":"Create a table that includes information about the books and authors, but nest the author fields inside the author column","val":true},{"ans":"Create a table that is wide and includes a column for each attribute, including the author\u2018s first name, last name, date of birth, etc","val":false},{"ans":"Keep the schema the same, maintain the different tables for the book and each of the attributes, and query as you are doing today","val":false},{"ans":"Keep the schema the same, create a view that joins all of the tables, and always query the view.","val":false}],"q_expl":"Best practice: Use nested and repeated fields to denormalize data storage and increase query performance. Denormalization is a common strategy for increasing read performance for relational datasets that were previously normalized. The recommended way to denormalize data in BigQuery is to use nested and repeated fields. It\u2018s best to use this strategy when the relationships are hierarchical and frequently queried together, such as in parent-child relationships. Ref: https:\/\/cloud.google.com\/bigquery\/docs\/best-practices-performance-nested"},{"label":"test_16","q_format":"single","q_text":"In a report-only data warehouse using BigQuery\u2018s streaming API, what is the best way to design data loading with both staging and production tables to ensure only one master dataset and no performance impact on ingestion or reporting?","answers":[{"ans":"Use an append-only staging table and update the production table every ninety minutes with the changes","val":false},{"ans":"Move staged data to the production table and delete the staging table contents every three hours","val":true},{"ans":"Move staged data to the production table and delete the staging table contents every thirty minutes.","val":false},{"ans":"Use an append-only staging table and update the production table every three hours with the changes","val":false}],"q_expl":"The best approach is to move the staged data over to the production table and delete the staging table contents every three hours (Option C). Ref: https:\/\/cloud.google.com\/blog\/products\/data-analytics\/moving-a-publishing-workflow-to-bigquery-for-new-data-insights"},{"label":"test_16","q_format":"single","q_text":"You are updating the code for a subscriber to a Pub\/Sub feed and want to ensure that message loss does not occur due to erroneous message acknowledgments after deployment. Which of the following options should you choose?","answers":[{"ans":"Create a Pub\/Sub snapshot before deploying new subscriber code, and use a Seek operation to re-deliver messages that became available after the snapshot was created if errors occur","val":true},{"ans":"Use Cloud Build for your deployment, and locate a timestamp logged by Cloud Build at the start of the deployment if errors occur","val":false},{"ans":"Test the new subscriber logic on the Pub\/Sub emulator on your local machine before deploying it to production.","val":false},{"ans":"Enable dead-lettering on the Pub\/Sub topic to capture unacknowledged messages, and re-deliver any messages captured by the dead-letter queue if errors occur","val":false}],"q_expl":"Create a Pub\/Sub snapshot before deploying new subscriber code. Use a Seek operation to re-deliver messages that became available after the snapshot was created. According to the second reference in the list below (Refer second link below ) a concern with deploying new subscriber code is that the new executable may erroneously acknowledge messages, leading to message loss. Incorporating snapshots into your deployment process gives you a way to recover from bugs in new subscriber code. Answer cannot be C because To seek to a timestamp, you must first configure the subscription to retain acknowledged messages using retain-acked-messages. If retain-acked-messages is set, Pub\/Sub retains acknowledged messages for 7 days References: https:\/\/cloud.google.com\/pubsub\/docs\/replay-message https:\/\/cloud.google.com\/pubsub\/docs\/replay-overview#seek_use_cases"},{"label":"test_16","q_format":"single","q_text":"You need to quickly and efficiently generate daily reports that show a customer\u2018s net consumption of Google Cloud compute resources and who used the resources. What should you do?","answers":[{"ans":"Filter data in Cloud Logging by project, log type, resource, and user daily, then import the data into BigQuery","val":false},{"ans":"Export Cloud Logging data to Cloud Storage in CSV format daily, cleanse the data using Dataprep, and filter by project, resource, and user","val":false},{"ans":"Export Cloud Logging data to BigQuery daily and create views that filter by project, log type, resource, and user","val":true},{"ans":"Filter data in Cloud Logging by project, resource, and user, then export the data in CSV format daily","val":false}],"q_expl":"The best approach to generating daily reports that show a customer\u2018s net consumption of Google Cloud compute resources and who used the resources is to do daily exports of Cloud Logging data to BigQuery and create views filtering by project, log type, resource, and user. This approach allows for quick and efficient generation of reports, as the data is already stored in BigQuery and can be easily queried and filtered to meet the specific requirements of the customer\u2018s report. Additionally, this approach enables easy automation of the report generation process using tools such as Cloud Scheduler and Cloud Functions."},{"label":"test_16","q_format":"single","q_text":"You have a table in BigQuery containing information about purchases made across several store locations. You need to choose the best data model for optimal query performance. The table includes information such as the time of the transaction, items purchased, the store ID, and the city and state in which the store is located. You often run queries to view the sales of each item over the past 30 days and to analyze purchasing trends by state, city, and individual store. How would you model this table for the best query performance?","answers":[{"ans":"Cluster the table by state, city, and then store ID without partitioning","val":false},{"ans":"Cluster the table by store ID, city, and then state without partitioning","val":false},{"ans":"Partition the table by transaction time and cluster it by state, city, and then store ID","val":true},{"ans":"Partition the table by transaction time and cluster it by store ID, city, and then state","val":false}],"q_expl":"Partitioning by transaction time will help with filtering data by the date of the transaction, which is a common use case. Clustering by state, city, and store ID in that order will help with grouping data by geographic regions and allow for efficient filtering and aggregation. This approach will also be helpful for analyzing trends by state, city, and store, as well as for identifying which products are selling better in different regions So Partitioning is obvious Clustering is already mentioned in the option A. So we need to both to achieve the best performance."},{"label":"test_16","q_format":"single","q_text":"The company currently runs a large on-premises cluster using Spark, Hive, and HDFS and is planning to move to the cloud to benefit from cost savings and to modernize their infrastructure. They have only 2 months for their initial migration due to the timing of their contract renewal with the colocation facility. Which migration strategy should they adopt to maximize cost savings while completing the migration on time?","answers":[{"ans":"Migrate the workloads to Dataproc plus Cloud Storage; modernize later: This option suggests moving the workloads to Dataproc plus Cloud Storage without modernizing and doing so at a later time","val":true},{"ans":"Modernize the Spark workload for Dataflow and the Hive workload for BigQuery: This option suggests modernizing the Spark workload for Dataflow and the Hive workload for BigQuery","val":false},{"ans":"Migrate the Spark workload to Dataproc plus HDFS and modernize the Hive workload for BigQuery: This option suggests migrating the Spark workload to Dataproc plus HDFS and modernizing the Hive workload for BigQuery","val":false},{"ans":"Migrate the workloads to Dataproc plus HDFS; modernize later: This option suggests moving the workloads to Dataproc plus HDFS without modernizing and doing so at a later time.","val":false}],"q_expl":"The best approach would be to migrate the workloads to Dataproc plus Cloud Storage, as this option enables cost savings while allowing for a later modernization of the workloads. This solution will also ensure that the migration is completed within the 2-month timeframe. Option A and C do not address modernization and suggest delaying it until a later time, which might lead to additional costs. Option D might be too ambitious given the 2-month timeframe and the need to migrate the workloads to the cloud."},{"label":"test_16","q_format":"single","q_text":"You have multiple Spark jobs that run on a Cloud Dataproc cluster at a particular schedule. Some jobs run in a sequence, while others run concurrently. You want to automate this process. Which option would be the best to achieve this?","answers":[{"ans":"Create a Bash script that uses the Cloud SDK to generate a cluster, run jobs, and then shut down the cluster","val":false},{"ans":"Generate a Cloud Dataproc Workflow Template","val":false},{"ans":"Create an initialization action to run the jobs","val":false},{"ans":"Create a Directed Acyclic Graph in Cloud Composer","val":true}],"q_expl":"1) Create a Dataproc workflow template that runs a Spark PI job 2) Create an Apache Airflow DAG that Cloud Composer will use to start the workflow at a specific time. Ref: https:\/\/cloud.google.com\/dataproc\/docs\/tutorials\/workflow-composer"},{"label":"test_16","q_format":"single","q_text":"How can you design a storage and processing platform for a petabyte of analytics data in Google Cloud, to perform data warehouse-style analytics on the data and expose the dataset as files for batch analysis tools in other cloud providers. (Choose the best option among the following)?","answers":[{"ans":"Store and process the entire dataset in Bigtable.","val":false},{"ans":"Store the full dataset in BigQuery, and store a compressed copy of the data in a Cloud Storage bucket","val":true},{"ans":"Store the warm data as files in Cloud Storage, and store the active data in BigQuery. Maintain the ratio of 80% warm data and 20% active data","val":false},{"ans":"Store and process the entire dataset in BigQuery","val":false}],"q_expl":"The best option for this scenario would be option C: Store the full dataset in BigQuery, and store a compressed copy of the data in a Cloud Storage bucket. This option allows for data warehouse-style analytics on the entire dataset in BigQuery, which is optimized for handling large datasets and performing complex queries. Additionally, storing a batch analysis tools in other cloud providers. Option A (Store and process the entire dataset in BigQuery) may be feasible for smaller datasets, but with a petabyte of data, it may not be cost-effective or practical to process everything in BigQuery alone. Option B (Store and process the entire dataset in Bigtable) may be suitable for real-time data processing, but it is not optimized for complex analytics queries. Option D (Store the warm data as files in Cloud Storage, and store the active data in BigQuery) may be a good option for datasets with varying levels of activity, but it doesn\u2018t address the need to perform data warehouse-style analytics on the entire dataset."},{"label":"test_16","q_format":"single","q_text":"How can you resolve a quotaExceeded error when updating a CSV with 1 million records in BigQuery?","answers":[{"ans":"Increase the BigQuery UPDATE DML statement limit in the Quota management section of the Google Cloud Platform Console","val":false},{"ans":". Split the source CSV file into smaller CSV files in Cloud Storage to reduce the number of BigQuery UPDATE DML statements per BigQuery job.","val":false},{"ans":"Limit the number of records updated each day to avoid hitting the BigQuery UPDATE DML statement limit.","val":false},{"ans":"Import the new records from the CSV file into a new BigQuery table. Merge the new records with the existing records using a BigQuery job and write the results to a new BigQuery table.","val":true}],"q_expl":"Option D can be a valid approach. Importing the new records into a new BigQuery table and creating a job to merge the records can avoid the quotaExceeded error and also allow for more control over the update process. This can be a good solution if you need to perform updates frequently and need to have more flexibility and control over the process"},{"label":"test_16","q_format":"multiple","q_text":"You are working with a data pipeline that writes data to Cloud Bigtable using effective row keys. Your goal is to monitor the pipeline and determine when to increase the size of your Cloud Bigtable cluster. Which two actions can you take to achieve this (Choose two from the following options)?","answers":[{"ans":"Track the latency of read operations and increase the size of the Cloud Bigtable cluster when read operations take longer than 100 ms","val":false},{"ans":"Keep track of storage utilization and increase the size of the Cloud Bigtable cluster when utilization rises above 70% of maximum capacity.","val":true},{"ans":"Check Key Visualizer metrics and increase the size of the Cloud Bigtable cluster when the Write pressure index exceeds 100.","val":false},{"ans":"Examine Key Visualizer metrics and increase the size of the Cloud Bigtable cluster when the Read pressure index exceeds 100","val":false},{"ans":"Monitor the latency of write operations and increase the size of the Cloud Bigtable cluster when write latency experiences a sustained increase.","val":true}],"q_expl":"C: Adding more nodes to a cluster (not replication) can improve the write performance Ref: https:\/\/cloud.google.com\/bigtable\/docs\/performance D : Google recommends adding nodes when storage utilization is > 70% Ref: https:\/\/cloud.google.com\/bigtable\/docs\/modifying-instance#nodes"},{"label":"test_16","q_format":"single","q_text":"You are tasked with migrating 1 PB of data from an on-premises data center to Google Cloud, with the goal of minimizing transfer time and following Google-recommended best practices for a secure connection. Which option should you choose?","answers":[{"ans":"Use a Transfer Appliance and have engineers manually encrypt, decrypt, and verify the data","val":false},{"ans":"Use the Storage Transfer Service after establishing a Cloud Interconnect connection between the on-premises data center and Google Cloud","val":true},{"ans":"Establish a Cloud VPN connection, transfer the data using gcloud compute scp jobs in parallel, and run checksums to verify the data.","val":false},{"ans":"Transfer the data using gsutil in 3 TB batches, and run checksums to verify the data.","val":false}],"q_expl":"To transfer a large amount of data (1 PB) within a few hours, it is recommended to use a dedicated network connection like Cloud Interconnect, which can provide higher bandwidth and lower latency compared to using the public internet. The Storage Transfer Service can be used to transfer data between on-premises and cloud storage, and supports Cloud Interconnect. This approach ensures that the data transfer is fast, secure, and reliable. Option B, using a Transfer Appliance, requires manual handling of the data and can take longer. Option C, using gcloud compute scp jobs, can also take longer and may not be as reliable. Option D, reducing the data into smaller batches, can increase the complexity of the migration process and may not meet the requirement to transfer the data within a few hours"},{"label":"test_16","q_format":"single","q_text":"What is the best solution for automating the detection of damaged packages in transit using cameras installed in a shipping company\u2018s delivery lines, flagging them for human review in real-time?","answers":[{"ans":"Use the Cloud Vision API to detect damage, raise an alert via Cloud Functions, and integrate it with package tracking applications.","val":false},{"ans":"Utilize BigQuery machine learning to analyze packages in batches and train the model at scale","val":false},{"ans":"Develop a TensorFlow model trained on a corpus of images and create a Python notebook in Cloud Datalab to analyze damaged packages.","val":false},{"ans":"Train an AutoML model on a corpus of images and build an API around it to integrate with package tracking applications","val":true}],"q_expl":"AutoML is used to train model and do damage detection Auto Vision is used is a pre trained model used to detect objects in images"},{"label":"test_16","q_format":"single","q_text":"Suppose you are using Google Cloud Bigtable for a real-time application that experiences a heavy load, which comprises a mix of read and write operations. Recently, you have identified a new use case that requires you to run an hourly analytical job to compute certain statistics across the entire database. Your main priority is to ensure the reliability of your production application while also successfully executing the analytical workload. Which option would you choose?","answers":[{"ans":"Export a copy of the Bigtable data to Google Cloud Storage (GCS) and run the hourly analytical job on the exported files.","val":false},{"ans":"Add a second cluster to an existing instance with multi-cluster routing. Configure a live-traffic app profile to handle your regular workload, and a batch-analytics profile to handle the analytical workload","val":false},{"ans":"Double the size of your existing cluster and execute the analytics workload on the newly resized cluster.","val":false},{"ans":"Add a second cluster to an existing instance with single-cluster routing. Configure a live-traffic app profile to handle your regular workload, and a batch-analytics profile to handle the analytical workload","val":true}],"q_expl":"When you use a single cluster to run a batch analytics job that performs numerous large reads alongside an application that performs a mix of reads and writes, the large batch job can slow things down for the application\u2018s users. With replication, you can use app profiles with single-cluster routing to route batch analytics jobs and application traffic to different clusters, so that batch jobs don\u2018t affect your applications\u2018 users. Ref: https:\/\/cloud.google.com\/bigtable\/docs\/replication-overview#use-cases"},{"label":"test_16","q_format":"single","q_text":"As an employee of a large real estate firm, you need to prepare 6 TB of home sales data for machine learning using SQL and BigQuery ML. You plan to use the resulting model for predictions against a raw dataset. To prevent skew at prediction time, which workflow should you follow?","answers":[{"ans":"Preprocess all data using Dataflow. At prediction time, use BigQuery\u2018s ML.EVALUATE clause without specifying any further transformations on the input data.","val":false},{"ans":"Define your preprocessing logic using a BigQuery view. Use the view as your model training data when creating your model. Then, at prediction time, use BigQuery\u2018s ML.EVALUATE clause without specifying any transformations on the raw input data.","val":false},{"ans":"Use BigQuery\u2018s TRANSFORM clause to define preprocessing steps when creating your model. Before requesting predictions, transform your raw input data using a saved query and then use ML.EVALUATE","val":false},{"ans":"Use BigQuery\u2018s TRANSFORM clause to define preprocessing steps when creating your model. Then, at prediction time, use BigQuery\u2018s ML.EVALUATE clause without specifying any transformations on the raw input data.","val":true}],"q_expl":"A is the answer. https:\/\/cloud.google.com\/bigquery-ml\/docs\/bigqueryml-transform Using the TRANSFORM clause, you can specify all preprocessing during model creation. The preprocessing is automatically applied during the prediction and evaluation phases of machine learning"},{"label":"test_16","q_format":"single","q_text":"As an employee of a global shipping company, you need to train a model on 40 TB of data to predict which ships in each geographic region are likely to cause delivery delays. The model will use various attributes collected from multiple sources, including telemetry data like location in GeoJSON format that is updated every hour. You need a storage solution that has native functionality for prediction and geospatial processing, and you also want a dashboard that displays which ships are likely to cause delays in a particular region. Which storage solution is the best fit for your needs?","answers":[{"ans":"Cloud SQL for PostgreSQL","val":false},{"ans":"BigQuery","val":true},{"ans":"Cloud Datastore","val":false},{"ans":"Cloud Bigtable","val":false}],"q_expl":"In a data warehouse like BigQuery, location information is very common. Many critical business decisions revolve around location data. For example, you may record the latitude and longitude of your delivery vehicles or packages over time. You may also record customer transactions and join the data to another table with store location data. You can use this type of location data to determine when a package is likely to arrive or to determine which customers should receive a mailer for a particular store location. Geospatial analytics let you analyze and visualize geospatial data in BigQuery by using geography data types and Google Standard SQL geography functions. Ref: https:\/\/cloud.google.com\/bigquery\/docs\/geospatial-intro"},{"label":"test_16","q_format":"single","q_text":"What system should your company choose to centralize data ingestion and delivery, while retaining per-key ordering and supporting the ability to seek to a particular offset in a topic and publish\/subscribe semantics on hundreds of topics?","answers":[{"ans":"Cloud Storage","val":false},{"ans":"Dataflow","val":false},{"ans":"Firebase Cloud Messaging","val":false},{"ans":"Apache Kafka","val":true}],"q_expl":"Key words are \u201cIngestion and Delivery\u201c . So have to choose Apache Kafka. Offset of a topic explains \u201cpartition of a topic and reprocess specific part of topic, its not possible in pub\/sub as it is designed for as come and go for 1 topic\u201c \u201cPer key ordering\u201c indicates message with same key can be process or assigned to a user in kafka."},{"label":"test_16","q_format":"single","q_text":"How can query performance be improved in BigQuery for analyzing geospatial trends in a package-tracking data table with ingest-date partitioning, originally sent to an Apache Kafka stream?","answers":[{"ans":"Use clustering in BigQuery on the ingest date column","val":false},{"ans":"Use external data source by tiering older data onto Cloud Storage files and creating a new table","val":false},{"ans":"Use clustering in BigQuery on the package-tracking ID column.","val":true},{"ans":"Re-create the table using data partitioning on the package delivery date.","val":false}],"q_expl":"Clustering improves query performance in BigQuery by organizing data in a table based on the values in one or more columns. Since the table has ingest-date partitioning, clustering on the tracking-id column would group data which can help reduce the amount of data that needs to be scanned when running a query. Option B may improve performance for queries that rely on package-tracking IDs Option C could be useful for tiering older data and reducing storage costs, but it would not necessarily improve query performance. Option D suggests re-creating the table with data partitioning on the package delivery date, which could improve performance, but this would require re-ingesting the data into a new table, which may not be practical."},{"label":"test_16","q_format":"single","q_text":"Your company wants to upload their historical data to Cloud Storage, but the security rules in place do not allow external IPs to access their on-premises resources. They plan to add new data from existing on-premises applications daily after an initial upload. What should they do to transfer the data securely?","answers":[{"ans":"Use Dataflow to write the data directly to Cloud Storage","val":false},{"ans":"Install an FTP server on a Compute Engine VM to receive the files and move them to Cloud Storage.","val":false},{"ans":"Use \u201cgsutil rsync\u201c command to sync the on-premises servers with Cloud Storage","val":true},{"ans":"Write a Dataproc job template to perform the data transfer.","val":false}],"q_expl":"The best option for this scenario is to use \u201cgsutil rsync\u201c command to sync the on-premises servers with Cloud Storage. This approach allows you to securely transfer data from on-premises servers to Cloud Storage without allowing external IP access to on-premises resources. Additionally, it provides a way to automate the transfer of data on a daily basis after the initial upload. Option B, using Dataflow, would require writing a custom pipeline to transfer data and may not be the best fit for a simple data transfer task like this. Option C, writing a Dataproc job template, would require more resources and configuration than necessary for a simple data transfer task. Option D, installing an FTP server on a Compute Engine VM, may not be the best choice as it adds an additional layer of complexity and security concerns. It also requires more maintenance and management than using \u201cgsutil rsync\u201c command."},{"label":"test_16","q_format":"single","q_text":"How can you design an ACID-compliant database system that requires minimal human intervention in the event of a failure. (Choose one option from the following:)?","answers":[{"ans":"Configure a Cloud SQL for PostgreSQL instance with high availability enabled.","val":true},{"ans":"Configure a Bigtable instance with more than one cluster.","val":false},{"ans":"Configure a BigQuery table with a multi-region configuration.","val":false},{"ans":"Configure a Cloud SQL for MySQL instance with point-in-time recovery enabled.","val":false}],"q_expl":"Configuring a Cloud SQL for PostgreSQL instance with high availability ensures minimal human intervention in case of a failure as it provides automatic failover to a standby replica instance in a different zone. This option also ensures that the database remains ACID-compliant. The other options do not provide automatic failover and may require human intervention in case of a failure. Option A does not provide cross-zone replication for high availability, Option C does not provide automatic failover, and Option D is not an ACID-compliant database as it is a data warehouse."},{"label":"test_16","q_format":"single","q_text":"How can a BigQuery dataset be shared with third-party companies in a cost-effective manner while ensuring that the data is always up-to-date. (Consider the following options)?","answers":[{"ans":"Export the data regularly from BigQuery to Cloud Storage using Cloud Scheduler and provide third-party companies with access to the exported data, ensuring that the data is up-to-date.","val":false},{"ans":"Create a new BigQuery dataset that includes only the relevant data to be shared with third-party companies and grant access to this new dataset.","val":false},{"ans":"Control data access with Analytics Hub and grant third-party companies access to the original dataset while monitoring and auditing data access.","val":true},{"ans":"Set up a Dataflow job to periodically read the data and write it to a designated BigQuery dataset or Cloud Storage bucket that can be accessed by third-party companies, ensuring that the data is always current.","val":false}],"q_expl":"With Analytics Hub, you can define access policies and permissions for specific datasets, views, or tables in BigQuery. This allows you to restrict access to certain users or groups, and set fine-grained access controls based on factors such as IP address or user identity. By using Analytics Hub, you can ensure that third-party companies only have access to the specific data they need, and can also monitor and audit data access to maintain compliance with regulations and security best practices. Ref: https:\/\/cloud.google.com\/bigquery\/docs\/analytics-hub-introduction"},{"label":"test_16","q_format":"single","q_text":"You are managing an Apache Kafka-based IoT pipeline that receives 5000 messages per second. You need to set up an alert that will notify you if the moving average over one hour falls below 4000 messages per second. Which approach should you take using Google Cloud Platform?","answers":[{"ans":"Use Kafka Connect to link Kafka to Pub\/Sub, write messages to BigQuery using a Dataflow template, use Cloud Scheduler to count the number of rows in BigQuery created in the last hour, and send an alert if the count is less than 4000.","val":false},{"ans":"Use Kafka Connect to link Kafka to Pub\/Sub, write messages to Bigtable using a Dataflow template, use Cloud Scheduler to count the number of rows in Bigtable created in the last hour, and send an alert if the count is less than 4000.","val":false},{"ans":"Consume the stream of data in Dataflow using Kafka IO, set a sliding time window of 1 hour every 5 minutes, compute the average when the window closes, and send an alert if the average is less than 4000 messages per second","val":true},{"ans":"Consume the stream of data in Dataflow using Kafka IO, set a fixed time window of 1 hour, compute the average when the window closes, and send an alert if the average is less than 4000 messages per second.","val":false}],"q_expl":"Option A is the best approach for this scenario. Using Dataflow with Kafka IO, you can consume the stream of data and set a sliding time window of 1 hour every 5 minutes to compute the moving average. When the window closes, if the average is less than 4000 messages per second, an alert can be sent. This approach is efficient and effective for detecting fluctuations in the incoming message rate. Options B, C, and D are not as suitable because they involve more steps and are not optimized for computing the moving average over a sliding time window."},{"label":"test_16","q_format":"single","q_text":"You have the task of copying millions of sensitive patient records from a relational database with a total size of 10 TB to BigQuery. Your solution must be secure and time-efficient. Which option is the best solution?","answers":[{"ans":"Export the records from the database as an Avro file, upload the file to Google Cloud Storage (GCS) using gsutil, and then load the Avro file into BigQuery using the BigQuery web UI in the GCP Console","val":false},{"ans":"Export the records from the database as an Avro file, create a public URL for the Avro file, use Storage Transfer Service to move the file to Cloud Storage, and then load the Avro file into BigQuery using the BigQuery web UI in the GCP Console.","val":false},{"ans":"Export the records from the database into a CSV file, create a public URL for the CSV file, use Storage Transfer Service to move the file to Cloud Storage, and then load the CSV file into BigQuery using the BigQuery web UI in the GCP Console.","val":false},{"ans":"Export the records from the database as an Avro file, copy the file onto a Transfer Appliance, send it to Google, and then load the Avro file into BigQuery using the BigQuery web UI in the GCP Console.","val":true}],"q_expl":"The most secure and time-efficient option for copying millions of sensitive patient records from a 10 TB relational database to BigQuery is option B. This option involves exporting the records from the database as an Avro file, copying the file onto a Transfer Appliance, and then sending it to Google. Once it has been received, the Avro file can then be loaded into BigQuery using the BigQuery web UI in the GCP Console. This method ensures both security and speed by avoiding the need to send the sensitive data over the internet and relying on the high transfer rates of the Transfer Appliance. Option A involves uploading the Avro file to GCS using gsutil and then loading it into BigQuery, which may be less time-efficient and poses potential security risks. Option C involves exporting the records into a CSV file, which may not be the most efficient method for transferring large amounts of data, and using a public URL, which is less secure than option B. Option D is similar to option C but involves exporting the records as an Avro file, which may be a more efficient method for transferring large amounts of data. However, creating a public URL for the Avro file and relying on Storage Transfer Service may be less secure than using a Transfer Appliance as in option B."},{"label":"test_16","q_format":"single","q_text":"As the operator of a logistics company, you are seeking to improve the reliability of event delivery for vehicle-based sensors. Currently, you have small data centers located in various parts of the world to capture these events. However, the leased lines connecting your event collection infrastructure to the event processing infrastructure are unreliable, resulting in unpredictable latency. You need to find a cost-effective solution to address this issue. Which of the following options would be the best course of action?","answers":[{"ans":"Configure the data acquisition devices to publish data to Cloud Pub\/Sub.","val":true},{"ans":"Implement small Kafka clusters in your data centers to buffer events.","val":false},{"ans":"Establish a Cloud Interconnect between all remote data centers and Google.","val":false},{"ans":"Develop a Cloud Dataflow pipeline that aggregates all data in session windows.","val":false}],"q_expl":"The correct answer is A. Configure the data acquisition devices to publish data to Cloud Pub\/Sub.\nHere\u2019s a breakdown of why the other options are incorrect:\nB. Implement small Kafka clusters in your data centers to buffer events.\n\nWhile Kafka is a good option for buffering events, it would add additional complexity and cost to your infrastructure.\nCloud Pub\/Sub is a fully managed service that provides reliable event delivery without the need for managing additional infrastructure.\n\nC. Establish a Cloud Interconnect between all remote data centers and Google.\n\nCloud Interconnect is a dedicated network connection between your on-premises network and Google Cloud. It\u2019s a more expensive option and might not be necessary for your specific use case.\nCloud Pub\/Sub can provide reliable event delivery over the public internet without the need for a dedicated network connection.\n\nD. Develop a Cloud Dataflow pipeline that aggregates all data in session windows.\n\nWhile Cloud Dataflow can be used to process and aggregate data, it\u2019s not the best solution for addressing the reliability issue of event delivery.\nCloud Pub\/Sub can provide reliable event delivery to your Cloud Dataflow pipeline, ensuring that your data is processed in a timely and reliable manner.\n\nExplanation for the correct answer (A):\n\nCloud Pub\/Sub: Cloud Pub\/Sub is a fully managed real-time messaging service that provides reliable, scalable, and durable event delivery.\nPub\/Sub model: Cloud Pub\/Sub uses a publisher-subscriber model, where publishers publish events to topics and subscribers subscribe to those topics.\nDurable delivery: Cloud Pub\/Sub ensures that events are delivered at least once, even if there are network failures or other disruptions.\nScalability: Cloud Pub\/Sub is highly scalable, allowing you to handle millions of events per second.\n\nBy configuring the data acquisition devices to publish data to Cloud Pub\/Sub, you can ensure that events are delivered reliably and efficiently to your event processing infrastructure, even in the face of unreliable network connections."},{"label":"test_16","q_format":"single","q_text":"Which solution is best for building a scalable data pipeline to share data between job generators and job runners without negatively affecting existing applications?","answers":[{"ans":"Establish a table on Cloud Spanner to manage the job information by inserting and deleting rows","val":false},{"ans":"Establish a table on Cloud SQL to manage the job information by inserting and deleting rows","val":false},{"ans":"Develop an API using App Engine to transmit messages to and from the applications.","val":false},{"ans":"Utilize a Cloud Pub\/Sub topic to publish jobs and execute them through subscriptions","val":true}],"q_expl":"Using Cloud Pub\/Sub topic is a more appropriate solution for a data pipeline that requires scalability to manage data between job generators and job runners. Cloud Pub\/Sub is a messaging service that can handle high throughput of data, and it supports a wide range of publishing and subscription methods. By using Cloud Pub\/Sub topic, the job information can be published and subscribers can execute them as required, without negatively impacting existing applications. Option A could be a valid solution, but App Engine may not be the best choice when it comes to scaling as it has certain limitations. Option C is not recommended as Cloud SQL tables may not be able to handle large amounts of data and it may negatively impact existing applications. Option D is a viable option, but it may not be necessary to use a globally distributed database like Cloud Spanner for this particular use case."},{"label":"test_16","q_format":"single","q_text":"You need to find a cost-effective solution to retrieve large result sets of medical information from a database with over 10 TBs of data, and store it in new tables for further query. The database should have a low-maintenance architecture and be accessible via SQL. Which option should you choose?","answers":[{"ans":"Use BigQuery as a data warehouse and set output destinations for caching large queries","val":true},{"ans":"Use Cloud SQL, organize data into tables, and use JOIN in queries to retrieve data.","val":false},{"ans":"Use a MySQL cluster installed on a Compute Engine managed instance group for scalability","val":false},{"ans":"Use Cloud Spanner to replicate data across regions and normalize it in a series of tables.","val":false}],"q_expl":"Given that the company wants to retrieve large result sets of medical information, a data warehouse solution like BigQuery is ideal. BigQuery is a fully managed, serverless data warehouse that can store and query large amounts of data quickly and cost-effectively. It also provides easy SQL-based access to data, making it accessible to analysts and data scientists. Setting output destinations for caching large queries can also help improve performance and reduce costs by avoiding repeated queries. Option A (using Cloud SQL) may not be suitable for such a large database, option C (using a MySQL cluster) requires more maintenance and management. Option D (using Cloud Spanner) is not cost-effective for such large datasets and may not be necessary unless high scalability and strong consistency are critical requirements"},{"label":"test_16","q_format":"single","q_text":"What is the best database and data model to store one-second interval samples of time series CPU and memory usage for millions of computers, allow for real-time, ad hoc analytics, ensure scalability, and avoid being charged for every query executed?","answers":[{"ans":"Create a narrow table in Bigtable with a row key that combines the Computer Engine computer identifier with the sample time at each second.","val":true},{"ans":"Create a wide table in BigQuery with a column for each sample value at each second and update the row with the interval for each second","val":false},{"ans":"Create a wide table in Bigtable with a row key that combines the computer identifier with the sample time at each minute, and combine the values for each second as column data.","val":false},{"ans":"Create a table in BigQuery, and append the new samples for CPU and memory to the table","val":false}],"q_expl":"The best database and data model to store time series CPU and memory usage for millions of computers with one-second interval samples, allowing for real-time, ad hoc analytics, scalability, and avoiding being charged for every query executed, is to create a narrow table in Bigtable with a row key that combines the Computer Engine computer identifier with the sample time at each second. This model ensures efficient and fast data retrieval and can handle large volumes of data without slowing down query performance. BigQuery is not suitable for storing time series data, and the wide table model in BigQuery and Bigtable are not suitable for efficient query performance and future scalability. Ref: https:\/\/cloud.google.com\/bigtable\/docs\/schema-design-time-series"},{"label":"test_16","q_format":"single","q_text":"What is the recommended way to encrypt data for archiving in Cloud Storage using the \u201cTrust No One\u201c (TNO) approach to prevent the cloud provider staff from decrypting your data?","answers":[{"ans":"Specify customer-supplied encryption key (CSEK) in the .boto configuration file. Use gsutil cp to upload each archival file to the Cloud Storage bucket. Save the CSEK in a different project that only the security team can access.","val":true},{"ans":"Use gcloud kms keys create to create a symmetric key. Then use gcloud kms encrypt to encrypt each archival file with the key. Use gsutil cp to upload each encrypted file to the Cloud Storage bucket. Manually destroy the key previously used for encryption, and rotate the key once.","val":false},{"ans":"Specify customer-supplied encryption key (CSEK) in the .boto configuration file. Use gsutil cp to upload each archival file to the Cloud Storage bucket. Save the CSEK in Cloud Memorystore as permanent storage of the secret.","val":false},{"ans":"Use gcloud kms keys create to create a symmetric key. Then use gcloud kms encrypt to encrypt each archival file with the key and unique additional authenticated data (AAD). Use gsutil cp to upload each encrypted file to the Cloud Storage bucket, and keep the AAD outside of Google Cloud","val":false}],"q_expl":"A and B can be eliminated immediately since kms generated keys are considered potentially accessible by CSP. C is incorrect because memory store is essentially a cache service. Additional authenticated data (AAD) acts as a \u201csalt\u201c, it is not a cipher."},{"label":"test_16","q_format":"single","q_text":"As an advertising company employee, you have developed a Spark ML model to predict click-through rates for ads. Your company is migrating to Google Cloud and your data will be moved to BigQuery. Since you periodically retrain your Spark ML models, you need to migrate your existing training pipelines to Google Cloud. What is the best approach for this?","answers":[{"ans":"Use Dataproc for training existing Spark ML models but read data directly from BigQuery","val":true},{"ans":"Use Vertex AI to train existing Spark ML models","val":false},{"ans":"Spin up a Spark cluster on Compute Engine and train Spark ML models on the data exported from BigQuery","val":false},{"ans":"Rewrite models using TensorFlow and use Vertex AI for training","val":false}],"q_expl":"Since the data has been moved to BigQuery, it is best to read the data directly from BigQuery, instead of exporting it to a Spark cluster on Compute Engine. Option A is a good choice, but since the employee is already using Spark ML, it is better to stick with it instead of switching to TensorFlow. Option C allows for training the existing Spark ML models while leveraging the benefits of BigQuery."},{"label":"test_16","q_format":"single","q_text":"Which action should you take to update a running Cloud Dataflow pipeline with a new version, while ensuring no data is lost?","answers":[{"ans":"Update the pipeline using the --update option and set the --jobName to the existing job name.","val":false},{"ans":"Stop the pipeline with the Drain option and create a new job with the updated code.","val":true},{"ans":"Stop the pipeline with the Cancel option and create a new job with the updated code","val":false},{"ans":"Update the pipeline using the --update option and set the --jobName to a new unique job name.","val":false}],"q_expl":"In this scenario pipeline is streaming pipeline with windowing algorithm and triggering strategy changes to new one without loss of data, so better to go with Drain option as it fulfill all precondition described in scenario which is :- 1. Streaming 2. Code changes with windowing algorithm and triggering strategy to new way 3. No loss of data during update Ref: https:\/\/cloud.google.com\/dataflow\/docs\/guides\/stopping-a-pipeline#drain"},{"label":"test_16","q_format":"single","q_text":"An application needs to share financial market data with consumers who will receive real-time event streams, access to real-time and historical data through ANSI SQL, and batch historical exports. The data is collected from the markets in real time. Which solution should be used?","answers":[{"ans":"Cloud Pub\/Sub, Cloud Dataproc, Cloud SQL","val":false},{"ans":"Cloud Pub\/Sub, Cloud Storage, BigQuery","val":true},{"ans":"Cloud Dataproc, Cloud Dataflow, BigQuery","val":false},{"ans":"Cloud Dataflow, Cloud SQL, Cloud Spanner","val":false}],"q_expl":"To meet the requirements of the application, a combination of three different services is necessary. First, the real-time event stream should be handled by a service like Cloud Pub\/Sub, which can handle high volumes of data streams with low latency. Second, to provide ANSI SQL access to real-time streams and historical data, a service like BigQuery is ideal as it allows users to query data using ANSI SQL in real-time. Finally, to provide batch historical exports, a storage service like Cloud Storage is needed where users can access the historical data in batches"},{"label":"test_16","q_format":"single","q_text":"Your financial institution allows customers to register online, and the user data is sent to Pub\/Sub before being ingested into BigQuery. However, you need to redact customers\u2018 Government-issued Identification Numbers (GIIN) for security reasons while still allowing customer service representatives to view the original values when necessary. Which option should you choose to achieve this?","answers":[{"ans":"Before loading the data into BigQuery, use Cloud Data Loss Prevention (DLP) to replace the input values with a cryptographic hash.","val":false},{"ans":"Before loading the data into BigQuery, use Cloud Data Loss Prevention (DLP) to replace the input values with a format-preserving encryption token","val":false},{"ans":"Use BigQuery column-level security, and set the table permissions such that only members of the Customer Service user group can view the GIIN column","val":true},{"ans":"Utilize BigQuery\u2018s built-in AEAD encryption to encrypt the GIIN column, and save the keys to a new table that can be viewed only by authorized users.","val":false}],"q_expl":"The requirements is to enable authorized personnel to view the ID. Option D does not explicitly mention this."},{"label":"test_16","q_format":"single","q_text":"How can you maximize transfer speeds for your company\u2018s hybrid deployment with GCP, where analytics are conducted on anonymized customer data imported from your data center to Cloud Storage through parallel uploads to a data transfer server running on GCP, if management has informed you that the daily transfers are taking too long?","answers":[{"ans":"Enlarge the CPU capacity of your server.","val":false},{"ans":"Enhance your network bandwidth from Compute Engine to Cloud Storage.","val":false},{"ans":"Expand the size of the Google Persistent Disk on your server.","val":false},{"ans":"Upgrade your datacenter\u2018s network bandwidth to GCP.","val":true}],"q_expl":"To maximize transfer speeds for data transfers from your data center to Cloud Storage, you should increase your network bandwidth from your data center to GCP. This will allow for more data to be transferred at once and will help to reduce the time it takes for the daily transfers to complete. Increasing the CPU size on your server (option A) or the size of the Google Persistent Disk on your server (option B) will not have a significant impact on transfer speeds. Increasing your network bandwidth from Compute Engine to Cloud Storage (option D) may improve the transfer speeds, but it would not address the bottleneck in the data transfer from your data center to GCP"},{"label":"test_16","q_format":"single","q_text":"Your organization has 15 TB of data stored in a POSIX-compliant source in your on-premises data center that you need to transfer to Google Cloud. The data changes weekly and your network operations team has granted you 500 Mbps bandwidth to the public internet. You want to follow Google\u2018s recommended practices for a reliable weekly data transfer. What should you do?","answers":[{"ans":"Install Storage Transfer Service agents on-premises in your data center and configure a weekly transfer job.","val":true},{"ans":"Use Transfer Appliance to transfer the data into a Google Kubernetes Engine cluster and then configure a weekly transfer job.","val":false},{"ans":"Install Storage Transfer Service on a Google Cloud virtual machine and configure a weekly transfer job.","val":false},{"ans":"Use Cloud Scheduler to schedule a weekly transfer of the data using gsutil with the -m parameter for optimal parallelism.","val":false}],"q_expl":"Install Storage Transfer Service agents (for on-premises data in your data center) and then configure a weekly transfer job. Storage Transfer Service is a fully managed and cost-effective service that can handle transferring large amounts of data from on-premises data centers to Google Cloud. With the ability to configure a weekly transfer job, it provides a reliable and automated solution for regularly transferring large amounts of data over a limited bandwidth network The reason to install the agent in on-premise to have better performance. Ref: https:\/\/cloud.google.com\/storage-transfer\/docs\/managing-on-prem-agents#file_system_transfer_details Ref: https:\/\/cloud.google.com\/storage-transfer\/docs\/managing-on-prem-agents"},{"label":"test_16","q_format":"single","q_text":"How can you use the Cloud Data Loss Prevention API (DLP API) to mask PII data in real-time streaming files, while maintaining referential integrity?","answers":[{"ans":"Create a pseudonym by replacing the PII data with a cryptographic format-preserving token to maintain referential integrity","val":true},{"ans":"Redact all PII data, and store an unredacted version of the data in a secure bucket.","val":false},{"ans":"Replace the PII data with cryptogenic tokens to create a pseudonym, and store the non-tokenized data in a secured bucket","val":false},{"ans":"Scan every table in BigQuery for PII data, and mask the data that it finds.","val":false}],"q_expl":"To maintain referential integrity while masking PII data, a cryptographic format-preserving token can be used to replace the sensitive information. This method ensures that data can still be joined using the same key values while also keeping the sensitive data hidden from unauthorized individuals"},{"label":"test_16","q_format":"single","q_text":"You need to migrate 2 PB of historical data from an on-premises storage appliance to Cloud Storage within six months. Your outbound network capacity is limited to 20 Mb\/sec. How should you perform this migration?","answers":[{"ans":"Compress the content using gsutil cp -J before uploading to Cloud Storage","val":false},{"ans":"Use Transfer Appliance to copy the data to Cloud Storage","val":true},{"ans":"Use a tool like trickle or ionice with gsutil cp to limit the bandwidth utilization to less than 20 Mb\/sec to avoid interfering with production traffic","val":false},{"ans":"Create a private URL for the historical data and use Storage Transfer Service to copy the data to Cloud Storage","val":false}],"q_expl":"Use Transfer Appliance to copy the data to Cloud Storage"},{"label":"test_16","q_format":"single","q_text":"You want to use BigQuery ML to build a machine learning model and create a Vertex AI endpoint to handle streaming data in near-real time from multiple vendors. The data may contain invalid values. What is the best approach to handle this situation?","answers":[{"ans":"Send vendor data to a Pub\/Sub topic and use a Cloud Function to process and store it in BigQuery","val":false},{"ans":"Send vendor data to a Pub\/Sub topic and use Dataflow to process and sanitize the data before streaming it to BigQuery.","val":true},{"ans":"Use BigQuery streaming inserts to load data from multiple vendors into a new BigQuery dataset. Configure your BigQuery ML model to use the ingestion dataset as the framing data","val":false},{"ans":"Use BigQuery streaming inserts to load data from multiple vendors into the same BigQuery dataset where your BigQuery ML model is deployed","val":false}],"q_expl":"To create a machine learning model using BigQuery ML and create an endpoint for hosting the model using Vertex AI that can handle continuous streaming data in near-real time from multiple vendors, and since the data may contain invalid values, the best approach is to use Pub\/Sub to ingest the data and use Dataflow to process and sanitize the data before streaming it to BigQuery. Therefore, option D is the correct answer.Pub\/Sub is a scalable and reliable messaging service that can handle large amounts of data from multiple vendors. Dataflow is a serverless data processing service that can be used to transform and sanitize streaming data in real-time. By using Pub\/Sub and Dataflow, you can create a scalable and fault-tolerant data processing pipeline that can handle invalid values in the data. Using BigQuery streaming inserts to land the data from multiple vendors in option B can be a viable option, but it may not be the best approach since the data may contain invalid values. option A can lead to increased complexity and may not be necessary to achieve the desired outcome. Option C can work, but it does not provide a scalable and fault-tolerant data processing pipeline to handle continuous streaming data in near-real time."},{"label":"test_16","q_format":"single","q_text":"You have a BigQuery dataset that contains critical data that needs to be highly available. What storage, backup, and recovery strategy should you use to minimize cost and ensure a recovery point objective (RPO) of 30 days?","answers":[{"ans":"Set the BigQuery dataset to be multi-regional. Create a scheduled query to copy the data to tables with timestamp suffixed. Use the backup copy in case of an emergency","val":true},{"ans":"Set the BigQuery dataset to be regional. Use a point-in-time snapshot to recover the data in case of an emergency.","val":false},{"ans":"Set the BigQuery dataset to be multi-regional. Use a point-in-time snapshot to recover the data in case of an emergency.","val":false},{"ans":"Set the BigQuery dataset to be regional. Create a scheduled query to copy the data to tables with timestamp suffixed. Use the backup copy in case of an emergency","val":false}],"q_expl":"The correct option is D. Setting the BigQuery dataset to be multi-regional ensures that the data is replicated across multiple regions for high availability. Creating a scheduled query to copy the data to tables with timestamp suffixed provides backup copies of the data, and in case of an emergency, the backup copy of the table can be used to restore the data. This strategy meets the requirement of a recovery point objective (RPO) of 30 days and minimizes the cost. Option A is incorrect because setting the BigQuery dataset to be regional means that the data is stored in one region only, which increases the risk of data loss in case of a regional failure. Option B is incorrect because it involves creating copies of the data on tables with timestamp suffixed, which can lead to additional storage costs, and the scheduled query is not a reliable backup solution as it cannot capture all changes to the data. Option C is incorrect because using a point-in-time snapshot to recover data in case of an emergency can result in data loss as it only captures the data up to a specific point in time, which may not be up-to-date."},{"label":"test_16","q_format":"single","q_text":"You are designing a data warehouse using BigQuery for your company\u2018s sales data. You migrated your on-premises sales data warehouse with a star schema to BigQuery, but you are experiencing performance issues when querying the data for the past 30 days. Which of the following Google-recommended practices should you adopt to speed up the query without increasing storage costs?","answers":[{"ans":"Partition the data by transaction date.","val":true},{"ans":"Create views to materialize the dimensional data.","val":false},{"ans":"DeNormalize the data","val":false},{"ans":"Divide the data by customer ID","val":false}],"q_expl":"This is a denormalized model, where a fact table collects metrics such as order amount, discount, and quantity, along with a group of keys. These keys belong to dimension tables such as customer, supplier, region, and so on. Graphically, the model resembles a star, with the fact table in the center surrounded by dimension tables. Star schema is already denormalized so partition makes more sense going with D Ref: https:\/\/cloud.google.com\/bigquery\/docs\/migration\/schema-data-overview#migrating_data_and_schema_from_on-premises_to_bigquery"},{"label":"test_16","q_format":"single","q_text":"A financial institution plans to use Dialogflow to create a chatbot for their mobile app. Chat logs have been reviewed and customer requests have been tagged for intent. 70% of the requests are simple and require 10 or fewer intents, while the other 30% require more complex requests. The company wants to know which intents should be automated first.?","answers":[{"ans":"Automate a blend of the shortest and longest intents to represent all intents","val":false},{"ans":"Automate the 10 intents that cover 70% of the requests so that live agents can handle more complicated requests","val":true},{"ans":"Automate intents where common words such as \u2018payment\u2018 appear only once so the software isn\u2018t confused","val":false},{"ans":"Automate the more complicated requests first because those require more of the agents\u2018 time","val":false}],"q_expl":"Automating the intents that cover the majority of customer requests will help reduce the workload for live agents, allowing them to focus on more complex requests. By automating these simpler requests first, the chatbot can provide a more efficient customer service experience overall."},{"label":"test_16","q_format":"single","q_text":"Your team is responsible for the development and maintenance of ETL pipelines in your company. One of your Dataflow jobs is failing due to errors in the input data, and you need to improve the pipeline\u2018s reliability, including the ability to reprocess all failing data. What is the recommended approach?","answers":[{"ans":"Include a try...catch block in your DoFn that transforms the data, extract erroneous rows from logs","val":false},{"ans":"Include a filtering step to avoid these types of errors in the future, extract erroneous rows from logs","val":false},{"ans":"Include a try...catch block in your DoFn that transforms the data, write erroneous rows directly to Pub\/Sub from the DoFn","val":false},{"ans":"Include a try...catch block in your DoFn that transforms the data, use a sideOutput to create a PCollection that can be stored to Pub\/Sub later","val":true}],"q_expl":"Include a try\u2026catch block in your DoFn that transforms the data, use a sideOutput to create a PCollection that can be stored to Pub\/Sub later. By adding a try\u2026catch block to the DoFn, the job can handle any errors that occur during data transformation, and the erroneous data can be passed to a side output instead of being dropped. This allows the pipeline to continue processing the remaining data, and the erroneous data can be stored to a Pub\/Sub topic for reprocessing later."},{"label":"test_16","q_format":"multiple","q_text":"You are tasked with creating a data pipeline for a 1.5 PB time-series transaction data set that grows by 3 TB per day. The goal is to copy the data to BigQuery for analysis by your data science team, who will build machine learning models on this data. The data is structured and updated with new status every hour. To maximize performance and usability for your data science team, which two strategies should you adopt?","answers":[{"ans":"Preserve the structure of the data as much as possible","val":false},{"ans":"Denormalize the data as much as possible.","val":true},{"ans":"Copy a daily snapshot of transaction data to Cloud Storage and store it as an Avro file. Use BigQuery\u2018s support for external data sources to query","val":false},{"ans":"Use BigQuery UPDATE to further reduce the size of the dataset.","val":false},{"ans":"Develop a data pipeline where status updates are appended to BigQuery instead of updated","val":true}],"q_expl":"A: Denormalization increases query speed for tables with billions of rows because BigQuery\u2018s performance degrades when doing JOINs on large tables, but with a denormalized data structure, you don\u2018t have to use JOINs, since all of the data has been combined into one table. Denormalization also makes queries simpler because you do not have to use JOIN clauses. https:\/\/cloud.google.com\/solutions\/bigquery-data-warehouse#denormalizing_data D: BigQuery append"},{"label":"test_16","q_format":"single","q_text":"How can you optimize a customer-facing dashboard that displays large quantities of aggregated data and is expected to have a high volume of concurrent users using BigQuery and Data Studio to provide quick visualizations with minimal latency?","answers":[{"ans":"Use BigQuery BI Engine with logical views.","val":false},{"ans":"Use BigQuery BI Engine with authorized views","val":false},{"ans":"Use BigQuery BI Engine with materialized views","val":true},{"ans":"Use BigQuery BI Engine with streaming data.","val":false}],"q_expl":"BigQuery BI Engine is a fully managed, in-memory analysis service for BigQuery that allows users to analyze large and complex datasets interactively with sub-second query response time and high concurrency. It can be used with materialized views to accelerate query processing for commonly-used analytical queries. Materialized views store the results of a query in a table, and then query that table instead of recomputing the results each time the view is queried. This can significantly reduce the query response time, especially for large and complex queries. Using BigQuery BI Engine with materialized views is the recommended approach for optimizing performance for customer-facing dashboards that display large quantities of aggregated data with high volume of concurrent users."},{"label":"test_16","q_format":"single","q_text":"You are developing a linear regression model in BigQuery ML to predict a customer\u2018s likelihood of purchasing your company\u2018s products. The model requires city names as a key predictive component, but the data must be organized into columns for training and serving the model. Which method is the most efficient way to prepare the data?","answers":[{"ans":"Use Cloud Data Fusion to assign a number to each city based on its region and represent it with that number in the model","val":false},{"ans":"Create a new view in BigQuery that excludes the city column","val":false},{"ans":"Use TensorFlow to generate a categorical variable with a vocabulary list and a vocabulary file that can be uploaded to BigQuery ML","val":false},{"ans":"Use SQL in BigQuery to apply one-hot encoding to the state column and convert each city to a binary value column","val":true}],"q_expl":"The most efficient method to prepare the data for a linear regression model on BigQuery ML with city names as a key predictive component is to use SQL in BigQuery to apply one-hot encoding to the state column and convert each city to a binary value column. Option B is the correct answer. One-hot encoding is a standard method for handling categorical variables in a linear regression model. It creates dummy variables for each city, allowing the model to incorporate the city information as a predictive variable. This approach requires less coding and is easier to implement compared to options C and D. Therefore, the correct answer is option B: Use SQL in BigQuery to apply one-hot encoding to the state column and convert each city to a binary value column"},{"label":"test_16","q_format":"single","q_text":"You are working on a regression problem in the natural language processing domain with a dataset of 100 million labeled examples. After randomly splitting your dataset into train and test samples (90\/10), training your neural network, and evaluating your model, you notice that the root-mean-squared error (RMSE) is twice as high on the train set compared to the test set. What is the best way to improve your model\u2018s performance?","answers":[{"ans":"Collect more data and increase the size of your dataset","val":false},{"ans":"Increase the percentage of the test sample in the train-test split","val":false},{"ans":"Use regularization techniques like dropout or batch normalization to prevent overfitting","val":true},{"ans":"Increase the model\u2018s complexity by adding an extra layer or increasing the size of vocabularies or n-grams","val":false}],"q_expl":"Regularization techniques such as dropout or batch normalization can help prevent overfitting, which is likely causing the higher RMSE on the train set compared to the test set. By introducing dropout, batch normalization, or other regularization techniques to your model, you can help prevent the model from memorizing the training data and thus, overfitting. Increasing the percentage of the test sample in the train-test split (option A) is unlikely to be effective as it may result in a smaller training set, which could lead to underfitting. Collecting more data (option B) can be helpful but is often costly and time-consuming. Increasing the complexity of your model (option D) could make overfitting even worse and lead to longer training times. Therefore, regularization techniques (option C) are the most effective solution in this scenario."},{"label":"test_16","q_format":"single","q_text":"Your organization is looking to optimize its batch pipeline for processing structured data on Google Cloud. Currently, you are using PySpark for data transformations, but your pipelines are taking more than 12 hours to run. You need a serverless solution that can support SQL syntax to expedite development and pipeline run time. Your raw data is already stored in Cloud Storage. Which approach should you take to build your pipeline on Google Cloud while meeting your performance and processing requirements?","answers":[{"ans":"Ingest your data into BigQuery from Cloud Storage, convert your PySpark commands into BigQuery SQL queries to transform the data, and then write the transformations to a new table","val":true},{"ans":"Convert your PySpark commands into SparkSQL queries to transform the data and use Dataproc to write the data into BigQuery.","val":false},{"ans":"Ingest your data into Cloud SQL, convert your PySpark commands into SparkSQL queries to transform the data, and then use federated queries from BigQuery for machine learning","val":false},{"ans":"Use the Apache Beam Python SDK to build the transformation pipelines and write the data into BigQuery.","val":false}],"q_expl":"BigQuery is an option to meet the performance and processing requirements. Ref: https:\/\/medium.com\/paypal-tech\/comparing-bigquery-processing-and-spark-dataproc-4c90c10e31ac"},{"label":"test_16","q_format":"multiple","q_text":"What are two recommended steps for a company migrating its on-premises data warehousing solutions to BigQuery to optimize CDC performance and reduce compute overhead while ensuring that changes to the source systems are available in near-real-time in the BigQuery reporting table (Choose two.)?","answers":[{"ans":"Periodically delete outdated records from the reporting table.","val":false},{"ans":"Insert each new CDC record and its corresponding operation type in real-time to the reporting table, and use a materialized view to expose only the latest version of each unique record","val":false},{"ans":"Replicate each individual CDC record in real-time using DML INSERT, UPDATE, or DELETE operations directly on the reporting table","val":false},{"ans":"Use DML MERGE periodically to perform several DML INSERT, UPDATE, and DELETE operations at the same time on the reporting table.","val":true},{"ans":"Insert each new CDC record and its corresponding operation type in real-time to a staging table","val":true}],"q_expl":"Delta tables contain all change events for a particular table since the initial load. Having all change events available can be valuable for identifying trends, the state of the entities that a table represents at a particular moment, or change frequency. The best way to merge data frequently and consistently is to use a MERGE statement, which lets you combine multiple INSERT, UPDATE, and DELETE statements into a single atomic operation Ref: https:\/\/cloud.google.com\/architecture\/database-replication-to-bigquery-using-change-data-capture#overview_of_cdc_data_replication"},{"label":"test_16","q_format":"single","q_text":"How can you migrate a Redis database from an on-premises data center to a Memorystore for Redis instance while following Google-recommended practices and minimizing cost, time, and effort. Which option is the best?","answers":[{"ans":"Backup the Redis database as an RDB file, copy the file to a Cloud Storage bucket using the gsutil utility, and import the file into the Memorystore for Redis instance","val":true},{"ans":"Create a secondary instance of the Redis database on a Compute Engine instance, then perform a live cutover.","val":false},{"ans":"Write a shell script to migrate the Redis data and create a new Memorystore for Redis instance.","val":false},{"ans":"Use Dataflow to read the Redis database from the on-premises data center and write the data to a Memorystore for Redis instance","val":false}],"q_expl":"Option A is the best choice. The recommended practice for migrating a Redis database to Memorystore for Redis is to create an RDB backup of the database, copy the RDB file to Cloud Storage, and then import it into the Memorystore instance. This method is quick and easy to implement and provides minimal downtime. Option B is not recommended because it involves creating a secondary instance of the Redis database on a Compute Engine instance and performing a live cutover. This method can be complex and may result in data loss or extended downtime. Option C involves creating a Dataflow job to read the Redis database from the on-premises data center and write the data to a Memorystore for Redis instance. This method can be complex and may result in increased cost and time. Option D involves writing a shell script to migrate the Redis data and create a new Memorystore for Redis instance. This method can be error-prone and may result in data loss or extended downtime. Therefore, it is not a recommended approach."},{"label":"test_16","q_format":"single","q_text":"What is the most efficient and cost-effective way to store both the raw social media posts for historical archiving and the data extracted from them, while meeting the requirements of analyzing hundreds of thousands of social media posts daily with minimal steps?","answers":[{"ans":"Feed the social media posts directly into the API from their source, and write the extracted data from the API into BigQuery","val":false},{"ans":"Utilize Cloud SQL to store both the social media posts and the data extracted from the API.","val":false},{"ans":"Utilize BigQuery to store both the social media posts and the data extracted from the API.","val":false},{"ans":"Store the raw social media posts in Cloud Storage, and write the extracted data from the API into BigQuery","val":true}],"q_expl":"The most efficient and cost-effective way to store both the raw social media posts for historical archiving and the data extracted from them, while meeting the requirements of analyzing hundreds of thousands of social media posts daily with minimal steps, is to store the raw social media posts in Cloud Storage and write the extracted data from the API into BigQuery."},{"label":"test_16","q_format":"single","q_text":"How can you cleanse monthly CSV data files received from a third party when the schema of the files changes every third month. The requirements for implementing these transformations include enabling non-developer analysts to modify transformations, providing a graphical tool for designing transformations, and executing the transformations on a schedule?","answers":[{"ans":"Load each month\u2018s CSV data into BigQuery, and write a SQL query to transform the data to a standard schema. Merge the transformed tables together with a SQL query","val":false},{"ans":"Help the analysts write a Dataflow pipeline in Python to perform the transformation. The Python code should be stored in a revision control system and modified as the incoming data\u2018s schema changes","val":false},{"ans":"Use Apache Spark on Dataproc to infer the schema of the CSV file before creating a Dataframe. Then implement the transformations in Spark SQL before writing the data out to Cloud Storage and loading into BigQuery.","val":false},{"ans":"Use Dataprep by Trifacta to build and maintain the transformation recipes, and execute them on a scheduled basis","val":true}],"q_expl":"Use Dataprep by Trifacta to build and maintain the transformation recipes, and execute them on a scheduled basis. Dataprep by Trifacta provides a graphical tool for designing transformations and enables non-developer analysts to modify transformations. It can also execute the transformations on a schedule"},{"label":"test_16","q_format":"single","q_text":"You are tasked with implementing workflow pipeline scheduling using open-source tools and Google Kubernetes Engine (GKE). To simplify and automate the task, you want to use a Google-managed service that also accommodates Shared VPC networking considerations. Which option should you choose?","answers":[{"ans":"Use Dataflow for your workflow pipelines and use shell scripts to schedule workflows.","val":false},{"ans":"Use Cloud Composer in a Shared VPC configuration and place the Cloud Composer resources in the service project","val":true},{"ans":"Use Cloud Composer in a Shared VPC configuration and place the Cloud Composer resources in the host project.","val":false},{"ans":"Use Dataflow for your workflow pipelines and use Cloud Run triggers for scheduling","val":false}],"q_expl":"Cloud Composer is a managed workflow orchestration service that is built on Apache Airflow, and it allows you to create, schedule, and monitor workflows. It integrates with other Google Cloud services such as BigQuery, Cloud Storage, and Dataflow, among others.To accommodate Shared VPC networking considerations, you should place the Cloud Composer resources in the service project. This is because, in a Shared VPC setup, the service project contains the resources that are shared across multiple projects, while the host project contains the shared VPC configuration. By placing the Cloud Composer resources in the service project, you can simplify the network configuration and ensure that the workflows can access the resources they need in the host project. This approach also enables you to centralize access control and monitoring, which can help you maintain security and compliance"},{"label":"test_16","q_format":"single","q_text":"You are tasked with designing an Apache Beam pipeline to enrich data from Cloud Pub\/Sub with static reference data from BigQuery. The reference data can be stored in memory on a single worker. The resulting data should be written to BigQuery for further analysis. Which job type and transforms should be used for this pipeline?","answers":[{"ans":"A streaming job that utilizes PubSubIO to stream data from Cloud Pub\/Sub, and BigQueryIO to enrich the data with the reference data from BigQuery. Side-inputs are used to store the reference data in memory on a single worker, and the enriched data is written to BigQuery for analysis","val":true},{"ans":"A streaming job that utilizes PubSubIO to stream data from Cloud Pub\/Sub, and BigQueryIO to enrich the data with the reference data from BigQuery. The enriched data is then written to BigQuery for further analysis using side-outputs.","val":false},{"ans":"A streaming job that uses PubSubIO and JdbcIO transforms to enrich the data with the reference data from BigQuery. The pipeline uses side-outputs to write the enriched data to BigQuery for further analysis","val":false},{"ans":"A batch job that utilizes PubSubIO to stream data from Cloud Pub\/Sub, and side-inputs to enrich the data with the reference data from BigQuery. The enriched data is then written to BigQuery for analysis.","val":false}],"q_expl":"For this scenario, a streaming job is necessary since data is being enriched in real-time. The PubSubIO transform is used to stream data from Cloud Pub\/Sub, and BigQueryIO is used to enrich the data with the reference data from BigQuery. The small reference data can be stored in memory on a single worker using side-inputs, and the enriched data can then be written to BigQuery for analysis using BigQueryIO In streaming analytics applications, data is often enriched with additional information that might be useful for further analysis. For example, if you have the store ID for a transaction, you might want to add information about the store location. This additional information is often added by taking an element and bringing in information from a lookup table. Ref: https:\/\/cloud.google.com\/dataflow\/docs\/tutorials\/ecommerce-java#side-input-pattern"},{"label":"test_16","q_format":"multiple","q_text":"Your organization is expanding its use of Google Cloud Platform, with many teams creating their own projects for different stages of deployment and target audiences. Each project requires unique access control configurations, and the central IT team needs access to all projects. Additionally, data from Cloud Storage and BigQuery must be shared between projects as needed. What steps should you take to simplify access control management and minimize the number of policies (Choose two.)?","answers":[{"ans":"Use Cloud Deployment Manager to automate access provisioning","val":false},{"ans":"Create separate Cloud IAM policies for each individual team and specify their groups.","val":false},{"ans":"Implement a resource hierarchy to leverage access control policy inheritance.","val":true},{"ans":"Only share data between projects using service accounts.","val":false},{"ans":"For each Cloud Storage bucket or BigQuery dataset, determine which projects need access and create a Cloud IAM policy granting access to all active members who have access to those projects","val":true}],"q_expl":"Option B allows for the implementation of a resource hierarchy that provides access control inheritance and reduces the number of policies required. Option E creates a Cloud IAM policy for each project that requires access to a specific Cloud Storage bucket or BigQuery dataset, which simplifies the management of policies and ensures that only the required projects have access to the data. Invalid Options :- Option A is not a valid solution to simplify access control management and minimize the number of policies since it automates access provisioning and does not necessarily reduce the number of policies. Option C would create a policy for each team and would not simplify the management of access control policies. Option D limits data sharing between projects to service accounts only, which can be overly restrictive and may not address all use cases where data sharing is required."},{"label":"test_17","q_format":"single","q_text":"Your company is migrating an on-premises long-term storage archive to Google Cloud. The archived files are accessed on average about once every 30 days. You would like to minimize the cost of storage. What storage option would you recommend?","answers":[{"ans":"Nearline Storage","val":true},{"ans":"Multi-regional storage","val":false},{"ans":"Persistent Disks","val":false},{"ans":"Coldline Storage","val":false}],"q_expl":"Nearline Storage is a class of Cloud Storage designed for objects that will be accessed at most once every 30 days. Coldline Storage is suitable for objects accessed at most once per 90 days. Multi-regional storage is best suited for objects that should have low latency access from multiple regions. Persistent disks are used with Compute Engine instances and Kubernetes Engine clusters and should not be used for archival storage. See https:\/\/cloud.google.com\/storage\/docs\/storage-classes"},{"label":"test_17","q_format":"single","q_text":"You are training a deep learning model for a classification task. The precision and recall of the model is quite low. What could you do to improve the precision and recall scores?","answers":[{"ans":"Use dropout","val":false},{"ans":"Use L2 regularization","val":false},{"ans":"Use more training instances","val":true},{"ans":"Use L1 regularization","val":false}],"q_expl":"The correct answer is to use more training instances. This is an example of underfitting. The other options are all regularizations used in cases of overfitting. See https:\/\/machinelearningmastery.com\/overfitting-and-underfitting-with-machine-learning-algorithms\/"},{"label":"test_17","q_format":"single","q_text":"A team of socio-economic researchers is analyzing documents as part of a research study. The documents have had personally identifying information redacted. The researchers are concerned that someone with access to the data may be able to use quasi-identifiers, such as age and postal code, to re-identify some individuals. How can the researchers quantify that risk?","answers":[{"ans":"Run a custom machine learning model trained to estimate the re-identification risk.","val":false},{"ans":"Run a re-identification risk analysis using the Data Loss Prevention service.","val":true},{"ans":"Apply the re-identification infotype to each document with quasi-identifiers to calculate the level of risk.","val":false},{"ans":"Use counts of the number of occurrences of quasi-identifiers identified using Data Loss Prevention infotypes.","val":false}],"q_expl":"A re-identification risk analysis job using DLP will provide the information needed by the researchers. Using a custom trained machine learning program to estimate risk would take longer, require maintenance, and assumes the researchers are also proficient in machine learning. DLP uses infotypes but there is no re-identification risk infotype. Counting specific infotypes may provide some indication of re-identification risk but it is unlikely that a simple linear model of risk will give accurate or useful information. See https:\/\/cloud.google.com\/blog\/products\/identity-security\/taking-charge-of-your-data-understanding-re-identification-risk-and-quasi-identifiers-with-cloud-dlp"},{"label":"test_17","q_format":"single","q_text":"You have been tasked with ensuring the successful transfer of 100 TB of data from the AWS S3 object storage system. This is a one time transfer. A complete and reliable transfer of all data is a top priority. How would you recommend loading this data into Cloud Storage?","answers":[{"ans":"gsutil cp","val":false},{"ans":"Transfer Appliance","val":false},{"ans":"Cloud Dataflow","val":false},{"ans":"Cloud Storage Transfer Service","val":true}],"q_expl":"Cloud Storage Transfer Service is designed to load terabytes of data using scheduled jobs and is well suited for transferring data from AWS. Gsutil is a command line utility for working with Cloud Storage but not designed to upload large volumes of data. Cloud Dataflow is a batch and stream processing system but is not designed for large data transfers from other public clouds. Transfer Appliance is designed for large data loads but requires attaching a storage device to the source system\u2018s network and so suitable only when you have physical access to the source system network. See https:\/\/cloud.google.com\/storage-transfer-service"},{"label":"test_17","q_format":"single","q_text":"A team of data scientists is becoming increasingly dependent on jobs running in a Cloud Dataproc cluster. They would like to increase the number of master nodes from 1 to 2 to improve availability. What command would they use?","answers":[{"ans":"gcloud dataproc create master-nodes","val":false},{"ans":"gcloud dataproc nodes add-master","val":false},{"ans":"The number of master nodes cannot be changed once a cluster is created.","val":true},{"ans":"gcloud dataproc master-nodes","val":false}],"q_expl":"The correct answer is that the number of master nodes in a Cloud Dataproc cluster cannot be changed once the cluster is created. See https:\/\/cloud.google.com\/dataproc\/docs\/guides\/manage-cluster"},{"label":"test_17","q_format":"single","q_text":"A machine learning model is not performing as well in production as the validation tests would predict. You suspect the model is overfitting. What technique can you use during training to reduce the risk of overfitting?","answers":[{"ans":"Backpropagation","val":false},{"ans":"L2 Regularization","val":true},{"ans":"Gradient descent","val":false},{"ans":"Label engineering","val":false}],"q_expl":"L2 regularization is a technique for preventing overfitting. Gradient descent is a general approach to finding optimal values. Backpropagation is an algorithm to adjusting weights in a neural network. There is no such thing as label engineering in ML; feature engineering is a practice for determining additional features used to model training. See https:\/\/cloud.google.com\/bigquery-ml\/docs\/preventing-overfitting"},{"label":"test_17","q_format":"multiple","q_text":"A group of data analysts has asked for your help setting up a Cloud Dataproc cluster to analyze large data sets using Spark. The cluster will run many jobs. You plan to follow Google recommended best practices. Which of the following would you do? (choose 2)","answers":[{"ans":"Use Cloud Storage for persistent storage","val":true},{"ans":"Use no more than 30% preemptible VMs for secondary workers","val":true},{"ans":"Use preemptible VMs only on workers that run HDFS","val":false},{"ans":"Use HDFS storage on persistent disks","val":false},{"ans":"Disable autoscaling","val":false}],"q_expl":"Using Cloud Storage for persistent storage is recommended over using HDFS on local disks. This allows data to persist when a cluster is shut down without having to copy data from the cluster to Cloud Storage. Google recommends a maximum of 30% preemptible VMs as secondary nodes. Autoscaling is recommended when using Cloud Storage and the cluster runs many jobs. See https:\/\/cloud.google.com\/blog\/topics\/developers-practitioners\/dataproc-best-practices-guide"},{"label":"test_17","q_format":"single","q_text":"A retailer is building machine learning models to help predict the number of products that will be sold and therefore should be available in inventory. What kind of model should they build?","answers":[{"ans":"Feature","val":false},{"ans":"Reinforcement","val":false},{"ans":"Regression","val":true},{"ans":"Classification","val":false}],"q_expl":"The correct answer is a regression model, which is used to predict a value based on a set of input parameters. Classification models categorize or label inputs, such as determining if a vehicle is a car or a truck. A feature is an attribute used in a machine learning model to describe the characteristics of an instance used in training. Reinforcement learning is a kind of machine learning in which an agent learns from the environment. See https:\/\/cloud.google.com\/automl-tables\/docs\/problem-types"},{"label":"test_17","q_format":"single","q_text":"A manufacturer of delivery drones has a monitoring system built on an Apache Beam runner. Temperature received over the past hour is analyzed and if any temperature reading is more than 2 standard deviations away from the mean for the past hour, an alert is triggered. What kind of windowing functions would you use to implement this operation?","answers":[{"ans":"session windows","val":false},{"ans":"fixed windows (also called tumbling windows)","val":false},{"ans":"concurrent windows","val":false},{"ans":"sliding window (also called hopping windows)","val":true}],"q_expl":"Sliding windows (also called hopping windows) model a consistent time interval in a stream so it is the best option for continuously averaging the temperature for the past hour. Fixed windows (also known as tumbling windows) model a consistent, disjoint time interval in the stream. Session windows can contain a gap in duration and are used to model non-continuous streams of data. There is no concurrent window type of functions in Apache Beam runners such as Cloud Dataflow. See https:\/\/cloud.google.com\/dataflow\/docs\/concepts\/streaming-pipelines"},{"label":"test_17","q_format":"single","q_text":"You are developing a deep learning model and have training data with a large number of features. You are not sure which features are important. You\u2018d like to use a regularization technique that will drive the parameter for the least important features toward zero. What regularization technique would you use?","answers":[{"ans":"L2 or Ridge Regression","val":false},{"ans":"Backpropagation","val":false},{"ans":"Dropout","val":false},{"ans":"L1 or Lasso Regression","val":true}],"q_expl":"L1 or Lassos Regression adds an absolute value of magnitude penalty which drives the parameters (or coefficients) of least useful features toward zero. L2 or Ridge Regression adds a squared magnitude penalty that penalizes large parameters. Dropout is another form of regularization that ignores some features at some steps of the training process. Backpropagation is an algorithm for assigning error penalties to nodes in a neural network. See https:\/\/cloud.google.com\/bigquery-ml\/docs\/preventing-overfitting and https:\/\/machinelearningmastery.com\/overfitting-and-underfitting-with-machine-learning-algorithms\/"},{"label":"test_17","q_format":"single","q_text":"A manufacturer has successfully migrated several data warehouses to BigQuery and is using Cloud Storage for machine learning data. ML engineers and data analysts are having difficulty finding data sets they need. The CTO of the company has asked for your advice on how to reduce the workload on ML engineers and analysts when they need to find data sets. What would you recommend?","answers":[{"ans":"Use Cloud Logging to track files uploaded to Cloud Storage and data sets to BigQuery.","val":false},{"ans":"Use Cloud Fusion to tracking both files uploaded to Cloud Storage and data sets loaded into BigQuery.","val":false},{"ans":"Use Cloud Data Catalog to automatically extract metadata from Cloud Storage objects and BigQuery data.","val":true},{"ans":"Query the metadata catalog of BigQuery and Cloud Storage and write the results to a BigQuery table where the ML engineers and data analysts can query the data with SQL.","val":false}],"q_expl":"The correct answer is to use Cloud Data Catalog, which can automatically extract metadata from sources including Cloud Storage, BigQuery, Cloud Bigtable, Cloud Pub\/Sub, and Google Sheets. Cloud Logging is used for recording data about events and is not the best way to collect metadata. Cloud Fusion is an ETL tool, not a metadata extraction tool. Developing your own metadata extraction tool, such as one that queries BigQuery metadata, requires more work and maintenance than using a managed service. See https:\/\/cloud.google.com\/data-catalog\/docs\/concepts\/overview"},{"label":"test_17","q_format":"single","q_text":"You are training a deep learning model with a relatively small set of features and a large number of instances. The model is not performing as well as you like. You believe the model is underfitting. What technique would you try to improve performance?","answers":[{"ans":"AI","val":false},{"ans":"Use gradient descent","val":false},{"ans":"Use backpropagation","val":false},{"ans":"Use feature crosses","val":true}],"q_expl":"Since there are a large number of training instances and few features, this is a good candidate for adding synthetic features by using feature crosses. Dropout is a type of regularization used to prevent overfitting. Backpropagation is an algorithm for adjusting parameters in a neural network. Gradient descent algorithm is used in optimization problems. See https:\/\/developers.google.com\/machine-learning\/crash-course\/feature-crosses\/encoding-nonlinearity"},{"label":"test_17","q_format":"single","q_text":"Your company has created a new data analytics team. Data analysts will need to read data from and write data to Cloud Storage and query data from BigQuery. Data engineers will also need to create Cloud Storage buckets and set data lifecycle management policies on buckets. You want to follow Google Cloud\u2018s recommended best practices. How would you manage access permission for the new team?","answers":[{"ans":"Grant roles to each user individually. Assign data engineers and data analysts the roles needed by either data analysts or data engineers.","val":false},{"ans":"Create a group for the data analytics team. Grant the group all roles needed by data analysts and data engineers to that group. Add the identities of all team members to the group.","val":false},{"ans":"Grant roles to each user individually. Assign data engineers the same roles as data analysts and additional roles needed for their additional responsibilities.","val":false},{"ans":"Create a group for data analysts and a group for data engineers. Add the identities of data analysts to the data analyst group. Add the identities of the data engineers to the data engineer group. Grant roles to the data analyst group to allow access needed by data analysts. Grant roles to the data engineer group needed by the data engineers.","val":true}],"q_expl":"Roles should be assigned to groups not individual identities. Each group should only have the roles needed to perform their job responsibilities in accordance with the Principle of Least Privilege. The correct answer is to crate two groups, assign data analysts to the data analyst group and data engineers to the data engineer group. Grant each group only the roles needed by that group. See https:\/\/cloud.google.com\/iam\/docs\/recommender-best-practices and https:\/\/cloud.google.com\/iam\/docs\/understanding-custom-roles"},{"label":"test_17","q_format":"single","q_text":"You are developing a data pipeline that will run several data transformation programs on Compute Engine virtual machines. You do not want to use your credentials for authenticating and authorizing these programs. You want to follow Google Cloud recommended practices, how would you authenticate and authorize the data transformation programs?","answers":[{"ans":"Create a service account and assign roles to the service account that are needed to execute the data transformation programs. Use Secret Manager to store service account keys.","val":false},{"ans":"Create a service account and assign roles to the service account that are needed to execute the data transformation programs. Use Google managed keys to store both public and private portion of the service account keys.","val":true},{"ans":"Create a Gmail account and use that account to create an IAM group. Store the password for the group in Secret Manager.","val":false},{"ans":"Create a Gmail account and use that account to create an IAM user. Store the password for the account in Secret Manager.","val":false}],"q_expl":"Service accounts should be uses, not a user identity or a group. A service account should be created and assigned necessary roles. Google managed keys should be used for managing service accounts, not Secret Manager, which is used for secrets such as usernames and passwords. See https:\/\/cloud.google.com\/docs\/authentication\/production"},{"label":"test_17","q_format":"single","q_text":"Data at rest in Google Cloud is encrypted at the hardware, infrastructure, and platform levels. What encryption algorithm is used for encryption at the infrastructure level?","answers":[{"ans":"AES256","val":true},{"ans":"Blowfish","val":false},{"ans":"DES","val":false},{"ans":"RSA","val":false}],"q_expl":"Advanced Encryption Standard (AES) with a 256-bit key is used for encrypting data at the infrastructure level in Google Cloud. Blowfish and RSA are also strong encryption algorithms but they are not used at the infrastructure level of GCP. Data Encryption Standard (DES) is a symmetric key algorithm that uses 56 bits and is considered weak encryption and should not be used for securing data. See https:\/\/cloud.google.com\/security\/encryption\/default-encryption"},{"label":"test_17","q_format":"multiple","q_text":"A Cloud Spanner database is experiencing hot-spotting. You suggest changing the primary keys of tables in the database. What methods would you consider when defining new keys? (Choose 2)","answers":[{"ans":"Start primary key with low cardinality attribute","val":false},{"ans":"Hash value of existing primary key","val":true},{"ans":"Timestamps","val":false},{"ans":"Big-reversed sequential values","val":true},{"ans":"Auto-incrementing values","val":false}],"q_expl":"Hash values of existing primary keys and bit-reversed sequential values would both provide well distributed keys and help avoid hot spotting. Auto-incrementing values, timestamps, and low cardinality attribute can all lead to hot spotting. See https:\/\/cloud.google.com\/spanner\/docs\/schema-design"},{"label":"test_17","q_format":"multiple","q_text":"You are currently using Apache Kafka to ingest messages from IoT sensors. A data pipeline based on Apache Flink reads the data from Kafka and processes the data before writing results to long-term storage. If you wanted to migrate to Google Cloud and use managed services instead of Apache Kafka and Apache Flink, what services would you use? (Choose 2)","answers":[{"ans":"Cloud Composer","val":false},{"ans":"Cloud Firestore","val":false},{"ans":"Cloud Data Fusion","val":false},{"ans":"Cloud Dataflow","val":true},{"ans":"Cloud Pub\/Sub","val":true}],"q_expl":"The correct answers are Cloud Pub\/Sub as a replacement for Apache Kafka and Cloud Dataflow as a replacement for Apache Flink. Cloud Pub\/Sub is a messaging service. Cloud Dataflow, like Apache Flink, implements an Apache Beam runner. Cloud Data Fusion is an ETL tool. Cloud Composer is a workflow orchestration tool based on Apache Airflow. Cloud Firestore is a NoSQL document database. See https:\/\/cloud.google.com\/pubsub, https:\/\/cloud.google.com\/dataflow, and https:\/\/cloud.google.com\/blog\/products\/data-analytics\/simplify-and-automate-data-processing-with-dataflow-prime"},{"label":"test_17","q_format":"single","q_text":"To comply with industry regulations, you will need to capture logs of all changes made to IAM roles and identities. Logs must be kept for 3 years. How would you meet this requirement?","answers":[{"ans":"Use Cloud Audit Logs and keep the logs in Cloud Logging. Specify a three year retention policy in Cloud Logging that automatically deletes the logs after three years.","val":false},{"ans":"Use Cloud Audit Logs and export them to Bigtable. Create a retention policy and retention policy lock to prevent the logs from being deleted prior to them reaching 3 years of age. Define a lifecycle policy to delete the logs after three years.","val":false},{"ans":"Use Cloud Audit Logs and keep the logs in Cloud Monitoring. Specify a three year retention policy in Cloud Logging that automatically deletes the logs after three years.","val":false},{"ans":"Use Cloud Audit Logs and export them to Cloud Storage. Create a retention policy and retention policy lock to prevent the logs from being deleted prior to them reaching 3 years of age. Define a lifecycle policy to delete the logs after three years.","val":true}],"q_expl":"Cloud Audit log captures changes to IAM entities and keeps logs for 30 days. To keep them longer, export them to Cloud Storage. Use a retention policy to define how long the logs should be kept and using a retention policy lock to prevent changes to the retention period. Cloud Logging does not keep logs beyond 30 days and does not support retention policies. Cloud Monitoring collects and displays metrics, it does not store logs. Bigtable is not a good storage option for logs, it is designed for low latency writes at high volumes and provides for key lookups and queries that require range scanning. See https:\/\/cloud.google.com\/logging\/docs\/audit"},{"label":"test_17","q_format":"multiple","q_text":"A North American retailer is planning to expand to Europe and specifically target individuals from ages 20 to 40 and living in Spain, France, Belgium, and Germany. The retailer plans to create detailed profiles about customer preferences so they can make recommendations. What regulations will the company need to comply with when it expands as planned? (Choose 2)","answers":[{"ans":"SOX","val":false},{"ans":"Expedited Funds Transfer Act","val":false},{"ans":"GDPR","val":true},{"ans":"PCI Data Security Standard","val":true},{"ans":"HIPAA","val":false}],"q_expl":"Retailers receive payment via payment cards and so are subject to the Payment Card Industry (PCI) Data Security Standard. Since the company will have data on European Union citizens, it must comply with GDPR. HIPAA is a healthcare regulation in the United States. Sarbanes Oxley (SOX) is a US regulation on public companies designed to prevent fraudulent accounting practices. There is no Expedited Funds Transfer Act. See https:\/\/cloud.google.com\/security\/compliance\/pci-dss and https:\/\/cloud.google.com\/privacy\/gdpr"},{"label":"test_17","q_format":"single","q_text":"An insurance company wants to implement a chatbot service to help direct customers to the best customer support team for their questions. What GCP service would you recommend?","answers":[{"ans":"Text-to-Speech API","val":false},{"ans":"Dialogflow","val":true},{"ans":"Speech-to-Text API","val":false},{"ans":"AutoML Tables","val":false}],"q_expl":"The correct answer is Dialogflow is a service for creating conversational user interfaces. Speech-to-Text converts spoken words to written words. Text-to-Speech converts text words to human voice-like sound. AutoML Tables is a machine learning service for structured data. See https:\/\/cloud.google.com\/dialogflow\/docs"},{"label":"test_17","q_format":"single","q_text":"What can be done to increase the training speed of a neural network model that is taking days to train?","answers":[{"ans":"Should you subsample your test dataset?","val":false},{"ans":"Should you increase the number of layers in your neural network?","val":false},{"ans":"Should you increase the number of input features to your model?","val":false},{"ans":"Should you subsample your training dataset?","val":true}],"q_expl":"Subsampling the training dataset is a common technique to reduce the training time of a neural network model. By using a smaller portion of the dataset, the model can be trained faster, as it requires fewer iterations to reach computationally expensive to process. Subsampling the test dataset (option A) is not recommended, as this can lead to inaccurate performance measures and compromise the validity of the results. Increasing the number of input features (option C) or the number of layers in the neural network (option D) can also help to improve the performance of the model, but they are not directly related to the training speed. In fact, increasing the complexity of the model may increase the training time, as more computations are required."},{"label":"test_17","q_format":"multiple","q_text":"You have created a Compute Engine instance with an attached GPU but the GPU is not used when you train a Tensorflow model. What might you do to ensure the GPU can be used for training your models? (Choose 2)","answers":[{"ans":"Update Python 3 on the VM","val":false},{"ans":"Use Pytorch instead of Tensorflow","val":false},{"ans":"Use a Deep Learning VM image","val":true},{"ans":"Grant the Owner basic role to the VM service account","val":false},{"ans":"Install GPU drivers","val":true}],"q_expl":"GPU drivers need to be installed if they are not installed already when using GPUs. Deep Learning VM images have GPU drivers installed. Using Pytorch instead of Tensorflow will require work to recode and Pytorch would not be able to use GPUs either if the drivers are not installed. Updating Python will not address the problem of missing drivers. Granting a new role to the service account of the VM will not address the need to install GPU drivers. See https:\/\/cloud.google.com\/compute\/docs\/gpus\/install-drivers-gpu"},{"label":"test_17","q_format":"single","q_text":"You have developed a DoFn function for a Cloud Dataflow workflow. You discover that the PCollection does not have all the data needed to perform a necessary computation. You want to provide additional input each time an element of a PCollection is processed. What kind of Apache Beam construct would you use?","answers":[{"ans":"Partition","val":false},{"ans":"Side input","val":true},{"ans":"Watermark","val":false},{"ans":"Custom window","val":false}],"q_expl":"The correct answer is a side input, which is an additional input for DoFn. A partition in Apache Beam separates elements of a collection into multiple output collections. A Watermark is used to indicate no data with timestamps earlier than the watermark will arrive in the future. A custom window is created using WindowFn functions to implement windows based on data-driven gaps. See https:\/\/cloud.google.com\/architecture\/e-commerce\/patterns\/slow-updating-side-inputs"},{"label":"test_17","q_format":"single","q_text":"If you are responsible for writing ETL pipelines to run on an Apache Hadoop cluster that requires checkpointing and splitting pipelines, which method should you use to write the pipelines?","answers":[{"ans":"Java using MapReduce","val":false},{"ans":"PigLatin using Pig","val":true},{"ans":"Python using MapReduce","val":false},{"ans":"HiveQL using Hive","val":false}],"q_expl":"Pig is scripting language which can be used for checkpointing and splitting pipelines"},{"label":"test_17","q_format":"single","q_text":"You would like to set a maximum number of concurrent jobs in Cloud Dataproc. How would you do that?","answers":[{"ans":"Set computengine:mgi.scheduler.max-concurrent-jobs property when creating a managed instance group for Cloud Dataproc cluster.","val":false},{"ans":"Use Cloud Monitoring to detect the number of jobs running and when the maximum threshold is exceeded, trigger a Cloud Function to terminate the most recently created job.","val":false},{"ans":"Set dataproc:dataproc.scheduler.max-concurrent-jobs property when adding worker nodes.","val":false},{"ans":"Set dataproc:dataproc.scheduler.max-concurrent-jobs property when creating a cluster.","val":true}],"q_expl":"The correct answer is to set the dataproc:dataproc.scheduler.max-concurrent-jobs property when creating a cluster. That property is not set when adding worker nodes. Properties of a Dataproc cluster that are specific to Dataproc are not set in Compute Engine. You do not need to monitor and terminate jobs using ad hoc procedures like triggering a Cloud Function after a maximum threshold is exceeded. See https:\/\/cloud.google.com\/dataproc\/docs\/concepts\/configuring-clusters\/cluster-properties"},{"label":"test_17","q_format":"single","q_text":"You are consulting to a company developing an IoT application that analyzes data from sensors deployed on drones. The application depends on a database that can write large volumes of data at low latency. The company has used Hadoop HBase in the past but wants to migrate to a managed database service. What service would you recommend?","answers":[{"ans":"Bigtable","val":true},{"ans":"Cloud Spanner","val":false},{"ans":"Cloud Firestore","val":false},{"ans":"BigQuery","val":false}],"q_expl":"Bigtable is a wide column database with low latency writes that is well suited for IoT data storage and it has an HBase API. BigQuery is a data warehouse service. Cloud Dataproc is a managed Spark\/Hadoop service. Cloud Firestore is a NoSQL document model database. See https:\/\/cloud.google.com\/bigtable\/docs\/hbase-bigtable"},{"label":"test_17","q_format":"single","q_text":"How can you collate bid events from multiple application servers in real-time to determine which user bid first in your globally distributed auction application?","answers":[{"ans":"Have each application server write the bid events to Google Cloud Pub\/Sub, and use a pull subscription with Google Cloud Dataflow to pull and process the events, giving the bid to the first user processed","val":true},{"ans":"Have each application server write bid events to Cloud Pub\/Sub, and push them to a custom endpoint that writes the events to Cloud SQL for real-time collation","val":false},{"ans":"Set up a MySQL database for each application server to write bid events to, and periodically query each database to update a master MySQL database with the events.","val":false},{"ans":"Create a shared file and have application servers write bid events to it, then use Apache Hadoop to process the file and identify the first bidder","val":false}],"q_expl":"Option A, creating a shared file and processing it with Apache Hadoop, would not be a good choice for real-time processing as it introduces additional latency and potential issues with data consistency. Option B, having each application server write bid events to Cloud Pub\/Sub and pushing them to a custom endpoint that writes the events to Cloud SQL, would not be a good choice for real-time processing as Cloud SQL is not designed for high-throughput real-time operations. Option C, setting up a MySQL database for each application server to write bid events to and periodically querying each database to update a master database, would also introduce latency and could lead to consistency issues. Therefore, option D, having each application server write bid events to Google Cloud Pub\/Sub and using a pull subscription with Google Cloud Dataflow to pull and process the events in real-time, giving the bid to the first user processed, is the most appropriate solution for real-time bid processing in a distributed auction application."},{"label":"test_17","q_format":"multiple","q_text":"You are tasked with selecting a NoSQL database to handle telemetry data from millions of IoT devices. The volume of data is increasing at a rate of 100 TB per year, and each data entry has approximately 100 attributes. The data processing pipeline does not require ACID compliance, but high availability and low latency are crucial. You need to query individual fields in the data for analysis. Which three databases are suitable for this purpose. (Choose three from the following options)?","answers":[{"ans":"HDFS with Hive","val":false},{"ans":"Cassandra","val":true},{"ans":"MongoDB","val":true},{"ans":"MySQL","val":false},{"ans":"Redis","val":false},{"ans":"HBase","val":true}],"q_expl":"Cassandra: It is a highly scalable, distributed NoSQL database designed for handling large volumes of data with high availability and low latency. It supports querying individual fields and can be easily integrated with data processing pipelines.\nMongoDB: Another popular NoSQL database, MongoDB is known for its flexibility and ease of use. It supports querying individual fields and can handle large volumes of data.\nHBase: A distributed, column-oriented NoSQL database, HBase is optimized for handling large datasets and real-time analytics. It supports querying individual fields and can be integrated with Hadoop ecosystem for data processing.\n\nThe other options are not suitable for this scenario:\n\nMySQL: While MySQL is a popular relational database, it might not be the best choice for handling large volumes of unstructured data and real-time analytics.\nRedis: Redis is an in-memory data store that is primarily used for caching and real-time applications. It might not be suitable for storing large amounts of historical data and complex queries.\nHDFS with Hive: HDFS is a distributed file system, not a database. Hive is a data warehouse infrastructure built on top of HDFS, which is not optimized for real-time analytics and querying individual fields.\n\nTherefore, Cassandra, MongoDB, and HBase are the most suitable databases for handling telemetry data from millions of IoT devices with the given requirements."},{"label":"test_17","q_format":"single","q_text":"A team of analysts working with healthcare data have analyzed data in a BigQuery dataset for personally identifiable information. They want to store the results of the analysis in a managed service that will make it easy for them to retrieving information about the PII analysis at later times. What service would you recommend?","answers":[{"ans":"Data Catalog","val":true},{"ans":"BigQuery","val":false},{"ans":"Data Loss Prevention","val":false},{"ans":"Cloud Spanner","val":false}],"q_expl":"Data Catalog is specifically designed to catalog, discover, and understand data across various Google Cloud Platform (GCP) services. It can store metadata about datasets, including details about PII analysis.\nData Loss Prevention is focused on preventing data loss and protecting sensitive data, but it might not be the best choice for storing the results of the analysis.\nBigQuery is a serverless data warehouse for large-scale data analysis and querying. While it can store the results, it\u2019s not optimized for storing and retrieving metadata about the analysis.\nCloud Spanner is a globally distributed, strongly consistent relational database. It\u2019s suitable for transactional workloads but might not be the most efficient for storing and retrieving metadata.\n\nBy using Data Catalog, the analysts can easily store, search, and retrieve information about the PII analysis, making it a valuable resource for future reference.\n\u00a0See https:\/\/cloud.google.com\/data-catalog\/docs\/concepts\/overview"},{"label":"test_17","q_format":"single","q_text":"You have uploaded 5 years of log data to Cloud Storage. A user reported that some data points in the log data are outside of their expected ranges, indicating errors. You need to fix this issue and ensure that the original data is retained for compliance reasons. What should you do?","answers":[{"ans":"Use a Dataflow workflow to read the data from Cloud Storage, identify and correct values outside the expected range, and write the updated data to a new dataset in Cloud Storage","val":true},{"ans":"Import the data from Cloud Storage to BigQuery, create a new table, and remove the rows with errors","val":false},{"ans":"Use a Dataflow workflow to read the data from Cloud Storage, identify and correct values outside the expected range, and overwrite the original dataset in Cloud Storage.","val":false},{"ans":". Create a new copy of the data in Cloud Storage using a Compute Engine instance, and remove the rows with errors","val":false}],"q_expl":"The correct answer is:\nUse a Dataflow workflow to read the data from Cloud Storage, identify and correct values outside the expected range, and write the updated data to a new dataset in Cloud Storage.\nHere\u2019s why:\n\nDataflow workflow: This is a scalable and managed service that can efficiently process large amounts of data. It\u2019s well-suited for tasks like reading, transforming, and writing data to Cloud Storage.\nIdentify and correct errors: You can use Dataflow to write code that identifies data points outside the expected range and corrects them based on your defined rules or logic.\nNew dataset: By writing the updated data to a new dataset, you preserve the original data for compliance purposes while also having a corrected dataset for analysis.\n\nThe other options are not as suitable:\n\nImporting to BigQuery: While BigQuery can be used for data analysis, it might not be the most efficient or cost-effective solution for this specific task, especially if you need to process large amounts of data.\nOverwriting original dataset: Overwriting the original dataset would destroy the historical data, which is required for compliance.\nUsing Compute Engine: While this approach might be possible, it would involve managing infrastructure and could be less scalable and efficient than using a managed service like Dataflow.\n\nBy using a Dataflow workflow, you can efficiently identify and correct errors in your log data while preserving the original data for compliance purposes."},{"label":"test_17","q_format":"single","q_text":"You are tasked with migrating a 3TB relational database to the Google Cloud Platform, but you do not have the resources to make significant changes to the application that uses this database. The primary consideration is the cost of operating the database. In this context, which service would you choose from the Google Cloud Platform to store and serve your data. (Your choices are)?","answers":[{"ans":"Cloud Firestore","val":false},{"ans":"Cloud SQL","val":true},{"ans":"Cloud Bigtable","val":false},{"ans":"Cloud Spanner","val":false}],"q_expl":"Cloud SQL is a fully-managed service that supports MySQL, PostgreSQL, and SQL Server databases. It provides a simple and cost-effective solution to run and manage relational databases in the cloud. Additionally, it offers features such as automatic backups, automated patching, and high availability, which can reduce the burden of database administration and improve the availability of the database. While other options such as Cloud Spanner, Cloud Bigtable, and Cloud Firestore offer high scalability and performance, they may require more significant changes to the application, and may not be the most cost-effective solution for a 2TB relational database."},{"label":"test_17","q_format":"single","q_text":"What service should you use to manage several batch jobs that have interdependent steps to be executed in a specific order and involve running shell scripts, Hadoop jobs, and BigQuery queries. These jobs are expected to run for many minutes up to several hours, and if the steps fail, they must be retried a fixed number of times. Choose the correct option?","answers":[{"ans":"Use Cloud Functions to manage the execution of the batch jobs","val":false},{"ans":"Use Cloud Scheduler to schedule and run the batch jobs","val":false},{"ans":"Use Cloud Composer to orchestrate and manage the execution of the batch jobs","val":true},{"ans":"Use Cloud Dataflow to manage the execution of the batch jobs","val":false}],"q_expl":"Option D (Cloud Composer) is a Fully managed workflow orchestration service that allows you to define, schedule, and monitor complex workflows consisting of multiple tasks with dependencies and retries. It is a suitable option for managing the execution of batch jobs with many interdependent steps that involve running shell scripts, Hadoop jobs, and BigQuery queries, and it allows you to retry failed tasks a fixed number of times Option A (Cloud Scheduler) is a fully managed cron job service that allows you to schedule and run batch jobs using Cloud Pub\/Sub, Cloud Functions, or HTTP targets. However, it does not provide the ability to manage the dependencies and order of interdependent steps in a batch job. Option B (Cloud Dataflow) is a fully managed service for executing ETL and data processing pipelines that parallelize the processing of large datasets. Although it supports retries, it does not provide a way to manage the dependencies and order of interdependent steps. Option C (Cloud Functions) is a serverless compute platform that allows you to run code in response to events, such as changes to data in Cloud Storage or messages in Pub\/Sub. It is not suitable for managing a complex series of interdependent steps."},{"label":"test_17","q_format":"single","q_text":"As an employee working at a bank, you possess a dataset that is labeled and contains information about loan applications that have been approved and whether or not these applications resulted in defaults. Your task is to create a model that can predict the default rates for future credit applicants. What actions should you take to achieve this goal?","answers":[{"ans":"Eliminate any biases present in the data and gather information on loan applications that have been declined","val":false},{"ans":"Connect the applicants\u2018 social profiles with their loan applications to facilitate feature engineering.","val":false},{"ans":"Train a predictive model, such as a linear regression, to estimate the credit default risk score","val":true},{"ans":"Expand the size of the dataset by gathering more data.","val":false}],"q_expl":"Train a predictive model: You should train a predictive model on the labeled dataset that you possess. A suitable model for this task would be a classification model, such as logistic regression or decision trees, that can classify loan applications as either default or non-default. The model should be trained using various features of the loan application, such as the applicant\u2018s credit history, income, and employment status"},{"label":"test_17","q_format":"single","q_text":"As a retailer, you want to expand your online sales by integrating with various in-home assistants like Google Home. In order to process customer voice commands and place orders in the backend systems, what solutions would be appropriate?","answers":[{"ans":"Utilize Cloud Natural Language API","val":false},{"ans":"Deploy AutoML Natural Language","val":false},{"ans":"Use Speech-to-Text API","val":false},{"ans":"Implement Dialogflow Enterprise Edition","val":true}],"q_expl":"Dialogflow is a conversational AI platform that provides tools to create and manage conversational interfaces, including chatbots and voice assistants. The Dialogflow Enterprise Edition is specifically designed for large-scale, mission-critical deployments that require high availability, security, and scalability. The Speech-to-Text API (option A) is a Google Cloud service that converts audio to text. While this could be useful for processing voice commands, it does not provide the conversational flow management or intent recognition capabilities that are needed for building a full-featured voice assistant. The Cloud Natural Language API (option B) is another Google Cloud service that provides natural language processing capabilities, such as sentiment analysis and entity recognition. While this could be useful for analyzing the text of customer voice commands, it does not provide the conversational flow management or intent recognition capabilities that are needed for building a full-featured voice assistant. AutoML Natural Language (option D) is a Google Cloud service that allows users to create custom machine learning models for natural language processing tasks, such as sentiment analysis and entity recognition. While this could be useful for customizing the natural language processing capabilities of a voice assistant, it does not provide the conversational flow management or intent recognition capabilities that are needed for building a full-featured voice assistant. Therefore, Dialogflow Enterprise Edition (option C) is the best solution for integrating online sales capabilities with in-home assistants because it provides the necessary conversational flow management and intent recognition capabilities for building a full-featured voice assistant"},{"label":"test_17","q_format":"multiple","q_text":"You have a Dataflow pipeline that writes time series metrics to Bigtable. However, you have noticed that the data is slow to update in Bigtable, which is used to feed a dashboard with thousands of concurrent users. You need to support more concurrent users while reducing the time it takes to write the data. What are two actions you should take?","answers":[{"ans":"Modify the pipeline to use the Flatten transform before writing to Bigtable","val":false},{"ans":"Use Cloud Dataflow service to execute the pipeline","val":false},{"ans":"Increase the maximum number of workers by setting maxNumWorkers in PipelineOptions","val":true},{"ans":"Modify the pipeline to use the CoGroupByKey transform before writing to Bigtable","val":false},{"ans":"Increase the number of nodes in the Bigtable cluster","val":true}],"q_expl":"The two actions to take to improve the performance of the pipeline and reduce the time it takes to write data to Bigtable are B and C. Increasing the maximum number of workers by setting maxNumWorkers in PipelineOptions can help the pipeline process data faster by distributing the workload among more workers. Increasing the number of nodes in the Bigtable cluster can help handle more concurrent users and increase the capacity to store and retrieve data. A is incorrect because using local execution can only be useful for testing the pipeline locally, but it won\u2018t improve the performance for handling larger amounts of data. D and E are incorrect because the Flatten and CoGroupByKey transforms are not designed to improve the performance of writing data to Bigtable."},{"label":"test_17","q_format":"single","q_text":"How can you address users not getting BigQuery slots when there is a quota of 2K concurrent on-demand slots per project, and you want to avoid adding new projects to the account. Your organization has multiple business units with different budgets and priorities?","answers":[{"ans":"Switch to flat-rate pricing and set up a hierarchical priority system for your projects","val":true},{"ans":"Create another project to bypass the 2K on-demand per-project quota.","val":false},{"ans":"Increase the number of concurrent slots per project on the Quotas page in the Cloud Console.","val":false},{"ans":"Transform your batch BigQuery queries to interactive ones.","val":false}],"q_expl":"The benefits of using BigQuery Reservations include: \u2013 Workload management. After you purchase slots, you can allocate them to workloads. That way, a workload has a dedicated pool of BigQuery computational resources available for use. At the same time, if a workload doesn\u2018t use all of its allocated slots, any unused slots are shared automatically across your other workloads. Centralized purchasing: You can purchase and allocate slots for your entire organization. You don\u2018t need to purchase slots for each project that uses BigQuery Ref: https:\/\/cloud.google.com\/bigquery\/docs\/reservations-intro"},{"label":"test_17","q_format":"single","q_text":"You are running a web application that currently serves customers from a single region in Asia. You are planning to expand your services globally, but initially, you want to optimize for cost. After securing funding, you plan to optimize for global presence and performance using a native JDBC driver. Which option should you choose?","answers":[{"ans":"Start with a zonal instance of Cloud SQL for PostgreSQL and then use Cloud SQL for PostgreSQL with highly available configuration after securing funding.","val":false},{"ans":"Start with a highly available instance of Cloud SQL for PostgreSQL, and then use Bigtable with US, Europe, and Asia replication after securing funding. Option","val":false},{"ans":"Configure a single region instance of Cloud Spanner initially and then configure multi-region Cloud Spanner instances after securing funding","val":true},{"ans":"Start with a zonal instance of Cloud SQL for PostgreSQL, and then use Bigtable with US, Europe, and Asia replication after securing funding","val":false}],"q_expl":"Using Cloud Spanner with a single region instance initially and then configuring multi-region Cloud Spanner instances after securing funding would be the most cost-effective and scalable option for this scenario. Cloud Spanner is a globally distributed relational database with strong consistency and scalability features. Starting with a single-region instance and then moving to multi-region instances after securing funding would help optimize for cost in the short term, while still allowing for global presence and performance in the long term. Using a native JDBC driver would also be possible with Cloud Spanner"},{"label":"test_17","q_format":"single","q_text":"What is the best fully managed database for a new project that needs to automatically scale up, provide transactional consistency, be able to scale up to 6 TB, and be queried using SQL?","answers":[{"ans":"Cloud Bigtable","val":false},{"ans":"Cloud SQL","val":true},{"ans":"Cloud Datastore","val":false},{"ans":"Cloud Spanner","val":false}],"q_expl":"Cloud SQL is a good option when you need relational database capabilities but don\u0092t need storage capacity over 10TB or more than 4000 concurrent connections. You also need to be skilled at on-premise management. Cloud Spanner Cloud Spanner is a good option when you plan to use large amounts of data (more than 10TB) and need transactional consistency. If you start with Cloud SQL and need to eventually move to Cloud Spanner, be prepared to re-write your application in addition to migrating your database."},{"label":"test_17","q_format":"single","q_text":"How can you enable each analytics team to monitor BigQuery slot usage within their respective projects?","answers":[{"ans":"Build a Cloud Monitoring dashboard using the BigQuery metric query\/scanned_bytes.","val":false},{"ans":"Develop a Cloud Monitoring dashboard utilizing the BigQuery metric slots\/allocated_for_project.","val":true},{"ans":"Export a log for each project, collect the BigQuery job execution logs, create a custom metric based on the totalSlotMs, and create a Cloud Monitoring dashboard based on the custom metric.","val":false},{"ans":"Export an aggregated log at the organization level, collect the BigQuery job execution logs, create a custom metric based on the totalSlotMs, and build a Cloud Monitoring dashboard based on the custom metric.","val":false}],"q_expl":"The correct answer is \nB. Create a Cloud Monitoring dashboard based on the BigQuery metric slots\/allocated_for_project. \nThis option allows each team to monitor the number of slots allocated to their project and how many of them are currently in use, giving them visibility into their current and historical usage. \nRef: https:\/\/cloud.google.com\/bigquery\/docs\/reservations-monitoring#viewing-slot-usage \nOption A is incorrect because it monitors data volume, not slot usage. \nOption C is inefficient because it requires creating a custom metric for each project and may result in a large number of metrics. \nOption D is also inefficient and may result in a large number of logs being exported, making it difficult to monitor slot usage for each project separately"},{"label":"test_17","q_format":"single","q_text":"A company in the aerospace industry has flight data stored in a proprietary format. You need to import this data into BigQuery and stream it efficiently with minimal resource consumption. Which option is the best?","answers":[{"ans":"Use a shell script that triggers a Cloud Function for periodic ETL batch jobs on the new data source.","val":false},{"ans":"Use a standard Dataflow pipeline to store the raw data in BigQuery, then transform the format later when the data is used","val":false},{"ans":"Write a Dataproc job using Apache Hive to stream the data into BigQuery in CSV format.","val":false},{"ans":"Use a custom connector with Apache Beam to write a Dataflow pipeline that streams the data into BigQuery in Avro format.","val":true}],"q_expl":"The best option for efficiently importing the proprietary flight data into BigQuery with minimal resource consumption is Option D \u2013 Use a custom connector with Apache Beam to write a Dataflow pipeline that streams the data into BigQuery in Avro format. \nOption D involves using a custom connector with Apache Beam to write a Dataflow pipeline that streams the data into BigQuery in Avro format. This option is the best because it offers a high level of flexibility, efficiency, and optimization for streaming data into BigQuery. \nApache Beam offers a flexible and powerful framework for building data pipelines, while the Avro format is optimized for streaming data and can help minimize resource consumption. \nOption A involves using a shell script to trigger a Cloud Function for periodic ETL batch jobs on the new data source. This option could be less efficient and resource-intensive since batch jobs can be resource-intensive, and the periodic execution of the ETL process may not be sufficient to keep up with the real-time nature of flight data. \nOption B involves using a standard Dataflow pipeline to store the raw data in BigQuery and transform the format later when the data is used. While this option could work, it may require more resources to store the raw data and may not be as efficient for real-time streaming of data. \nOption C involves writing a Dataproc job using Apache Hive to stream the data into BigQuery in CSV format. While this option could work, it may not be as efficient as using a custom connector with Apache Beam, and the CSV format may not be as optimized for streaming."},{"label":"test_17","q_format":"single","q_text":"Your company, based in the United States, has developed an application for analyzing and responding to user actions. The primary table of the application has a data volume that increases by 200,000 records per second. Many third-party developers use your application\u2018s APIs to integrate its functionality into their own frontend applications. Your APIs should meet the following requirements: 1)Provide a single global endpoint (2)Support ANSI SQL (3)Ensure consistent access to the most up-to-date data. Which of the following options should you choose?","answers":[{"ans":"Use Bigtable with the primary cluster located in North America and secondary clusters located in Asia and Europe.","val":false},{"ans":"Use BigQuery without selecting any specific region for storage or processing","val":false},{"ans":"Use Cloud Spanner with the master located in North America and read-only replicas located in Asia and Europe","val":true},{"ans":"Use Cloud SQL for PostgreSQL with the master located in North America and read replicas located in Asia and Europe.","val":false}],"q_expl":"Given that the primary table\u2018s data volume grows by 250,000 records per second, the application requires a scalable and distributed database solution. Cloud Spanner is a globally distributed, horizontally scalable, and strongly consistent relational database service that can handle such workloads. It also provides a single global endpoint, supports ANSI SQL, and ensures consistent access to the most up-to-date data. Additionally, with its read-only replicas located in Asia and Europe, the API users can access the data with low latency"},{"label":"test_17","q_format":"single","q_text":"You have a set of historic data stored in Cloud Storage, and you need to analyze it. You want a solution that can identify invalid data entries and perform data transformations without requiring programming or SQL knowledge. Which option would you choose?","answers":[{"ans":"Use Cloud Dataproc with a Hadoop job for error detection and data transformations","val":false},{"ans":"Utilize Cloud Dataprep with recipes for error detection and data transformations","val":true},{"ans":"Use Cloud Dataflow with Beam to perform error detection and data transformations.","val":false},{"ans":"Use federated tables in BigQuery with queries to perform error detection and data transformations.","val":false}],"q_expl":"Cloud Dataprep is a powerful and user-friendly data preparation tool that can help users clean and transform data without any programming or SQL knowledge. It allows users to visually explore and transform data using a simple interface and intuitive \u201crecipes.\u201c These recipes include pre-built transformations and error detection functions that can be easily applied to the data with just a few clicks. In addition, Cloud Dataprep provides real-time previews of the changes, so users can quickly see the results of the transformations. Cloud Dataprep also integrates with Cloud Storage, making it easy to access and process data stored in Cloud Storage. This eliminates the need for any data migration, saving time and reducing costs. Therefore, utilizing Cloud Dataprep with recipes is a good option for identifying invalid data entries and performing data transformations on historic data stored in Cloud Storage, without requiring any programming or SQL knowledge."},{"label":"test_17","q_format":"multiple","q_text":"You are migrating an on-premises Hadoop system that uses ORC data format and Hive as the primary tool to Cloud Dataproc. You have successfully copied all ORC files to a Cloud Storage bucket, and you need to replicate some data to the cluster\u2018s local Hadoop Distributed File System (HDFS) to improve performance. Which of the following are two ways to start using Hive in Cloud Dataproc (Choose two)?","answers":[{"ans":"Transfer all ORC files from the Cloud Storage bucket to HDFS using the gsutil utility, and mount the Hive tables locally.","val":false},{"ans":"Load the ORC files into BigQuery, use the BigQuery connector for Hadoop to mount the BigQuery tables as external Hive tables, and replicate the external Hive tables to the native ones.","val":false},{"ans":"Transfer all ORC files from the Cloud Storage bucket to any node of the Dataproc cluster using the gsutil utility, and mount the Hive tables locally.","val":false},{"ans":"Use the Cloud Storage connector for Hadoop to mount the ORC files as external Hive tables, and replicate the external Hive tables to the native ones","val":true},{"ans":"Transfer all ORC files from the Cloud Storage bucket to the master node of the Dataproc cluster using the gsutil utility, run the Hadoop utility to copy them to HDFS, and mount the Hive tables from HDFS.","val":true}],"q_expl":"Option C is a valid solution, but it involves extra steps, such as manually copying the files to HDFS, that can be avoided. \nOption D is a valid solution because it leverages the Cloud Storage connector for Hadoop to mount the ORC files as external Hive tables and then replicates the external Hive tables to the native ones, allowing for efficient data access and replication. \nOption A is incorrect because it suggests copying all ORC files to HDFS, which may not be the most efficient solution. Additionally, mounting Hive tables locally may not be the best option as it would not take advantage of Dataproc\u2018s distributed nature. \nOption B is not ideal because copying all ORC files to any node in the cluster may result in data skew or network congestion. It also does not address how to replicate the data to HDFS. \nOption E is not a direct solution to the problem because it involves loading the ORC files into BigQuery, which may not be the desired data storage solution for the use case."},{"label":"test_17","q_format":"single","q_text":"You have trained a support vector machine (SVM) classifier with default parameters for a binary classification problem and achieved an area under the curve (AUC) of 0.87 on the validation set. You want to improve the AUC of the model. Which option should you choose?","answers":[{"ans":"Scale the predictions of the model by tuning a scaling factor as a hyperparameter to achieve the highest AUC","val":false},{"ans":"Train a deep neural network classifier, as they are always better than SVMs","val":false},{"ans":"Deploy the model and expect a higher real-world AUC due to generalization","val":false},{"ans":"Perform hyperparameter tuning on the SVM classifier","val":true}],"q_expl":"Hyperparameter tuning involves optimizing the parameters of a model to improve its performance. Since the SVM model was trained with default parameters, it is likely that there is room for improvement. By performing hyperparameter tuning, the performance of the SVM classifier can be improved, potentially leading to a higher AUC. Option B is not necessarily true, as the performance of deep neural networks depends on the nature of the data and the problem. Option C is not a reliable approach, as the real-world performance of a model can vary greatly depending on the context. Option D is not a common approach for improving model performance and may not be appropriate for a binary classification problem."},{"label":"test_17","q_format":"single","q_text":"As an employee of a manufacturing company that sources 750 different components from 750 suppliers, you have a labeled dataset that contains around 1000 examples of each unique component. Your team wants to create an app that can help warehouse workers identify incoming components by analyzing photos of them. You need to create a Proof-Of-Concept for this app within a few working days. Which approach should you take?","answers":[{"ans":"Use Cloud Vision API and provide custom labels as hints for recognition","val":false},{"ans":"Use Cloud Vision AutoML, but reduce the size of the dataset by half.","val":false},{"ans":"Use Cloud Vision AutoML with the existing dataset.","val":true},{"ans":"Train your own image recognition model using transfer learning techniques.","val":false}],"q_expl":"Given that the company has a large number of unique components (750) with 1000 examples of each component, and the goal is to create a working proof-of-concept for the app in a few working days, using Cloud Vision AutoML with the existing dataset would be the most practical and efficient option. Cloud Vision AutoML is a machine learning tool provided by Google Cloud that enables users to train custom models for image recognition. With the existing labeled dataset, it would be possible to quickly train an image recognition model using Cloud Vision AutoML. Option B is not an ideal approach because reducing the dataset size would result in less data to train the model on, which could potentially decrease its accuracy. Option C is not the best approach because it relies on custom labels as recognition hints, which might not be accurate or relevant for all components. Option D would require more time to train the model using transfer learning techniques, which would not be feasible for creating a working proof-of-concept within a few working days."},{"label":"test_17","q_format":"single","q_text":"As part of your data pipeline, you are loading CSV files from Cloud Storage to BigQuery. However, these files have known data quality issues, such as mismatched data types and inconsistent formatting. You want to ensure data quality is maintained and perform the required cleansing and transformation. Which approach should you take?","answers":[{"ans":"Create a table with the desired schema, load the CSV files into the table, and perform the transformations in place using SQL","val":false},{"ans":"Use Data Fusion to convert the CSV files to a self-describing data format, such as AVRO, before loading the data to BigQuery.","val":false},{"ans":"Use Data Fusion to transform the data before loading it into BigQuery","val":true},{"ans":"Load the CSV files into a staging table with the desired schema, perform the transformations with SQL, and then write the results to the final destination table.","val":false}],"q_expl":"Data Fusion is a fully managed, cloud-native, enterprise data integration service for quickly building and managing data pipelines. The Cloud Data Fusion web UI lets you to build scalable data integration solutions to clean, prepare, blend, transfer, and transform data, without having to manage the infrastructure https:\/\/cloud.google.com\/data-fusion\/docs\/concepts\/overview When there is a service in GCP to automate the requirements, we can\u2018t consider other options."},{"label":"test_17","q_format":"single","q_text":"You are part of a team that is working on a niche product in the image recognition domain. Your team has developed a model that relies heavily on custom C++ TensorFlow ops for performing bulky matrix multiplications. As a result, it takes several days to train the model, and you want to reduce this time and keep the cost low by using an accelerator on Google Cloud. What should you do?","answers":[{"ans":"Use Cloud TPUs after implementing GPU kernel support for your custom ops","val":false},{"ans":"Use Cloud TPUs without any additional adjustments to your code.","val":false},{"ans":"Stay on CPUs and increase the size of the cluster you are training your model on.","val":false},{"ans":"Use Cloud GPUs after implementing GPU kernel support for your custom ops.","val":true}],"q_expl":"Ref: https:\/\/cloud.google.com\/tpu\/docs\/tpus#when_to_use_tpus"},{"label":"test_17","q_format":"single","q_text":"A financial services company is required to keep audit records for at least seven years. The data is unlikely to be accessed but must be kept anyway. The company has been storing this data in an on-premises file system but the CIO wants to a lower cost solution. The company is migrating several workloads to Google Cloud and is considering a Google Cloud-based solution. What would you recommend?","answers":[{"ans":"Cloud Storage Coldline storage","val":false},{"ans":"Cloud Storage Dual-Region storage","val":false},{"ans":"Cloud Storage Multi-Region storage","val":false},{"ans":"Cloud Storage Archive class storage","val":true}],"q_expl":"Cloud Storage Archive class storage is the best choice for this kind of long term, low frequency access storage requirement. Coldline storage could be used but Archive class storage costs less. Multi-region and dual-region storage are not required according to the problem description and would cost more. See https:\/\/cloud.google.com\/storage\/docs\/storage-classes"},{"label":"test_17","q_format":"single","q_text":"You have to migrate a large volume of data from an on-premises data store to Cloud Storage. You want to add metadata tags to objects with personally identifiable information. What two Google Cloud managed services could you use to accomplish this?","answers":[{"ans":"Data Loss Prevention and Data Catalog","val":true},{"ans":"Data Catalog and Cloud Firestore","val":false},{"ans":"Data Loss Prevention and Compute Engine","val":false},{"ans":"Compute Engine and Data Catalog","val":false}],"q_expl":"Data Loss Prevention can identify personal identifiable information and Data Catalog can assign and store metadata tags to objects. Compute Engine could be used with custom applications but it is not a managed service. Cloud Firestore is a document database and could be used for storage but Data Catalog is a better option because it is designed specifically for this kind of use case. See https:\/\/cloud.google.com\/dlp and https:\/\/cloud.google.com\/data-catalog"},{"label":"test_17","q_format":"single","q_text":"A European health care company uses Cloud Pub\/Sub as part of a data processing pipeline. The CTO of the company is concerned that data might accidentally be written to a region outside the European Union, which would violate the GDPR regulation. What would you recommend the company does to ensure data stays within Google Cloud regions in the European Union?","answers":[{"ans":"Set a Resource Location Restriction organization policy to ensure all buckets are stored only in acceptable regions.","val":false},{"ans":"Only define Cloud Pub\/Sub endpoints in acceptable regions when creating subscriptions.","val":false},{"ans":"Only define Cloud Pub\/Sub endpoints in acceptable regions when creating topics.","val":false},{"ans":"Set a Resource Location Restriction organization policy to ensure all topics are stored only in acceptable regions.","val":true}],"q_expl":"The correct answer is to set a Resource Location Restriction organization policy to ensure all topics are stored only in acceptable regions. Topics, not buckets, store messages in Cloud Pub\/Sub. Users of Cloud Pub\/Sub do not create endpoints; it is a globally managed service that does not require any endpoint configuration by users of Cloud Pub\/Sub. see https:\/\/cloud.google.com\/resource-manager\/docs\/organization-policy\/defining-locations"},{"label":"test_17","q_format":"multiple","q_text":"You are in the process of creating lifecycle policies to manage objects stored in Cloud storage. Which of the following are lifecycle conditions you can use in your policies? (Choose 3)","answers":[{"ans":"Age","val":true},{"ans":"Matches Storage Class","val":true},{"ans":"File type","val":false},{"ans":"Is Live","val":true},{"ans":"File size","val":false}],"q_expl":"The correct answers are age, matches storage class, and is live. File type and file size are not conditions available in lifecycle management policies. See https:\/\/cloud.google.com\/storage\/docs\/lifecycle"},{"label":"test_17","q_format":"single","q_text":"You have created a function that should run whenever a message is written to a Cloud Pub\/Sub topic. What command would you use to deploy that function?","answers":[{"ans":"gcloud pubsub topics publish","val":false},{"ans":"gcloud pubsub topics pull","val":false},{"ans":"gcloud pubsub subscription publish","val":false},{"ans":"gcloud functions deploy","val":true}],"q_expl":"The correct command is gcloud functions deploy. Gcloud pubsub topics publish publishes a message to a topic. The others are not valid gcloud pubsub commands. See https:\/\/cloud.google.com\/sdk\/gcloud\/reference\/functions\/deploy"},{"label":"test_17","q_format":"single","q_text":"The CIO of a online gaming company is concerned with the increasing cost of maintaining a MongoDB database used to store player game data. What managed service in Google Cloud would you recommend as an alternative option to MongoDB?","answers":[{"ans":"Cloud Dataproc","val":false},{"ans":"Cloud SQL","val":false},{"ans":"Bigtable","val":false},{"ans":"Cloud Firestore","val":true}],"q_expl":"MongoDB is a NoSQL database that uses a document model so Cloud Firestore is a good option for a managed service. Cloud SQL is a relational database. Cloud Dataproc is a managed Spark and Hadoop service, not a database. Bigtable is a NoSQL database but it is a wide column database, not a document database. See https:\/\/cloud.google.com\/firestore"},{"label":"test_17","q_format":"single","q_text":"Your team is deploying a new data pipeline. Developers who will maintain the pipeline will need permissions granted by three different roles. Those roles also have permissions that are not needed by the maintainers. Following Google Cloud recommended practices, what would you recommend?","answers":[{"ans":"Assign the three existing roles to the maintainers in order to minimize role management overhead.","val":false},{"ans":"Assign the Owner role instead of the three roles to minimize role management overhead.","val":false},{"ans":"Create a custom role with only the permissions needed. This follows the principal of least privilege.","val":true},{"ans":"Create a custom group with all the permissions in the three different roles. This follows the principle of maximum privilege.","val":false}],"q_expl":"Creating a custom role with only the permissions needed is the correct answer. This follows the principle of least privilege. Permissions are assignee to roles not groups. The Owner role is a primitive role that grants excessive privileges and should only be used in limited cases when security risks are minimal. Assigning the three existing roles would grant more permissions than needed and would violate the principle of least privilege. There is no principle of maximum privilege. https:\/\/cloud.google.com\/blog\/products\/identity-security\/dont-get-pwned-practicing-the-principle-of-least-privilege"},{"label":"test_17","q_format":"single","q_text":"You are migrating a data warehouse from on-premises to Google Cloud. Users of the data warehouse are concerned that they will not have access to highly performant, in memory analysis. What service would you suggest to have comparable features and performance in Google Cloud?","answers":[{"ans":"BigQuery BI Engine","val":true},{"ans":"BigQuery Cloud Memorystore with memcached","val":false},{"ans":"BigQuery with Cloud Memorystore using Redis","val":false},{"ans":"Bigtable with BI Engine","val":false}],"q_expl":"BigQuery BI Engine is an in-memory analytics engine. Cloud Memorystore is a cache and better suited to storing key-value data for applications that need low latency access to data. There is no Bigtable BI Engine service. See https:\/\/cloud.google.com\/bigquery\/docs\/bi-engine-intro"},{"label":"test_17","q_format":"single","q_text":"An online gaming company has used a normalized database to manage players\u2018 in-game possessions but it is difficult to maintain because the schema has to change frequently to support new game features and types of possessions. What kind of data model would you recommend instead of a normalized data model?","answers":[{"ans":"Document model","val":true},{"ans":"Star schema","val":false},{"ans":"Network model","val":false},{"ans":"Snowflake schema","val":false}],"q_expl":"A document model supports semi-structured schemas that frequently change. Both a star schema and snowflake schema are denormalized relational data models used in data warehousing but would not meet the needs of an interactive game. A network model is used to model graph-like structures such as transportation networks and is not as good a fit for the requirements as a document model. See https:\/\/firebase.google.com\/docs\/firestore\/data-model"},{"label":"test_17","q_format":"single","q_text":"What is the recommended approach for designing storage for very large text files in a data pipeline on Google Cloud that supports ANSI SQL queries, compression, and parallel loading from input locations based on Google\u2018s best practices. Which of the following options is the most appropriate?","answers":[{"ans":"Use the Grid Computing Tools to compress text files to gzip format and store the data in BigQuery for querying","val":false},{"ans":"Use Cloud Dataflow to convert text files to compressed Avro format and store the data in BigQuery for querying.","val":false},{"ans":"Use Cloud Dataflow to convert text files to compressed Avro format and store the data in Cloud Storage, with BigQuery permanent linked tables for querying.","val":true},{"ans":"Use the Grid Computing Tools to compress text files to gzip format and store the data in Cloud Storage, then import the data into Cloud Bigtable for querying.","val":false}],"q_expl":"The recommended approach for designing storage for very large text files in a data pipeline on Google Cloud that supports ANSI SQL queries, compression, and parallel loading from input locations based on Google\u2018s best practices is Option B: Use Cloud Dataflow to convert text files to compressed Avro format and store the data in Cloud Storage, with BigQuery permanent linked tables for querying. Option A is not the best choice because using BigQuery as storage may lead to additional costs and is less scalable than Cloud Storage. Option C is not recommended since gzip is not an optimized format for querying in BigQuery Option D is not the best choice because Cloud Bigtable is a NoSQL database that is not optimized for querying with ANSI SQL."},{"label":"test_17","q_format":"single","q_text":"A team of data analysts want to run a series of jobs on large data sets. There is a complicated set of dependencies between the jobs. They want to use a managed service if possible. Which of the following would you recommend they try?","answers":[{"ans":"Write Airflow directed acyclic graphs in SQL and execute them with Cloud Workflows.","val":false},{"ans":"Write Airflow directed acyclic graphs in SQL and execute them with Cloud Composer.","val":false},{"ans":"Write Airflow directed acyclic graphs in Python and execute them with Cloud Composer.","val":true},{"ans":"Write Airflow directed acyclic graphs in Python and execute them with Cloud Workflows.","val":false}],"q_expl":"The correct answer is to write Airflow directed acyclic graphs in Python and execute them with Cloud Composer. Cloud Composer does not support writing directed acyclic graphs in SQL. Cloud Workflows is used with API workflows, not complicated batch job workflows. See https:\/\/cloud.google.com\/composer"},{"label":"test_17","q_format":"single","q_text":"An industry regulation requires that when analyzing personal identifying information (PII), you must not run analysis on physical servers that are shared with other cloud customers. You plan to use Cloud Dataproc for analyzing data with PII. What will you need to do when creating a Cloud Dataproc Cluster to ensure you are in compliance with this regulation?","answers":[{"ans":"Create an unmanaged instance group and specify that instance group when creating the cluster.","val":false},{"ans":"Disable autoscaling to prevent the addition of non-sole tenant VMs.","val":false},{"ans":"You cannot configure Cloud Dataproc to use sole tenant nodes. You will need to run Spark in a Compute Engine managed instance group that you manage yourself.","val":false},{"ans":"Create a sole-tenant node group and specify that node group when creating the cluster.","val":true}],"q_expl":"The correct answer is to create a sole-tenant node group and specify that node group when creating the cluster. Cloud Dataproc does support sole tenants so you don\u2018t need to run a self-managed Spark cluster. You can use autoscaling with sole tenant node groups. Unmanaged instance groups are not required and not recommend except for legacy, heterogeneous clusters migrating to Compute Engine. https:\/\/cloud.google.com\/dataproc\/docs\/concepts\/configuring-clusters\/sole-tenant-nodes"},{"label":"test_18","q_format":"single","q_text":"As a database administrator, you are finding that a Cloud SQL database using PostgreSQL is not meeting read operation SLAs. You want to improve performance with minimal changes to database applications. What would you try first to improve read performance?","answers":[{"ans":"Use Cloud Memorystore to cache data to be read.","val":false},{"ans":"Use change data capture to keep a second database in a different region in synch with the primary database while having read operations sent to the secondary database.","val":false},{"ans":"Use PostgreSQL\u2018s explain plan feature to analyze queries and rewrite them to improve performance.","val":false},{"ans":"Create a read replica.","val":true}],"q_expl":"The correct answer is to create a read replica for read operations. Using a cache could improve read performance but would require changes to database applications. Using a secondary database would require changes to the application to read from the secondary database instead of the primary. Using explain plan could help with query optimization but that also requires changes to database applications. See https:\/\/cloud.google.com\/sql\/docs\/mysql\/replication"},{"label":"test_18","q_format":"single","q_text":"A machine learning model has been containerized and deployed on Kubernetes Engine. It is currently deployed with 2 replicas. You need to increase the number of replicas to 4. What command would you use?","answers":[{"ans":"kubectl scale deployment command with the --replicas 4 parameter.","val":true},{"ans":"kubectl scale deployment command with the --deploy 2 parameter.","val":false},{"ans":"kubectl scale deployment command with the --deploy 4 parameter.","val":false},{"ans":"kubectl scale deployment command with the --replicas 2 parameter.","val":false}],"q_expl":"The kubectl scale deployment with the \u2013replicas 4 parameter will scale the deployment to 4 replicas. There is no \u2013deploy parameter in the kubectl scale deployment command. See https:\/\/kubernetes.io\/docs\/concepts\/workloads\/controllers\/deployment\/"},{"label":"test_18","q_format":"single","q_text":"A global transportation company is using Cloud Spanner for managing shipping orders. They have migrated an Oracle database to Cloud Spanner with minimal changes and are experiencing similar performance problems with joins. In particular, a one-to-many join between an orders table and an order items table is not performing as needed. What would you recommend?","answers":[{"ans":"Use interleaved tables","val":true},{"ans":"Use Cloud SQL for better join performance","val":false},{"ans":"Use Cloud Bigtable for better join performance","val":false},{"ans":"Use interleaved hashes","val":false}],"q_expl":"Interleaved tables store parent and children records together, such as orders and order items. This is more efficient than storing related items separately since the parent and child data can be read at the same time. Cloud Bigtable is a NoSQL database and would not meet requirements. Cloud SQL does not scale beyond regional-scale databases and would not meet requirements. There is no such thing as interleaved hashes in Cloud Spanner. See https:\/\/cloud.google.com\/spanner\/docs\/schema-and-data-model and https:\/\/cloud.google.com\/spanner\/docs\/whitepapers\/optimizing-schema-design"},{"label":"test_18","q_format":"single","q_text":"The performance of an application that uses Bigtable is starting to degrade as volumes grow. You suspect your row key design may not be optimal. You want to review access patterns for a group of row keys. What tool would you use?","answers":[{"ans":"Cloud Key Trace","val":false},{"ans":"Cloud Monitoring","val":false},{"ans":"Cloud Logging","val":false},{"ans":"Key Visualizer","val":true}],"q_expl":"Key Visualizer is a Bigtable tool for analyzing Bigtable usage patterns. Cloud Monitoring can be used for performance monitoring but it is not specifically designed to diagnose row key design problems. Cloud Logging can help when reviewing events in Bigtable but it is not designed to support row key design problems. There is no Google Cloud tool called Cloud Key Trace. See https:\/\/cloud.google.com\/bigtable\/docs\/keyvis-overview"},{"label":"test_18","q_format":"single","q_text":"As part of the ingestion process, you want to ensure any messages written to a Cloud Pub\/Sub topic all have a standard structure. What is the recommended way to ensure messages have the standard structure?","answers":[{"ans":"Use a data quality function in Cloud Function to check the structure as it is written to Cloud Pub\/Sub.","val":false},{"ans":"Create a schema and assign it to a topic during topic creation.","val":true},{"ans":"Use a data quality function in Cloud Function to reformat the message if needed before it is read from a subscription.","val":false},{"ans":"Create a schema and assign it to a subscription during subscription creation.","val":false}],"q_expl":"Schemas are used to define a standard message structure and they are assigned to topics during creation. Schemas are not assigned to subscription. Cloud Functions should not be used to implement a feature that is available in Cloud Pub\/Sub. Cloud Functions support only one type of Pub\/Sub event, google.pubsub.topic.publish. See https:\/\/cloud.google.com\/pubsub\/docs\/schemas"},{"label":"test_18","q_format":"single","q_text":"How can you query data for the past 30 days from the Firebase Analytics integration with Google BigQuery, where daily tables in the format app_events_YYYYMMDD are automatically created and the query is required to use legacy SQL?","answers":[{"ans":"Use WHERE date BETWEEN YYYY-MM-DD AND YYYY-MM-DD.","val":false},{"ans":"Use the WHERE_PARTITIONTIME pseudo column.","val":false},{"ans":"Use the TABLE_DATE_RANGE function.","val":true},{"ans":"Use SELECT IF(date >= YYYY-MM-DD AND date <= YYYY-MM-DD.","val":false}],"q_expl":"https:\/\/cloud.google.com\/bigquery\/docs\/reference\/legacy-sql"},{"label":"test_18","q_format":"single","q_text":"A insurance claim review company provides expert opinion on contested insurance claims. The company uses Google Cloud for it\u2018s data analysis pipelines. Clients of the company upload documents to Cloud Storage. When a file is uploaded, the company wants to immediately move the files to a Classified Data bucket if the file contains personally identifying information. What method would you recommend to accomplish this?","answers":[{"ans":"Create a quarantine bucket for uploading, once a file is uploaded trigger a Cloud Function to call the Data Loss Prevention API to apply infotypes to detect PII. If PII is detected, move file to the Classified Data bucket.","val":true},{"ans":"Create a quarantine bucket for uploading, once a file is uploaded trigger a Cloud Function to call a custom built machine learning model trained to detect PII. If PII is detected, move the file to the Classified Data bucket.","val":false},{"ans":"Create a quarantine bucket for uploading, use Cloud Scheduler to run a job to run hourly that will call a custom built machine learning model trained to detect PII. If PII is detected, move file to the Classified Data bucket.","val":false},{"ans":"Create a quarantine bucket for uploading, use Cloud Scheduler to run a job to run hourly that will call the Data Loss Prevention API to apply infotypes to detect PII. If PII is detected, move file to the Classified Data bucket.","val":false}],"q_expl":"The correct solution is to use a quarantine bucket that triggers a Cloud Function on upload to invoke the DLP API and move the file if PII is found. Cloud Scheduler runs jobs at regular intervals but this calls for immediate processing of a file once uploaded so Cloud Functions should be used. You could train a custom machine learning model but that requires development time and maintenance. A managed service like DLP is a better option. See https:\/\/cloud.google.com\/dlp\/docs\/reference\/rest"},{"label":"test_18","q_format":"single","q_text":"A retailer has been using Kubernetes to deploy new applications built on microservices architectures. They now want to start building machine learning pipelines while leveraging their expertise in Kubernetes. What service would you recommend for running machine learning workflows on Kubernetes?","answers":[{"ans":"AutoML Tables","val":false},{"ans":"Vertex AI","val":false},{"ans":"Cloud Composer","val":false},{"ans":"Kubeflow","val":true}],"q_expl":"The correct answer is Kubeflow, an open source tool for running ML pipelines in Kubernetes. Vertex AI is a set of managed machine learning services in Google Cloud. AutoML Tables is a managed ML service specifically for structured data. Cloud Composer is a workflow orchestration service. See https:\/\/cloud.google.com\/blog\/products\/ai-machine-learning\/getting-started-kubeflow-pipelines"},{"label":"test_18","q_format":"single","q_text":"A group of analysts is migrating a Hadoop cluster from on premises to GCP. They want to follow Google Cloud recommended best practices. What should they do as part of the migration?","answers":[{"ans":"Use ephemeral clusters and Cloud Storage instead of HDFS on local storage.","val":true},{"ans":"Continually run clusters and use Cloud Storage instead of HDFS on local storage.","val":false},{"ans":"Continually run clusters and use HDFS on local storage.","val":false},{"ans":"Use ephemeral clusters and use HDFS on local storage.","val":false}],"q_expl":"Google Cloud recommends using ephemeral clusters. Since clusters start quickly you do not need to keep clusters running to avoid long startup time. Google Cloud also recommends using Cloud Storage to store persistent data so data does not have to be copied from a cluster before shutting down and then copied back to a cluster when starting a new cluster. https:\/\/cloud.google.com\/blog\/topics\/developers-practitioners\/dataproc-best-practices-guide"},{"label":"test_18","q_format":"single","q_text":"How should a financial services company store 50 TB of frequently updated financial time-series data in the cloud while also accommodating new data streaming in constantly, and also migrate their current Apache Hadoop jobs to the cloud for data analysis. Which cloud product would be best suited for their needs?","answers":[{"ans":"Cloud Bigtable","val":true},{"ans":"Google Cloud Storage","val":false},{"ans":"Google Cloud Datastore","val":false},{"ans":"Google BigQuery","val":false}],"q_expl":"Cloud Bigtable (Option A) is a NoSQL wide-column database service that is optimized for massive scale and low latency. It can be a good choice for real-time analytics and time-series data"},{"label":"test_18","q_format":"single","q_text":"How can you design a solution to transfer log data from your set of YouTube channels into Google Cloud for analysis. Your design should allow your global marketing teams to perform ANSI SQL and other types of analysis on the latest YouTube channel log data.?","answers":[{"ans":"Use BigQuery Data Transfer Service to transfer the offsite backup files to a Cloud Storage Regional storage bucket as a final destination","val":false},{"ans":"Use Storage Transfer Service to transfer the offsite backup files to a Cloud Storage Multi-Regional storage bucket as a final destination","val":true},{"ans":"Use Storage Transfer Service to transfer the offsite backup files to a Cloud Storage Regional bucket as a final destination.","val":false},{"ans":"Use BigQuery Data Transfer Service to transfer the offsite backup files to a Cloud Storage Multi-Regional storage bucket as a final destination.","val":false}],"q_expl":"This question is a bit tricky, and I\u2018ll provide the answer by eliminating the incorrect options: The keywords \u201cGlobal Marketing Team\u201c suggest worldwide access to the data. The question doesn\u2018t specify a target platform other than GCP. If we use BigQuery transfer service, it will only transfer data to BigQuery and won\u2018t support other targets, limiting the team\u2018s ability for different types of analysis. So options C and D are ruled out. All the options point to Google Cloud Storage (GCS). Considering the need for \u201cworldwide\u201c access and the absence of a specific regional requirement, option A is the most appropriate because it mentions \u201cmulti-regional\u201c storage. Option B is less suitable as it references \u201cRegional,\u201c which implies a more limited scope. Destination is GCS and having multi-regional so A is the best option available. Even since BigQuery Data Transfer Service initially supports Google application sources like Google Ads, Campaign Manager, Google Ad Manager and YouTube but it does not support destination anything other than bq data set"},{"label":"test_18","q_format":"multiple","q_text":"What are two recommended actions to ensure that existing applications can connect to the events data in Google BigQuery next month? The majority of the data analyzed is in a time-partitioned table called events_partitioned. To reduce query costs, a view named events was created that queries only the last 14 days of data and is described in legacy SQL.","answers":[{"ans":"Create a new view over events using standard SQL.","val":false},{"ans":"Create a new view over events_partitioned using standard SQL.","val":true},{"ans":"Create a service account for the ODBC connection to use for authentication.","val":true},{"ans":"Create a Google Cloud Identity and Access Management (Cloud IAM) role for the ODBC connection and shared events","val":false},{"ans":"Create a new partitioned table using a standard SQL query.","val":false}],"q_expl":"C = A standard SQL query cannot reference a view defined using legacy SQL syntax. D = For the ODBC drivers is needed a service account which will get a standard Bigquery role."},{"label":"test_18","q_format":"single","q_text":"What data structure in the Cloud Firestore document data model is analogous to a row in a relational database?","answers":[{"ans":"Interleaved row","val":false},{"ans":"Entity","val":true},{"ans":"Kinds","val":false},{"ans":"Index","val":false}],"q_expl":"Entities are analogous to rows in relational data models, both of which describe a single modeled element. Kinds are collections of related entities and analogous to a table in relational data models. An index is used to implement efficient querying in both Cloud Firestore and relational databases. There is no such thing as an interleaved row; interleaved tables are a feature of Cloud Spanner which improves query performance by storing related data together. See https:\/\/firebase.google.com\/docs\/firestore\/data-model"},{"label":"test_18","q_format":"single","q_text":"A data scientist is just learning to use Google Cloud for analytics. They would like to perform data quality checks and exploratory analysis on data sets stored in Cloud Storage. What Google Cloud service would you recommend they use?","answers":[{"ans":"Vertex AI","val":false},{"ans":"Cloud Dataproc","val":false},{"ans":"Cloud Dataprep","val":true},{"ans":"Cloud Dataflow","val":false}],"q_expl":"Cloud Dataprep is a managed service for preparing data for analysis and machine learning, including exploratory analysis. Cloud Dataflow is used for stream and batch processing. Cloud Dataproc is a managed Spark and Hadoop cluster service. Vertex AI is a comprehensive machine learning service. See https:\/\/cloud.google.com\/dataprep\/docs\/concepts"},{"label":"test_18","q_format":"single","q_text":"A financial services company wants to use BigQuery for data warehousing and analytics. The company is required to ensure encryption keys are stored and managed in a key management system that\u0092s deployed outside of a public cloud. They want to minimize the management overhead of key management while remaining in compliance. What would you recommend they do?","answers":[{"ans":"Use Dataproc for external data management, specifically keys","val":false},{"ans":"Use external data sources with BigQuery and encrypt the external data sources outside of Google Cloud","val":false},{"ans":"Use Data Catalog for external data management, specifically keys","val":false},{"ans":"Use Cloud EKM for external key management","val":true}],"q_expl":"The correct answer is to use External Key Management, it allows the company to maintain separation between data in BigQuery and their encryption keys. Data Catalog is a metadata and data discovery service, not a key management service. BigQuery external data sources allow for accessing data not stored in BigQuery and do not address the requirements. Cloud Dataproc is a managed Spark and Hadoop service, not a key management service. See https:\/\/cloud.google.com\/kms\/docs\/ekm"},{"label":"test_18","q_format":"single","q_text":"An IoT service uses Bigtable to store timeseries data. You have noticed that write operations tend to happen on one node at a time rather than being evenly distributed across nodes. What could be the cause of this problem?","answers":[{"ans":"Using the wrong type of GCP load balancer in front of Bigtable","val":false},{"ans":"Using a row key that causes data that arrives close in time to be written to a single node, rather than evenly distributed.","val":true},{"ans":"Misconfiguring replication","val":false},{"ans":"Using too many columns in your data model","val":false}],"q_expl":"This is an example of hot spotting, where workload is skewed toward a small number of nodes instead of evenly distributed. In Bigtable, this can be caused by row keys that are lexically close to each other and generated close in time. Bigtable distributes write operations based on the row key, not one of the GCP load balancers. Replication does not impact where data is originally written. Bigtable is a wide column database and can support a large number of columns and the number of columns does not affect the distribution of data across nodes. See https:\/\/cloud.google.com\/bigtable\/docs\/performance"},{"label":"test_18","q_format":"single","q_text":"You are using Cloud Dataflow to process data that is represented as key value pairs. What Apache Beam construct will you likely use in your workflow?","answers":[{"ans":"PCollections","val":true},{"ans":"User-defined function (UDF)","val":false},{"ans":"Database connection","val":false},{"ans":"Watermark","val":false}],"q_expl":"The correct answer is a PCollection, which is a representation of key value pair data. A database connection may be used but it is not necessarily required. A watermark is used with streaming data but there is no indication it is needed for this use case. User defined functions may be used but there is no requirement for functionality not already provided by Cloud Dataflow. See https:\/\/cloud.google.com\/dataflow\/docs\/concepts\/beam-programming-model"},{"label":"test_18","q_format":"single","q_text":"A project sponsor wants to develop a machine learning model to classify potentially fraudulent transactions. They want to rank models based on a combination of precision and recall. What evaluation metric would you recommend?","answers":[{"ans":"F-score","val":true},{"ans":"Mean squared error","val":false},{"ans":"Feature crosses","val":false},{"ans":"Root mean squared error","val":false}],"q_expl":"F-score is the harmonic mean of precision and recall and is often used to measure the overall performance of classification models. Root mean squared error and mean squared error are used to evaluate regression models. Feature crosses is a way to generate synthetic features, not measure the performance of machine learning models. See https:\/\/cloud.google.com\/automl-tables\/docs\/evaluate"},{"label":"test_18","q_format":"multiple","q_text":"You are developing a distributed system and want to decouple two services. You want to ensure messages use a standard format and you plan to use Cloud Pub\/Sub. What schema types are supported by Cloud Pub\/Sub? (Choose 2)","answers":[{"ans":"Thrift","val":false},{"ans":"Protocol Buffer","val":true},{"ans":"CSV","val":false},{"ans":"Avro","val":true},{"ans":"Parquet","val":false}],"q_expl":"Cloud Pub\/Sub supports Avro and Protocol Buffer schemas. Thrift is an alternative to Protocol Buffers but is not supported for schemas. Parquet is an open source file format used in Hadoop. CSV is a file format often used when sharing data between applications. See https:\/\/cloud.google.com\/pubsub\/docs\/schemas"},{"label":"test_18","q_format":"single","q_text":"A financial services company is using a single Bigtable cluster to store data about equity prices. There is a large volume of write operations during the trading day. There are also analytic batch jobs that run through the day. You have been hired to help optimize the performance of Bigtable. What would you recommend they do?","answers":[{"ans":"Continue to write data to Bigtable but create a Cloud Dataflow job to copy data to a Cloud Spanner data warehouse for batch operations.","val":false},{"ans":"Isolate the write and batch workloads by adding a second cluster to the Bigtable instance and create two app profiles, one for write traffic and one for batch jobs.","val":true},{"ans":"Continue to write data to Bigtable but create a Cloud Dataflow job to copy data to a Cloud Firestore data warehouse for batch operations.","val":false},{"ans":"Isolate the write and batch workloads by adding a second set of tables to the Bigtable instance and write the data needed by batch jobs to the second set of tables.","val":false}],"q_expl":"To isolate batch analytics jobs from other operations in Bigtable, Google Cloud recommends using two clusters in a single instance and using app profile to route operations to the appropriate cluster. Cloud Spanner and Cloud Datastore are not designed for data warehousing. A second set of tables would be managed by the same set of nodes and not reduce the total workload on the nodes in the cluster. See https:\/\/cloud.google.com\/bigtable\/docs\/performance"},{"label":"test_18","q_format":"multiple","q_text":"What types of indexes are automatically created in Cloud Firestore? (Choose 2)","answers":[{"ans":"Hash indexes","val":false},{"ans":"Atomic values, descending","val":true},{"ans":"Composite indexes, single value","val":false},{"ans":"Atomic values, ascending","val":true},{"ans":"Composite indexes, multi-value","val":false}],"q_expl":"Cloud Firestore automatically creates atomic value ascending and descending indexes. A composite index is made up of two or more values and are not created manually. There is no single valued composite index; all composite indexes have multiple values. There isn\u2018t a hash index type in Cloud Firestore. sEe https:\/\/firebase.google.com\/docs\/firestore\/query-data\/index-overview"},{"label":"test_18","q_format":"single","q_text":"A data engineer needs to load data stored in Avro files in Cloud Storage into Bigtable. They would like to have a reliable, easily monitored process for copying the data. What would you recommend they use to copy the data?","answers":[{"ans":"Custom Python 3 program","val":false},{"ans":"Storage Transfer Service","val":false},{"ans":"Cloud Dataflow, starting with a Cloud Storage Avro to Bigtable template.","val":true},{"ans":"gsutil","val":false}],"q_expl":"The correct answer is to use Cloud Dataflow with a Cloud Storage Avro to Bigtable template. Using Python 3 would require more work than necessary. Gsutil is used to load data into Cloud Storage, not Bigtable. Storage Transfer Service is for copying data into Cloud Storage from other object storage system, such as AWS S3. A custom Python 3 program would require more development effort than using Cloud Dataflow. See https:\/\/cloud.google.com\/architecture\/streaming-avro-records-into-bigquery-using-dataflow"},{"label":"test_18","q_format":"single","q_text":"A developer is deploying a Cloud SQL database to production and wants to follow Google Cloud recommended best practices. What should the developer use for authentication?","answers":[{"ans":"Strong encryption","val":false},{"ans":"Cloud Identity","val":false},{"ans":"IAM","val":false},{"ans":"Cloud SQL Auth proxy","val":true}],"q_expl":"Cloud SQL Auth proxy is the recommended way to connect to Cloud SQL. Cloud Identity is an Identity as a Service provided by Google Cloud. IAM is Identity and Access Management service for managing identities and their authorizations. Strong encryption is used to protect the confidentiality and integrity of data, not to perform authentication. See https:\/\/cloud.google.com\/sql\/docs\/mysql\/sql-proxy"},{"label":"test_18","q_format":"single","q_text":"You have a latency-sensitive application that uses Bigtable. You want to follow Google Cloud recommended best practices. What would you do?","answers":[{"ans":"Use a service account for all read operations","val":false},{"ans":"Keep storage utilization per node below 60%","val":true},{"ans":"Use a global load balancer in front of Bigtable","val":false},{"ans":"Use HDD storage instead of SSD storage","val":false}],"q_expl":"For low latency applications, Google Cloud recommends keeping storage utilization below 60%. Using service accounts for read operations will not affect latency of write operations. HDD storage is less performant than SSD storage. A load balancer is not needed to distribute workload within a Bigtable cluster. Bigtable distributes operations based on row keys. See https:\/\/cloud.google.com\/blog\/products\/databases\/check-out-how-to-optimize-database-service-cloud-bigtable"},{"label":"test_18","q_format":"single","q_text":"You want to monitor a Cloud Dataflow job and know the maximum duration that an item has been waiting in the pipeline. What Cloud Monitoring metric would you use to track the maximum duration?","answers":[{"ans":"job\/element_count","val":false},{"ans":"job\/system_lag","val":true},{"ans":"job\/data_watermark_age","val":false},{"ans":"job\/elapsed_time","val":false}],"q_expl":"The correct answer is job\/system_lag. The job\/data_watermark_age is the age of the most recent item that\u2018s been fully processed by the pipeline. Job\/elapsed_time is the elapsed time of the pipeline run time. Job\/element_count is the number of items processed in a PCollection for the Read_input and Process_element transforms. See https:\/\/cloud.google.com\/architecture\/building-production-ready-data-pipelines-using-dataflow-monitoring"},{"label":"test_18","q_format":"single","q_text":"What is the best course of action to take when developing an application on Google Cloud that needs to automatically generate subject labels for users\u2018 blog posts, given competitive pressure to add this feature quickly and no additional developer resources, and no one on the team has machine learning experience. Which of the following options is the most suitable?","answers":[{"ans":"Develop and train a text classification model using TensorFlow, deploy it using a Kubernetes Engine cluster, call the model from the application, and process the results as labels","val":false},{"ans":"Utilize the Cloud Natural Language API in the application and process the generated Sentiment Analysis as labels.","val":false},{"ans":"Develop and train a text classification model using TensorFlow, deploy it using Cloud Machine Learning Engine, call the model from the application, and process the results as labels.","val":false},{"ans":"Utilize the Cloud Natural Language API in the application and process the generated Entity Analysis as labels","val":true}],"q_expl":"The Cloud Natural Language API is a pre-trained machine learning model that can perform entity analysis, sentiment analysis, and other natural language processing tasks. By utilizing the Entity Analysis feature, the API can automatically identify and extract relevant subject labels from the text, making it a quick and easy solution to implement without the need for additional development or machine learning expertise. Option B is not recommended since Sentiment Analysis is focused on analyzing the tone of the text and may not be suitable for generating subject labels. Options C and D require expertise in developing and training a text classification model using TensorFlow, which may not be feasible given the limited developer resources and competitive pressure to quickly add the feature"},{"label":"test_18","q_format":"single","q_text":"How should you design storage for 20 TB of text files when deploying a data pipeline on Google Cloud in order to minimize the cost of querying aggregate values for multiple users who will query the data in Cloud Storage with multiple engines, given that your input data is in CSV format. (Which storage service and schema design should be utilized)?","answers":[{"ans":"Use Cloud Bigtable for storage and link the data as permanent tables in BigQuery for querying","val":false},{"ans":"Use Cloud Bigtable for storage and install the HBase shell on a Compute Engine instance to query the data.","val":false},{"ans":"Use Cloud Storage for storage and link the data as temporary tables in BigQuery for querying.","val":false},{"ans":"Use Cloud Storage for storage and link the data as permanent tables in BigQuery for querying.","val":true}],"q_expl":"The recommended solution for storing 20 TB of text files for a data pipeline on Google Cloud in order to minimize the cost of querying aggregate values for multiple users who will query the data in Cloud Storage with multiple engines, given that the input data is in CSV format, is to use Cloud Storage for storage and link the data as permanent tables in BigQuery for querying."},{"label":"test_18","q_format":"single","q_text":"How can you design the storage for two relational tables in a 10-TB database on Google Cloud to enable horizontally scalable transactions and optimize data for range queries on non-key columns. (Which option from the following alternatives should you choose)?","answers":[{"ans":"Use Cloud Spanner for storage and add secondary indexes to support query patterns","val":true},{"ans":"Use Cloud Spanner for storage and use Cloud Dataflow to transform data to support query patterns.","val":false},{"ans":"Use Cloud SQL for storage and add secondary indexes to support query patterns","val":false},{"ans":"Use Cloud SQL for storage and utilize Cloud Dataflow to transform data to support query patterns.","val":false}],"q_expl":"Option A, using Cloud SQL for storage and adding secondary indexes to support query patterns, may not be sufficient for scaling horizontally as Cloud SQL has limitations on scalability. In addition, adding too many indexes can negatively impact write performance. Option B, using Cloud SQL for storage and using Cloud Dataflow to transform data to support query patterns, may not be the most efficient solution as it requires additional processing overhead for transforming the data. Option D, using Cloud Spanner for storage and using Cloud Dataflow to transform data to support query patterns, may not be necessary as Cloud Spanner already has built-in scalability and can handle large datasets with high concurrency. However, adding secondary indexes to support query patterns is still a viable solution. Therefore, option C, using Cloud Spanner for storage and adding secondary indexes to support query patterns, is the most suitable option as it provides scalability and efficient querying with secondary indexes."},{"label":"test_18","q_format":"single","q_text":"As an analyst with a major metropolitan public transportation agency, you are tasked with monitoring data about passengers on all modes of transport provided by the agency. Since you know SQL, you would like to run a SQL query using Cloud Dataflow. What command allows you to run a SQL query and write results to a BigQuery table? (Assume all need parameters will be specified).","answers":[{"ans":"gcloud bigquery sql query","val":false},{"ans":"bq bigquery sql query","val":true},{"ans":"gcloud dataflow sql query","val":false},{"ans":"bq dataflow sql query","val":false}],"q_expl":"The correct command to run a SQL query and write results to a BigQuery table using Cloud Dataflow is:\nbq query \u2013destination_table [destination_table_name]\nHere\u2019s a breakdown of the command and its parameters:\n\nbq: This is the command-line tool for interacting with BigQuery.\nquery: This indicates that you want to run a SQL query.\n\u2013destination_table [destination_table_name]: This specifies the name of the BigQuery table where you want to write the query results.\n\nHere\u2019s an example of how you would use this command to run a SQL query and write the results to a BigQuery table named \u201cpassenger_data\u201d:\n\nBash\n\nbq query --destination_table passenger_data \"SELECT * FROM my_dataset.passengers\"\n\n\n Use code with caution.\n\n\nThis command will run the SQL query SELECT * FROM my_dataset.passengers and write the results to the BigQuery table \u201cpassenger_data\u201d.\nNote that you need to replace my_dataset with the actual name of your BigQuery dataset. Also, make sure that the query is syntactically correct and that the destination table has the appropriate schema to accommodate the query results.\nSee https:\/\/cloud.google.com\/sdk\/gcloud\/reference\/dataflow\/sql\/query"},{"label":"test_18","q_format":"single","q_text":"You are creating a set of Cloud Storage buckets for storing data that will be accessed by several different teams in your organization. The teams have different access requirements. You want to follow Google Cloud\u2018s recommended best practices. How would you implement access controls to objects and buckets?","answers":[{"ans":"Uniform bucket level access","val":true},{"ans":"Signed URLs","val":false},{"ans":"Fine-grained access controls","val":false},{"ans":"Signed policy documents","val":false}],"q_expl":"Uniformed access control is the recommended method and uses IAM to apply permissions to buckets or groups of objects. Fine-grained access control is a legacy method based on access control lists and is not recommended. Signed URLs are used for time-limited access to an object. Signed policy documents are used to control what can be uploaded to a bucket. See https:\/\/cloud.google.com\/storage\/docs\/uniform-bucket-level-access"},{"label":"test_18","q_format":"multiple","q_text":"Your BigQuery costs are higher than expected. You want to help data analysts using the BigQuery data warehouse reduce overall costs. Which of the following would you recommend? (choose 2)","answers":[{"ans":"Use LIMIT only with clustered tables","val":true},{"ans":"Avoid using partitioned tables","val":false},{"ans":"Use the bq --estimate-bytes command to estimate the number of bytes read","val":false},{"ans":"Avoid using clustered tables","val":false},{"ans":"Avoid using SELECT *","val":true}],"q_expl":"SELECT * can scan large amounts of data and should be avoided. Using LIMIT only with clustered tables can reduce the amount of data scanned. Using LIMIT on non-clustered tables does not limit the number of bytes scanned. Partitioning and clustering can both help limit the amount of data scanned and therefore help reduce costs. The bq command to estimate the number of bytes scanned is bq \u2013dry_run, not \u2013estimate_bytes. See https:\/\/cloud.google.com\/bigquery\/docs\/best-practices-performance-overview"},{"label":"test_18","q_format":"single","q_text":"A Web hosting company has been using a custom built data store modeled on the sparse multidimensional array data structure. The CIO no longer wants to pay to develop and maintain a custom data store. Instead, the CIO wants a managed database service if possible and if not, they want to use a well supported open source database that is also based on sparse multidimensional array data structure. The Web hosting company is already using Google Cloud Compute Engine, Cloud Storage, and Kubernetes Engine. What would you recommend to the company?","answers":[{"ans":"BigQuery","val":false},{"ans":"Cloud Spanner","val":false},{"ans":"Cloud Bigtable","val":true},{"ans":"Cassandra","val":false}],"q_expl":"Cloud Bigtable is a managed database service based on sparse multidimensional arrays and so meets the requirements, including using a managed database service. Cassandra is an open source database that is based on multidimensional sparse arrays but it is not a managed service and so not as good of a choice for the given requirements. BigQuery uses a compressed, columnar data model not a sparse multidimensional array and does not meet the requirements. Cloud Spanner is a global scale relational database and does not meet the requirements. See https:\/\/cloud.google.com\/bigtable\/docs\/overview"},{"label":"test_18","q_format":"single","q_text":"Epidemiology and infectious disease researchers are collecting data on the genomic sequences of several pathogens. The data is stored in a bioinformatics-specific format called FASTQ and are tens of gigabytes in size. They will eventually store several terabytes of FASTQ data. The data will be processed by Cloud Dataflow and results will be written to BigQuery. What is a good option for storing FASTQ data?","answers":[{"ans":"Cloud Storage","val":true},{"ans":"BigQuery","val":false},{"ans":"Bigtable","val":false},{"ans":"Cloud Firestore","val":false}],"q_expl":"The specialized data format in this scenario makes object storage a good option so Cloud Storage is the best choice. Cloud Firestore is a good option for document storage, such as JSON structures. BigQuery and Bigtable are not suited to store large objects. See https:\/\/cloud.google.com\/blog\/topics\/developers-practitioners\/map-storage-options-google-cloud"},{"label":"test_18","q_format":"single","q_text":"A manufacturer of delivery drones has been using a PostgreSQL database running in Compute Engine to store data. The company is growing and the database is not able to keep up with the ingestion of telemetry data from the drones. The CTO would like to use a managed database service that will provide low latency writes and scale to petabytes of data. The top priority is scalability and the CTO is willing to invest development time in changing the application if needed. What managed Google Cloud database service would you recommend?","answers":[{"ans":"BigQuery","val":false},{"ans":"Cloud SQL using PostgreSQL","val":false},{"ans":"Cloud Spanner","val":false},{"ans":"Cloud Bigtable","val":true}],"q_expl":"Cloud Bigtable is designed for low latency, high volume writes and scales to petabyte sized databases making it the best choice. There would likely be some changes to the application to work with Cloud Bigtable, which is a NoSQL database (PostgreSQL is a relational database). Cloud SQL using PostgreSQL will not scale to meet the requirements. BigQuery is an analytical database that can scale to petabytes but it is not designed for low latency writes such as needed in this application. Cloud Spanner can scale to petabytes and is a relational database but it is not designed for the kind of low latency writes needed here. Also, key features of Cloud Spanner, such as SQL query language and strong consistency are not mentioned in the requirements. See https:\/\/cloud.google.com\/bigtable\/docs\/overview"},{"label":"test_18","q_format":"single","q_text":"Machine learning engineers working in the us-central1 region have approximately 200 TB of data that will be used to train machine learning models. To train a model, only a small subset of that data is used. Data is organized into files that will be accessed about once per month. You would like to minimize storage costs but still have reliable and highly available storage. What would you recommend for storing this data?","answers":[{"ans":"Cloud Storage Nearline storage","val":true},{"ans":"Cloud Storage Multi-Region storage","val":false},{"ans":"Balanced Persistent Disks","val":false},{"ans":"SSD Persistent Disks","val":false}],"q_expl":"Cloud Storage Nearline storage is the best option for highly available data that is accessed once per month. Cloud Storage Multi-region meets the need but is more expensive and since the team is working in one region, multi-region storage is not necessary. Persistent disks are used with virtual machines and would require additional cost and operational overhead to store this data on persistent disk. Cloud Storage is a managed service that does not require as much operational overhead and would therefore cost less. See https:\/\/cloud.google.com\/storage\/docs\/storage-classes"},{"label":"test_18","q_format":"single","q_text":"Auditors have informed the CIO of your company that all logs from applications running in Google Cloud will need to be retained for 60 days. You would also like to access logs from 3rd party tools up to 60 days old. What solution would you recommend to meet this requirement?","answers":[{"ans":"Use Cloud Logging and keep log data in the Cloud Firestore service for 60 days. Create a logging policy to delete the data after 60 days.","val":false},{"ans":"Use Cloud Logging and set up a Pub\/Sub topic to receive log data and write that data to a Cloud Storage bucket to keep the logs 60 days. Create a data lifecycle policy to delete logs after 60 days.","val":false},{"ans":"Use Cloud Logging and set up a Log Router to create a Cloud Storage sink to keep the logs 60 days. Create a data lifecycle policy to delete logs after 60 days.","val":true},{"ans":"Use Cloud Logging and set up a Log Router to create a Bigtable sink to keep the logs 60 days. Create a data lifecycle policy to delete logs after 60 days.","val":false}],"q_expl":"Cloud Logging keeps logs by default up to 30 days and could be stored for a custom time period. By setting up a log sink for Cloud Storage, you can route logs to Cloud Storage where it can be kept for 60 days and accessed by 3rd party tools during that time. If log data were written to Cloud Pub\/Sub another service would have to read that data and write it to a long term storage system, such as Cloud Storage. Bigtable is not a Cloud Logging sink option. See https:\/\/cloud.google.com\/logging\/docs\/routing\/overview and https:\/\/cloud.google.com\/logging\/docs\/buckets."},{"label":"test_18","q_format":"single","q_text":"A developer tries to create a service account for a data pipeline but is unable to complete the operation. Which of the following could be the cause?","answers":[{"ans":"A policy has been applied to the resource hierarchy that enforces the constraints\/iam.disableServiceAccountKeyCreation constraint.","val":true},{"ans":"The developer has not properly configured a Domain Name Services (DNS) A record. An A record should be added to DNS.","val":false},{"ans":"A policy has been applied to an IAM group that disables the permission to create service accounts. That policy should be dropped.","val":false},{"ans":"The developer has not specified a properly configured deployment.yaml file. The yaml file should be corrected.","val":false}],"q_expl":"The constraints\/iam.disableServiceAccountKeyCreation is enabled and that prevents principals from creating user-managed service account keys. The constraint should be removed to allow developers to create a service account. Deployment.yaml files are used to configure Kubernetes deployments. DNS A records associate an IP address with a domain name. Policies are not attached to identity types, they are attached to entities in the resource hierarchy. See https:\/\/cloud.google.com\/resource-manager\/docs\/organization-policy\/org-policy-constraints"},{"label":"test_18","q_format":"single","q_text":"In order to comply with industry regulations, you will need to use customer managed keys when analyzing data using Cloud Dataproc. You will be managing Cloud Dataproc clusters using command line tools. What command would you use with the \u2013gce-pd-kms-key parameter to specify a Cloud KMS resource ID to use with the cluster?","answers":[{"ans":"gcloud dataproc clusters create","val":true},{"ans":"gcloud clusters dataproc kms","val":false},{"ans":"gcloud clusters dataproc create","val":false},{"ans":"gcloud dataproc clusters kms","val":false}],"q_expl":"The correct answer is gcloud dataproc clusters create. The other options are not valid gcloud commands. See https:\/\/cloud.google.com\/sdk\/gcloud\/reference\/dataproc\/clusters\/create"},{"label":"test_18","q_format":"single","q_text":"A manufacturer of delivery drones is implementing a new data analysis pipeline to detect part failures before they occur. The drones have multiple sensors that send performance and environment data to an analytics pipeline. Currently, data is sent to a REST API endpoint. The REST API endpoint that receives data cannot always keep up with the pace data is arriving. When that happens, data is lost. Machine learning engineers have asked you to change the ingestion process to reduce this data loss. What would you do?","answers":[{"ans":"Write data to a Cloud SQL Postgres database endpoint and have the ingestion application query the database.","val":false},{"ans":"Write data to a Cloud Storage bucket instead of a REST API endpoint and have the ingestion application read from the bucket.","val":false},{"ans":"Write data to a Cloud Pub\/Sub topic instead of a REST API endpoint and have the ingestion application read from the topic.","val":true},{"ans":"Create a Hadoop cluster in Compute Engine using managed instance groups and write data to an Hbase database and have the application query the database.","val":false}],"q_expl":"Write data to a Cloud Pub\/Sub topic instead of a REST API endpoint and have the ingestion application read from the topic. In the event of a spike in data, Cloud Pub\/Sub will buffer the data until it can be processed. Cloud Storage is an object storage system and is often used for ingesting large objects, such as images, videos or documents but Cloud Pub\/Sub is a better way to ingest small amounts of data, such as telemetry data from an IoT sensor. Cloud SQL is not sufficiently low latency or scalable enough for this use case. HBase on Hadoop would require more administrative overhead than using Cloud Pub\/Sub and would not scale as well as Cloud Pub\/Sub for this use case. See https:\/\/cloud.google.com\/dataflow\/docs\/concepts\/streaming-with-cloud-pubsub"},{"label":"test_18","q_format":"single","q_text":"A user of a Cloud Dataproc cluster needs permission to stop a cluster. They will also need to instantiate workflow templates and other common user tasks. You want to follow Google Cloud recommend best practices for security. What role would you use to grant permission to stop a cluster?","answers":[{"ans":"roles\/dataproc.editor","val":true},{"ans":"roles\/dataproc.viewer","val":false},{"ans":"Create a custom role with only permissions to stop the cluster and imitate workflows.","val":false},{"ans":"roles\/dataproc.admin","val":false}],"q_expl":"Roles\/dataproc.editor will provide permissions to stop clusters, initiate workflow templates, and other common user tasks. Roles\/dataproc.admin would provide more privileges than needed and violate the principle of Least Privilege. Roles.dataproc.viewer would not provide permission needed. There is no need to create a custom role and Google Cloud recommends using predefined roles when they meet your needs and only use custom roles when a predefined role does not exist that meets your needs. See https:\/\/cloud.google.com\/dataproc\/docs\/concepts\/iam\/iam"},{"label":"test_18","q_format":"single","q_text":"What technique is used in backpropagation to update parameters of a model during training?","answers":[{"ans":"L2 or Ridge Regression","val":false},{"ans":"L1 or Lasso Regression","val":false},{"ans":"Gradient descent","val":true},{"ans":"Feature crosses","val":false}],"q_expl":"Gradient descent is an optimization algorithm that minimizes a function by iteratively moving in the direction of the steepest descent. L1 or Lasso and L2 or Ridge Regression are regularization techniques. Feature crosses are a way to create synthetic features from two or more features. See https:\/\/builtin.com\/data-science\/gradient-descent"},{"label":"test_18","q_format":"multiple","q_text":"You use materialized views in BigQuery. You are incurring higher than expected charges for BigQuery and suspect it may be related to materialized views. What materialized view characteristic could increase your BigQuery costs? (Choose 2)","answers":[{"ans":"The frequency of materialized view refresh","val":true},{"ans":"The datatypes used in materialized views","val":false},{"ans":"The total volume of data stored in materialized views","val":true},{"ans":"The number of users with read access to the materialized view","val":false}],"q_expl":"The amount of data stored and the frequency of refresh jobs can increase the cost of maintaining materialized views. The data types used in the materialized view do not affect the cost. The number of users reading a materialized view does not affect cost but the total amount of data scanned would impact cost. See https:\/\/cloud.google.com\/bigquery\/docs\/materialized-views-intro"},{"label":"test_18","q_format":"single","q_text":"You are uploading several hundred files to Google Cloud using gsutil rsynch. The set of files fails to fully upload. You\u2018d rather not reload files that were successfully uploaded. What command would you use to resume the rsynch operation?","answers":[{"ans":"The gsutil rsynch resume command","val":false},{"ans":"The same command that was used initially and the --resume parameter.","val":false},{"ans":"The same command that was used initially and the --upload parameter.","val":false},{"ans":"The same command that was used initially. Gsutil rsynch will automatically resume.","val":true}],"q_expl":"The correct answer is the same command that was initially used. There is no need to specify other parameters to resume a gsutil rsynch command. See https:\/\/cloud.google.com\/storage\/docs\/gsutil\/commands\/rsync"},{"label":"test_18","q_format":"single","q_text":"A group of data engineers will be working on several initiatives. Each initiative will have their own VMs, storage buckets, and sets of Cloud Functions. The initiatives will all be governed by the same set of constraints that are required to stay in compliance with regulations. How would you recommend the data engineers organize their Google Cloud resources?","answers":[{"ans":"Use an organization for each initiative and place those folders in a project. Attach policies to the organization to enforce constraints.","val":false},{"ans":"Use a project for each initiative and place those projects in a folder. Attach policies to the folder to enforce constraints.","val":true},{"ans":"Use a project for each initiative and place those projects in an organization. Attach policies to the organization to enforce constraints.","val":false},{"ans":"Use a folder for each initiative and place those folders in a project. Attach policies to the project to enforce constraints.","val":false}],"q_expl":"Each initiative should have it\u2018s own project to isolate and manage it\u2018s resources. All projects should be in the same folder and policies should be attached to that folder so all projects in the folder will inherit them. Folders cannot be in a project and organizations cannot be in folders. See https:\/\/cloud.google.com\/resource-manager\/docs\/cloud-platform-resource-hierarchy"},{"label":"test_18","q_format":"multiple","q_text":"What are three possible ways to address overfitting when training a spam classifier (Select three options.)?","answers":[{"ans":"Use an expanded set of features","val":false},{"ans":"Decrease the number of training examples","val":false},{"ans":"Increase the number of training examples","val":true},{"ans":"Increase the value of regularization parameters","val":true},{"ans":"Decrease the value of regularization parameter","val":false},{"ans":"Use a reduced set of features","val":true}],"q_expl":"Get more training examples: Having more training examples can help the model generalize better by capturing a wider range of variations in the data. Use a smaller set of features: Using a smaller set of features can help simplify the model and reduce overfitting by limiting its capacity to fit the noise in the data. Increase the regularization parameters: Increasing the regularization parameters, such as L1 or L2 regularization, can help control overfitting by penalizing large weights and biases and forcing the model to focus on the most relevant features."},{"label":"test_18","q_format":"single","q_text":"A team of data scientists has been using an on-premises cluster running Hadoop and HBase. They want to migrate to a managed service in Google Cloud. They also want to minimize changes to programs that make extensive use of the HBase API. What GCP service would you recommend?","answers":[{"ans":"Cloud Dataflow","val":false},{"ans":"Bigtable","val":true},{"ans":"BigQuery","val":false},{"ans":"Cloud Spanner","val":false}],"q_expl":"The correct answer is Bigtable, which is a data store providing an HBASE compatible API. BigQuery is a data warehouse service that supports SQL but does not have an HBASE compatible API. Cloud Spanner is a relational database and not a replacement for Hadoop and HBASE. Cloud Dataflow is a data pipeline service that includes an Apache Beam runner. See https:\/\/cloud.google.com\/bigtable\/docs\/hbase-bigtable"},{"label":"test_18","q_format":"single","q_text":"As the architect of a seismic data analysis system that utilizes an Apache Hadoop cluster for its ETL process, you encounter an issue where the ETL process takes several days to process a data set due to the computational complexity of some steps. After investigating the issue, you discover that the sensor calibration step has been omitted. To ensure that sensor calibration is carried out systematically in the future, which of the following actions should you take?","answers":[{"ans":"Develop an algorithm that uses simulation to predict the variance of the data output from the last MapReduce job based on calibration factors, and then apply the correction to all data","val":false},{"ans":"Append sensor calibration data to the output of the ETL process, and provide documentation for users to apply sensor calibration themselves","val":false},{"ans":"Modify the existing MapReduce jobs to apply sensor calibration before anything else is done.","val":false},{"ans":"Add a new MapReduce job to apply sensor calibration to the raw data, and ensure that all other MapReduce jobs are chained after this step.","val":true}],"q_expl":"To carry out sensor calibration systematically in the future, you should modify the ETL process to introduce a new MapReduce job that applies sensor calibration to the raw data. This new job should be the first job in the ETL process to ensure that all subsequent MapReduce jobs are chained after this. By doing this, you can ensure that all the data is accurately calibrated and the subsequent computational expensive jobs will process the data more efficiently. Option A is not recommended because applying sensor calibration after some of the transformMapReduce jobs might lead to inaccurate data analysis. Option C is not recommended because it would require users to apply the sensor calibration manually, which may lead to inconsistency and errors. Option D is not recommended because it involves predicting the variance of the output and applying corrections, which may not be accurate or effective."},{"label":"test_18","q_format":"single","q_text":"A team of researchers is running a high performance distributed computing platform on premises but wants to migrate to Google Cloud. The platform uses virtual machines. The researchers want to be able to scale up the number of virtual machines in the cluster based on CPU load. What would you recommend they use?","answers":[{"ans":"Kubernetes cluster","val":false},{"ans":"Cloud Run","val":false},{"ans":"Unmanaged instance groups","val":false},{"ans":"Managed instance groups","val":true}],"q_expl":"The correct answer is managed instance groups, which is a way of deploying Compute Engine instances basesed on a template. Kubernetes is used for running containers, not virtual machines. Cloud Run is also used to run containers not virtual machines. Unmanaged instance groups run virtual machines but do not support autoscaling. See https:\/\/cloud.google.com\/compute\/docs\/instance-groups"},{"label":"test_18","q_format":"single","q_text":"The CTO of your company is concerned about the costs of running data pipelines, especially some large batch processing jobs. The jobs do not have to be run on a fixed schedule and the CTO is willing to wait longer for jobs to complete if it can reduce costs. You are using Cloud Dataflow for most pipelines and would like to cut costs but not make any more changes than necessary. What would you recommend?","answers":[{"ans":"Use a different Apache Beam Runner","val":false},{"ans":"Use Dataflow FlexRS","val":true},{"ans":"Use Dataflow Streaming Engine","val":false},{"ans":"Use Dataflow Shuffle","val":false}],"q_expl":"The correct answer is to use Cloud Dataflow flexible resource scheduling (FlexRS) which reduces batch processing costs using scheduling techniques and preemptible VMs along with regular VMs. Streaming Engine is an optimization for stream, not batch, processing. Dataflow Shuffle provides for faster execution of batch jobs but does not necessarily reduce costs. Using a different Apache Beam runner would require more management overhead, for example, by running Apache Flink in Compute Engine. See https:\/\/cloud.google.com\/dataflow\/docs\/guides\/flexrs"},{"label":"test_18","q_format":"single","q_text":"As the developer of a new Cloud Dataflow pipeline, you\u2018d like to limit the processing resources used when testing a new pipeline. What parameter would you specify when executing your new Cloud Dataflow job?","answers":[{"ans":"--maxVMs","val":false},{"ans":"--maxContainers","val":false},{"ans":"--jobWorkers","val":false},{"ans":"--maxNumWorkers","val":true}],"q_expl":"The correct answer is \u2013maxNumWorkers, the other options are not valid parameters when executing a workflow in Cloud Dataflow. See https:\/\/cloud.google.com\/dataflow\/docs\/guides\/deploying-a-pipeline"},{"label":"test_18","q_format":"single","q_text":"As a consultant to a multi-national company, you are tasked with helping design a service to support an inventory management system that is strongly consistent, supports SQL, and can scale to support hundreds of users in North America, Asia, and Europe. What Google Cloud service would you recommend for this service?","answers":[{"ans":"Cloud SQL","val":false},{"ans":"BigQuery","val":false},{"ans":"Cloud Firestore","val":false},{"ans":"Cloud Spanner","val":true}],"q_expl":"Cloud Spanner is a global, horizontally scalable relational database with strong consistency and is the best option. Cloud SQL is not scalable beyond a single region. Cloud Firestore does not support SQL. BigQuery is an analytical database for data warehousing not an OLTP system such as an inventory management system. See https:\/\/cloud.google.com\/blog\/products\/gcp\/introducing-cloud-spanner-a-global-database-service-for-mission-critical-applications"},{"label":"test_18","q_format":"single","q_text":"A manufacturer of delivery drones has equipped drones with multiple sensors that send performance and environment data to the analytics pipeline. Temperature received over the past hour is analyzed and if any temperature reading is more than 2 standard deviations away from the mean for the past hour, an alert is triggered. You would like to build the analysis pipeline using a managed service. What Google Cloud service would you recommend?","answers":[{"ans":"Cloud Dataproc","val":false},{"ans":"Cloud Firestore","val":false},{"ans":"Cloud Data Fusion","val":false},{"ans":"Cloud Dataflow","val":true}],"q_expl":"Cloud Dataflow is a managed Apache Beam runner used for stream and batch processing and is the best choice. Cloud Dataproc is a managed Hadoop\/Spark cluster service. Cloud Data Fusion is an extraction, transformation, and load service typically used with data warehouses and related data analytic services. Cloud Firestore is a NoSQL document database service. See https:\/\/cloud.google.com\/dataflow\/docs\/concepts"},{"label":"test_18","q_format":"single","q_text":"A team of analysts is building machine learning models. They want to use managed services when possible but they would also like the ability to customize and tune their models. In particular, they want to be able to tune hyperparameters themselves. What managed AI service would you recommend they use?","answers":[{"ans":"Vertex AI custom training","val":true},{"ans":"Vertex AI AutoML training","val":false},{"ans":"BigQuery ML","val":false},{"ans":"Cloud TPUs","val":false}],"q_expl":"Vertex AI custom training allows for tuning hyperparameters. Vertex AI AutoML training tunes hyperparameters for you. BigQuery ML does not allow for hyperparameter tuning. Cloud TPUs are accelerators you can use to train large deep learning models. See https:\/\/cloud.google.com\/vertex-ai\/docs\/start\/introduction-unified-platform"},{"label":"test_18","q_format":"single","q_text":"How can you securely automate a data pipeline process that involves nightly batch files with non-public information from Google Cloud Storage. The batch files need to be processed by a Spark Scala job on a Google Cloud Dataproc cluster, and the results must be deposited into Google BigQuery. Which of the following is the most secure approach?","answers":[{"ans":"Use a user account with the Project Viewer role on the Cloud Dataproc cluster to read the batch files and write to BigQuery.","val":false},{"ans":"Use a service account that has permission to read the batch files and write to BigQuery.","val":true},{"ans":"Restrict access to the Google Cloud Storage bucket to only allow you to see the files","val":false},{"ans":"Grant the Project Owner role to a service account and run the job using that account.","val":false}],"q_expl":"It is best practice to use service accounts with the least privilege necessary to perform a specific task when automating jobs. In this case, the job needs to read the batch files from Cloud Storage and write the results to BigQuery. Therefore, you should create a service account with the ability to read from the Cloud Storage bucket and write to BigQuery, and use that service account to run the job. Ref: https:\/\/cloud.google.com\/dataproc\/docs\/concepts\/configuring-clusters\/service-accounts#dataproc_service_accounts_2"},{"label":"test_18","q_format":"single","q_text":"A multi-national financial services company is creating a new service to facilitate cross-currency transactions. The database must provide strong consistency for transactions that may be initiated by any customer. Customers are initially located in Europe but the company plans to expand to Asia, Africa, North America and South America within a year. The database must support normalized data models. What Google Cloud managed database service would you use?","answers":[{"ans":"Cloud Bigtable","val":false},{"ans":"Cloud SQL","val":false},{"ans":"BigQuery","val":false},{"ans":"Cloud Spanner","val":true}],"q_expl":"Cloud Spanner is the correct choice, it provides global scale relational database services, including strong consistency. Cloud SQL is appropriate for regional-scale databases. BigQuery is an analytical database designed for data warehousing and analytics. Cloud Bigtable is a NoSQL database and does not meet the specified requirements. https:\/\/cloud.google.com\/blog\/topics\/developers-practitioners\/what-cloud-spanner"},{"label":"test_18","q_format":"single","q_text":"A university research group has started a company to commercialize a laboratory management system. Their application uses a MongoDB database but the group would like to migrate to a managed database service in Google Cloud. What service would you recommend they use?","answers":[{"ans":"BigQuery","val":false},{"ans":"Cloud Bigtable","val":false},{"ans":"Cloud SQL","val":false},{"ans":"Cloud Firestore","val":true}],"q_expl":"MongoDB is a document database so Cloud Firestore is the best option since it is also a document database. Cloud Bigtable is a wide-column NoSQL database and not a good replacement for MongoDB. BigQuery is an analytical database designed for data warehousing and data analysis. Cloud SQL is a relational database and not a good replacement for a NoSQL database. See https:\/\/firebase.google.com\/docs\/firestore\/data-model"},{"label":"test_18","q_format":"single","q_text":"You are designing a Bigtable schema and have several groups of columns that are frequently used together. You want to optimize read performance and follow Google Cloud recommended best practices. How would you treat these groups of columns?","answers":[{"ans":"Put related columns in column family","val":true},{"ans":"Put only one set of related columns in a table and use one table for each group","val":false},{"ans":"Create secondary indexes that include all columns in a group. Create one secondary index for each group.","val":false},{"ans":"Define a separate row key for each group.","val":false}],"q_expl":"Related columns should be placed in a column family. A single table can have multiple column families. Related data should be in one table, not multiple tables. Bigtable does not support secondary indexes. Row keys are specified for each row, not for each column family. See https:\/\/cloud.google.com\/bigtable\/docs\/schema-design#best-practices"},{"label":"test_19","q_format":"single","q_text":"You have a real-time monitoring application that streams data to Bigtable. It is not performing as well as expected. You use a row key that starts with a unique ID of each source system. Each source system sends 500K of data per minute and that is written to one row. There are approximately 200 column families, each having on average 10 columns. What could be the cause of the poor performance?","answers":[{"ans":"10 columns per column family is exceeds the recommended 5 columns maximum per column family","val":false},{"ans":"200 column families exceeds the recommended 100 column family limit","val":true},{"ans":"500K is more data than Bigtable can efficiently ingest per row","val":false},{"ans":"Row keys should not start with a unique ID","val":false}],"q_expl":"Google recommends limiting a table to no more than 100 column families otherwise performance can degrade. Bigtable can store up to 10MB per row so 500K is not too much data. Row keys should start with non-sequential prefixes to avoid hot spotting. 10 columns is not too many columns for a column family. See https:\/\/cloud.google.com\/bigtable\/docs\/schema-design"},{"label":"test_19","q_format":"single","q_text":"The online retailer has a Google App Engine-based application and wants to allow customers to transact directly through it, in addition to managing shopping transactions and analyzing data from multiple datasets via a business intelligence tool. They have a preference to utilize only one database for this purpose. The question is, which Google Cloud database would be the most appropriate option?","answers":[{"ans":"Cloud SQL","val":false},{"ans":"BigQuery","val":true},{"ans":"Cloud BigTable","val":false},{"ans":"Cloud Datastore","val":false}],"q_expl":"Therefore, BigQuery is the most suitable option for the online retailer\u2018s needs, as it is designed specifically for running complex analytical queries on large datasets and can easily integrate with other Google Cloud services such as Google App Engine."},{"label":"test_19","q_format":"single","q_text":"To avoid hot-spotting in your Bigtable clusters, you have designed a row key that uses a UUID prefix. This is not working as expected and there is hot-spotting when writing data to Bigtable. What could be the cause of the hot-spotting?","answers":[{"ans":"The name of the row key column is too long.","val":false},{"ans":"You have chosen a type of UUID that has sequentially ordered strings.","val":true},{"ans":"You have incorrectly configured column families.","val":false},{"ans":"Secondary indexes are slowing write operations.","val":false}],"q_expl":"This could be caused by UUIDs that are sequentially generated. You should use UUID version 4 that uses a random number generator. Column families structure do not affect hot spotting. The name of a row key does not cause hot spotting. Bigtable does not support secondary indexes. See https:\/\/cloud.google.com\/bigtable\/docs\/performance"},{"label":"test_19","q_format":"single","q_text":"A Cloud Dataproc cluster is experiencing a higher than normal workload and you\u2018d like to add several preemptible VMs as worker nodes. What command would you use?","answers":[{"ans":"The number of preemptible nodes in a Cloud Dataproc cluster cannot be changed once the cluster is created.","val":false},{"ans":"gcloud dataproc clusters update with the --preemptible-vms parameter","val":false},{"ans":"gcloud dataproc clusters update with the --num-secondary-workers parameter","val":true},{"ans":"gcloud dataproc clusters update with the --num-workers parameter","val":false}],"q_expl":"The number of preemptible nodes can be updated using gcloud dataproc clusters update with the \u2013num-secondary-workers parameter. The \u2013num-workers parameter is used to change the number of primary (non-preemptible) workers. There is no \u2013preemptible-vms parameter in the gcloud dataproc command. The number of preemptible (secondary) workers can be changed after creating a cluster. See https:\/\/cloud.google.com\/sdk\/gcloud\/reference\/dataproc\/clusters\/update"},{"label":"test_19","q_format":"single","q_text":"A new workload has been deployed to Cloud Dataproc, which is configured with an autoscaling policy. You are noticing a FetchFailedException is occurring intermittently. What would be the most likely cause of this problem?","answers":[{"ans":"You are using Google Cloud Storage instead of local storage for persistent storage.","val":false},{"ans":"The autoscaling policy is adding nodes too fast and data is being dropped.","val":false},{"ans":"The autoscaling policy is scaling down and shuffle data is lost when a node is decommissioned.","val":true},{"ans":"You are using a GCS bucket with improper access controls.","val":false}],"q_expl":"The FetchFailedException can occur when shuffle data is lost when a node is decommissioned. The autopolicy should be configured based on the longest running job in the cluster. Adding nodes will not cause a loss of data. Cloud Storage is the preferred persistent storage method for Dataproc clusters. While FetchFailedException can be caused by network issues, that is not likely to be a problem when using Cloud Storage for a Cloud Dataproc cluster. If the storage bucket had improper access controls then errors would occur consistently, not intermittently. See https:\/\/cloud.google.com\/blog\/topics\/developers-practitioners\/dataproc-best-practices-guide"},{"label":"test_19","q_format":"single","q_text":"You work for a game developer that is using Cloud Firestore and needs to regularly create backups. You\u2018d like to issue a command and have it return immediately while the backup runs in the background. You want the backup file to be stored in a Cloud Storage bucket named game-ds-backup. What command would you use?","answers":[{"ans":"gsutil datastore export gs:\/\/game-ds-backup","val":false},{"ans":"gsutil datastore export gs:\/\/game-ds-backup --async","val":false},{"ans":"gcloud datastore export gs:\/\/game-ds-backup --async","val":true},{"ans":"gcloud datastore backup gs:\/\/game-ds-backup","val":false}],"q_expl":"The correct command is gcloud datastore export gs:\/\/game-ds-backup \u2013async. Export, not backup, is the datastore command to save data to a Cloud Storage bucket. Gsutil is used to manage Cloud Storage, not Cloud Datastore. See https:\/\/cloud.google.com\/datastore\/docs\/export-import-entities and https:\/\/cloud.google.com\/sdk\/gcloud\/reference\/datastore\/export"},{"label":"test_19","q_format":"single","q_text":"When using Cloud Data Fusion you receive an error that a Dataproc operation failed and the user is not authorized to act as a service account. What would you do to correct this problem?","answers":[{"ans":"Create a Cloud Dataproc cluster before starting Cloud Data Fusion.","val":false},{"ans":"Ensure both Cloud Data Fusion and Cloud Dataproc are running in the same zone.","val":false},{"ans":"Grant the Service Account User role to Cloud Data Fusion.","val":true},{"ans":"Grant the Service Account User role to Cloud Dataproc.","val":false}],"q_expl":"The correct answer is to grant the Service Account User role to Cloud Data Fusion. Cloud Dataproc does not need that role assigned to it. This is an access control issue and not related to the location of clusters. Cloud Data Fusion will manage its use of Cloud Dataproc clusters. See https:\/\/cloud.google.com\/data-fusion\/docs\/concepts\/service-accounts"},{"label":"test_19","q_format":"single","q_text":"A data pipeline uses Cloud Pub\/Sub for ingesting data. The data is stored in topics and a Dataflow workflow reads from a subscription to that topic, processes the data, and writes output to BigQuery. What is the recommended way to authenticate when reading data from Cloud Pub\/Sub?","answers":[{"ans":"Custom role","val":false},{"ans":"Google Workspace Identity","val":false},{"ans":"Basic role","val":false},{"ans":"Use service accounts","val":true}],"q_expl":"Service accounts are the recommended way to authentic for most use cases when using Cloud Pub\/Sub. Google Workspace Identity should be used by human users, service accounts are used for applications. Custom roles and basic roles are for authorization not authentication. See https:\/\/cloud.google.com\/iam\/docs\/service-accounts"},{"label":"test_19","q_format":"single","q_text":"Messages are unexpectedly accumulating in service using Cloud Pub\/Sub. A developer unfamiliar with Cloud Pub\/Sub has asked for our help in diagnosing the problem. What would you point out with respect to how messages are removed from Cloud Pub\/Sub topics?","answers":[{"ans":"Once at least one subscriber for each subscription has acknowledged the message it will be deleted from storage.","val":true},{"ans":"Once at least one subscriber for each bucket has acknowledged the message it will be deleted from storage.","val":false},{"ans":"Once at least one subscriber for each topic has acknowledged the message it will be deleted from storage.","val":false},{"ans":"Once at least one subscriber for any subscription has acknowledged the message it will be deleted from storage.","val":false}],"q_expl":"The correct answer is that a message is deleted once at least one subscriber for a each subscription has acknowledged the message it will be deleted from storage. See https:\/\/cloud.google.com\/pubsub\/docs\/subscriber"},{"label":"test_19","q_format":"single","q_text":"A team of data analysts are proficient in using SQL but not programming in Java, Python, or other programming languages. They want to experiment with building machine learning models trained on relational data. They have approximately 1 TB of data to work with. What would you recommend they use?","answers":[{"ans":"Cloud TPUs","val":false},{"ans":"BigQuery ML","val":true},{"ans":"Bigtable","val":false},{"ans":"Cloud Fusion","val":false}],"q_expl":"The correct answer is BigQuery ML, which incorporates SQL functions to build, evaluate, and invoke machine learning models within SQL. Cloud TPUs are accelerators and are used with deep learning applications. Cloud Fusion is an ETL service. Bigtable is a NoSQL database for high volume, low latency write applications, such as IoT data ingestion and storage. See https:\/\/cloud.google.com\/bigquery-ml\/docs\/introduction"},{"label":"test_19","q_format":"single","q_text":"Your organization is setting up data pipelines for their campaign and needs to periodically identify the inputs and timings for all Google Cloud Pub\/Sub streaming data. To achieve this, engineers are using windowing and transformation in Google Cloud Dataflow. However, during testing, they encounter a problem where the Cloud Dataflow job fails for all streaming inserts. What is the most likely cause of this issue?","answers":[{"ans":"The job fails because a global windowing function has not been applied, causing the job to fail during pipeline creation.","val":false},{"ans":"The job fails because the engineers have not assigned the timestamp for the data","val":false},{"ans":"The job fails because a non-global windowing function has not been applied, causing the job to fail during pipeline creation.","val":true},{"ans":"The job fails because the triggers have not been set to handle data coming in late.","val":false}],"q_expl":"The question states that the engineers have decided to use windowing and transformation in Google Cloud Dataflow for periodic identification of inputs and their timings during their campaign. In streaming data, windowing is used to divide the data stream into finite windows and perform computations on each window of data. There are two types of windowing functions: global and non-global. Global windowing applies to all data elements in the entire data stream, and it processes all data elements uniformly without considering their time of arrival. Non-global windowing, on the other hand, applies different windows to different data elements based on their timestamp. Since the question mentions that the Cloud Dataflow job fails for all streaming inserts, it is likely that the engineers have not applied a non-global windowing function. Without a non-global windowing function, the job will fail because streaming data does not have a finite end time, and global windowing cannot be used for streaming data. Ref: https:\/\/beam.apache.org\/documentation\/programming-guide\/#windowing"},{"label":"test_19","q_format":"single","q_text":"Analysts are using Cloud Data Studio for analyzing data sets. They would like to improve the performance of the time required to update tables and charts when working with the data. What would you recommend they try to improve performance?","answers":[{"ans":"Use a live data source","val":false},{"ans":"Use an imported data source","val":false},{"ans":"Use an extracted data source","val":true},{"ans":"Use a blended data source","val":false}],"q_expl":"Extracted data sources are snapshots and can provide better performance than live data sources. Blended data sources are used to combine data from multiple data sources. There is no imported data source. See https:\/\/cloud.google.com\/bigquery\/external-data-sources"},{"label":"test_19","q_format":"single","q_text":"A data warehouse team is concerned that some data sources may have poor quality controls. They do not want to bring incorrect or invalid data into the data warehouse. What could they do to understand the scope of the problem before starting to write ETL code?","answers":[{"ans":"Load all source data into a data lake and then load it to the data warehouse.","val":false},{"ans":"Perform a data quality assessment on the source data after it is extracted from the source system. These should include checks for ranges of values in each attribute, distribution of values in each attribute, counts of the number of invalid and missing values, and other checks on source data.","val":true},{"ans":"Have administrators of the source systems produce a data quality verification before exporting the data.","val":false},{"ans":"Load the data into the data warehouse and log any records that fail integrity or consistency checks.","val":false}],"q_expl":"The correct answer is performing a data quality assessment on data extracted from the source system. Loading data from a data lake to a data warehouse will not provide an assessment of the range of the problem. Loading data into the data warehouse and logging failed checks is less efficient because it will provide log messages but not aggregate statistics on the full scope of the problem. The source systems may not have the ability to perform data quality assessments and if they do, you may get different kinds of reports from different systems. By performing a data quality assessment on extracted data you can produce a consistent set of reports for all data sources. See https:\/\/cloud.google.com\/blog\/products\/data-analytics\/principles-and-best-practices-for-data-governance-in-the-cloud"},{"label":"test_19","q_format":"single","q_text":"Where is the recommended location to store data that is subject to government regulations mandating an auditable record of access to certain types of data, assuming all expiring logs are correctly archived?","answers":[{"ans":"Should the data be stored in a BigQuery dataset that is viewable only by authorized personnel, with the Data Access log used to provide auditability.","val":true},{"ans":"Should the data be encrypted on Cloud Storage with user-supplied encryption keys, and each authorized user given a separate decryption key.","val":false},{"ans":"Should the data be stored in Cloud SQL, with separate database user names to each user, and the Cloud SQL Admin activity logs used to provide auditability.","val":false},{"ans":"Should the data be stored in a bucket on Cloud Storage that is accessible only by an AppEngine service that collects user information and logs the access before providing a link to the bucket.","val":false}],"q_expl":"The correct answer is:\nA. Should the data be stored in a BigQuery dataset that is viewable only by authorized personnel, with the Data Access log used to provide auditability.\nHere\u2019s a breakdown of the options and why only A aligns best with regulatory compliance for auditable data access:\n\nA. BigQuery with Data Access Log: BigQuery offers granular access controls (viewable only by authorized personnel) and maintains a comprehensive Data Access log that tracks all user access attempts. This detailed audit log is crucial for demonstrating compliance with regulations.\n\nB. Cloud Storage with User Keys: While encryption is essential, managing separate decryption keys per user for Cloud Storage becomes cumbersome and error-prone. It might not provide a centralized audit trail for access attempts.\n\nC. Cloud SQL with Separate Users: Cloud SQL with separate user accounts offers some access control, but its audit logs might not be as detailed as BigQuery\u2019s Data Access log for regulatory purposes.\n\nD. App Engine and Cloud Storage Bucket: This option adds unnecessary complexity and potential security risks. Cloud Storage access logs can be enabled, but the App Engine intermediary might not provide the level of auditability required for regulations.\n\nKey considerations for regulatory compliance:\n\nGranular Access Control: Restricting data access to authorized personnel is critical. BigQuery\u2019s access control features excel in this area.\nDetailed Audit Logs: Maintaining a comprehensive audit trail of all data access attempts is essential for demonstrating compliance. BigQuery\u2019s Data Access log fulfills this requirement.\nCentralized Management: A centralized solution for data storage and access management simplifies compliance efforts. BigQuery provides a well-managed environment.\n\nWhile other options might offer some level of security, BigQuery, with its granular access controls and comprehensive Data Access log, is best suited for storing data subject to government regulations requiring auditable access records."},{"label":"test_19","q_format":"single","q_text":"What is the recommended approach for an organization to share aggregate data from their Google BigQuery dataset containing user-level tables with other Google Cloud projects, while still maintaining control over access to the original user-level data and minimizing storage and analysis costs for other projects?","answers":[{"ans":"Should they create dataViewer Identity and Access Management (IAM) roles on the dataset to enable sharing.","val":false},{"ans":"Should they create and share a new dataset and table that contains the aggregate results.","val":false},{"ans":"Should they create and share an authorized view that provides the aggregate results.","val":true},{"ans":"Should they create and share a new dataset and view that provides the aggregate results.","val":false}],"q_expl":"The correct answer for the given question is C. Should they create and share an authorized view that provides the aggregate results.\nExplanation:\nHere\u2019s a breakdown of why this is the correct option and why the others are not:\nC. Should they create and share an authorized view that provides the aggregate results:\n\nCorrect: An authorized view is a virtual table that provides a filtered view of an existing table. By creating an authorized view that contains only the aggregate data, you can share the view with other Google Cloud projects while restricting access to the underlying user-level data. This approach effectively balances the need for data sharing with the requirement to maintain control over access to sensitive data.\n\nA. Should they create dataViewer Identity and Access Management (IAM) roles on the dataset to enable sharing:\n\nIncorrect: While creating dataViewer IAM roles can grant access to the dataset, it doesn\u2019t provide a way to filter the data and share only the aggregate results. This would still allow access to the original user-level data, which might not be desirable.\n\nB. Should they create and share a new dataset and table that contains the aggregate results:\n\nIncorrect: This approach would involve copying the aggregate data into a new dataset and table, which can be inefficient and increase storage costs. Additionally, maintaining consistency between the original data and the aggregated data can be challenging.\n\nD. Should they create and share a new dataset and view that provides the aggregate results:\n\nIncorrect: Creating a new dataset and view is similar to option B, and it would still involve copying the data and potentially increasing storage costs."}]